{
  
    
        "post0": {
            "title": "My talk at data science leuven",
            "content": "The video is recorded on Youtube: . youtube https://youtu.be/hk0arqhcX9U?t=4950 . You can see the slides here: slides . The talk itself is based on this notebook that appeared this the blog yesterday and that I used to demo during the talk. . The host of the conference was Istvan Hajnal. He said the following: . twitter https://twitter.com/dsleuven/status/1253391470444371968 . He also took the R out of my family name NachteRgaele. Troubles with R, it&#39;s becoming a story of my life... :joy: Behind the scene Kris Peeters calmly took the heat of doing the live streaming. :+1: Big thanks to the whole Data Science Leuven team that is doing all this on voluntary basis. . Standing on the shoulders of the giants . This talks was not possible without the awesome Altair visualisation library made by Jake VanderPlas. Secondly, it builds upon the open source Shap library made by Scott Lundberg. Those two libraries had a major impact on my daily work as datascientist at Colruyt group. They inspired me in trying to give back to the open source community with this talk. :metal: . If you want to learn how to use Altair I recommend the tutorial made by Vincent Warmerdam on his calm code site: (https://calmcode.io/altair/introduction.htm) . I would also like to thank my collegues at work who endured the dry-run of this talk and who made the suggestion to try to use a classifier to explain the clustering result. Top team! . Awesome fastpages . Finally, this blog is build with the awesome fastpages. I can now share a rendered Jupyter notebook, with working interactive demos, that can be opened in My binder or Google Colab with one click on a button. This means that readers can directly tinker around with the code and methods discussed in the talk. All you need is a browser and an internet connection. So thank you Jeremy Howard, Hamel Husain, and the fastdotai team. I will cast for two how awesome this is. .",
            "url": "https://cast42.github.io/blog/cast42/dsleuven/clustering/altair/shap/2020/04/24/talk-datascience-leuven_explain_clustering.html",
            "relUrl": "/cast42/dsleuven/clustering/altair/shap/2020/04/24/talk-datascience-leuven_explain_clustering.html",
            "date": " • Apr 24, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Explain clusters to business with Altair and Shapley values",
            "content": "# Load python libraries for data handling and plotting import numpy as np import pandas as pd import altair as alt import matplotlib.pyplot as plt . # Load the data set with wines from the internet df = pd.read_csv(&#39;https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv&#39;, sep=&#39;;&#39;) . # Show the first lines of the data set to get an idea what&#39;s in there. df.head() . fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality . 0 7.0 | 0.27 | 0.36 | 20.7 | 0.045 | 45.0 | 170.0 | 1.0010 | 3.00 | 0.45 | 8.8 | 6 | . 1 6.3 | 0.30 | 0.34 | 1.6 | 0.049 | 14.0 | 132.0 | 0.9940 | 3.30 | 0.49 | 9.5 | 6 | . 2 8.1 | 0.28 | 0.40 | 6.9 | 0.050 | 30.0 | 97.0 | 0.9951 | 3.26 | 0.44 | 10.1 | 6 | . 3 7.2 | 0.23 | 0.32 | 8.5 | 0.058 | 47.0 | 186.0 | 0.9956 | 3.19 | 0.40 | 9.9 | 6 | . 4 7.2 | 0.23 | 0.32 | 8.5 | 0.058 | 47.0 | 186.0 | 0.9956 | 3.19 | 0.40 | 9.9 | 6 | . # How many wines and features do we have ? df.shape . (4898, 12) . df.columns . Index([&#39;fixed acidity&#39;, &#39;volatile acidity&#39;, &#39;citric acid&#39;, &#39;residual sugar&#39;, &#39;chlorides&#39;, &#39;free sulfur dioxide&#39;, &#39;total sulfur dioxide&#39;, &#39;density&#39;, &#39;pH&#39;, &#39;sulphates&#39;, &#39;alcohol&#39;, &#39;quality&#39;], dtype=&#39;object&#39;) . # Define the standard X (feature matrix) and target series y (not used in here) X = df.drop(columns=&#39;quality&#39;) all_features = X.columns y = df[&#39;quality&#39;] . df.describe() . fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality . count 4898.000000 | 4898.000000 | 4898.000000 | 4898.000000 | 4898.000000 | 4898.000000 | 4898.000000 | 4898.000000 | 4898.000000 | 4898.000000 | 4898.000000 | 4898.000000 | . mean 6.854788 | 0.278241 | 0.334192 | 6.391415 | 0.045772 | 35.308085 | 138.360657 | 0.994027 | 3.188267 | 0.489847 | 10.514267 | 5.877909 | . std 0.843868 | 0.100795 | 0.121020 | 5.072058 | 0.021848 | 17.007137 | 42.498065 | 0.002991 | 0.151001 | 0.114126 | 1.230621 | 0.885639 | . min 3.800000 | 0.080000 | 0.000000 | 0.600000 | 0.009000 | 2.000000 | 9.000000 | 0.987110 | 2.720000 | 0.220000 | 8.000000 | 3.000000 | . 25% 6.300000 | 0.210000 | 0.270000 | 1.700000 | 0.036000 | 23.000000 | 108.000000 | 0.991723 | 3.090000 | 0.410000 | 9.500000 | 5.000000 | . 50% 6.800000 | 0.260000 | 0.320000 | 5.200000 | 0.043000 | 34.000000 | 134.000000 | 0.993740 | 3.180000 | 0.470000 | 10.400000 | 6.000000 | . 75% 7.300000 | 0.320000 | 0.390000 | 9.900000 | 0.050000 | 46.000000 | 167.000000 | 0.996100 | 3.280000 | 0.550000 | 11.400000 | 6.000000 | . max 14.200000 | 1.100000 | 1.660000 | 65.800000 | 0.346000 | 289.000000 | 440.000000 | 1.038980 | 3.820000 | 1.080000 | 14.200000 | 9.000000 | . alt.Chart(df).mark_point().encode(x=&#39;fixed acidity&#39;, y=&#39;volatile acidity&#39;, color=&#39;quality:N&#39;) . # Add interactivity alt.Chart(df).mark_point().encode(x=&#39;fixed acidity&#39;, y=&#39;volatile acidity&#39;, color=&#39;quality:N&#39;, tooltip=[&#39;alcohol&#39;, &#39;quality&#39;]).interactive() . Scaling . From the describe, we see that domains of the feature differ widely. Feature &#39;fixed acidity&#39; has mean 6.854788, while feature &#39;volatile acidity&#39; has mean 0.278241. This is a sign that are on a different scale. We scale the features first so that we can use them together. . # When features have different scale we have to scale them so that we can use them together from sklearn.preprocessing import RobustScaler from sklearn.preprocessing import StandardScaler . scaler = StandardScaler() # scaler = RobustScaler() # Take the robust scaler when data contains outliers that you want to remove . X_scaled = scaler.fit_transform(X) . PCA . Perform principal component analysis to get an idea of the dimensionality of the wine dataset. Next reduce to two dimension so that we can make a 2D scatter plot were each dot is a wine from the set. . from sklearn.decomposition import PCA pca = PCA().fit(X_scaled) . df_plot = pd.DataFrame({&#39;Component Number&#39;: 1+np.arange(X.shape[1]), &#39;Cumulative explained variance&#39;: np.cumsum(pca.explained_variance_ratio_)}) alt.Chart(df_plot).mark_bar().encode( x=alt.X(&#39;Component Number:N&#39;, axis=alt.Axis(labelAngle=0)), y=alt.Y(&#39;Cumulative explained variance&#39;, axis=alt.Axis(format=&#39;%&#39;, title=&#39;Percentage&#39;)), tooltip=[alt.Tooltip(&#39;Cumulative explained variance&#39;, format=&#39;.0%&#39;)] ).properties( title=&#39;Cumulative explained variance&#39; ).properties( width=400 ) . We see that almost all features are needed to explain all the variance. Only with 9 comoponent we can explain 97% of the variance. This means that we should use all features from the dataset and feature selection is not necessary in this case. . PCA with 2 Dimensions . Perform dimension reduction to two dimentions with PCA to be able to draw all the wines in one graph (not for the clustering!) . twod_pca = PCA(n_components=2) X_pca = twod_pca.fit_transform(X_scaled) . # Add first to PCA components to the dataset so we can plot it df[&#39;pca1&#39;] = X_pca[:,0] df[&#39;pca2&#39;] = X_pca[:,1] . df[&#39;member&#39;] = 1 df.groupby(&#39;quality&#39;)[&#39;member&#39;].transform(&#39;count&#39;).div(df.shape[0]) . 0 0.448755 1 0.448755 2 0.448755 3 0.448755 4 0.448755 ... 4893 0.448755 4894 0.297468 4895 0.448755 4896 0.179665 4897 0.448755 Name: member, Length: 4898, dtype: float64 . selection = alt.selection_multi(fields=[&#39;quality&#39;], bind=&#39;legend&#39;) df[&#39;quality_weight&#39;] = df[&#39;quality&#39;].map(df[&#39;quality&#39;].value_counts(normalize=True).to_dict()) # Draw 20% stratified sample alt.Chart(df.sample(1000, weights=&#39;quality_weight&#39;)).mark_circle(size=60).encode( x=alt.X(&#39;pca1&#39;, title=&#39;First component&#39;), y=alt.Y(&#39;pca2&#39;, title=&#39;Second component&#39;), color=alt.Color(&#39;quality:N&#39;), tooltip=[&#39;quality&#39;], opacity=alt.condition(selection, alt.value(1), alt.value(0.2)) ).properties( title=&#39;PCA analyse&#39;, width=600, height=400 ).add_selection( selection ) . df_twod_pca = pd.DataFrame(data=twod_pca.components_.T, columns=[&#39;pca1&#39;, &#39;pca2&#39;], index=X.columns) . pca1 = alt.Chart(df_twod_pca.reset_index()).mark_bar().encode( y=alt.Y(&#39;index:O&#39;, title=None), x=&#39;pca1&#39;, color=alt.Color(&#39;pca1&#39;, scale=alt.Scale(scheme=&#39;viridis&#39;)), tooltip = [alt.Tooltip(&#39;index&#39;, title=&#39;Feature&#39;), alt.Tooltip(&#39;pca1&#39;, format=&#39;.2f&#39;)] ) pca2 = alt.Chart(df_twod_pca.reset_index()).mark_bar().encode( y=alt.Y(&#39;index:O&#39;, title=None), x=&#39;pca2&#39;, color=alt.Color(&#39;pca2&#39;, scale=alt.Scale(scheme=&#39;viridis&#39;)), tooltip = [alt.Tooltip(&#39;index&#39;, title=&#39;Feature&#39;), alt.Tooltip(&#39;pca2&#39;, format=&#39;.2f&#39;)] ) (pca1 &amp; pca2).properties( title=&#39;Loadings of the first two principal components&#39; ) . Kmeans clustering . Determine the number of cluster for Kmeans clustering by lopping from 2 to 11 cluster. Calculate for each result the within-cluster variation (inertia), the silhoutte) and the Davies-Bouldin index. Use the elbow method to determine the number of clusters. The elbow method seeks the value of k after which the clustering quality improves only marginally. . from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score, davies_bouldin_score . km_scores= [] km_silhouette = [] km_db_score = [] for i in range(2,X.shape[1]): km = KMeans(n_clusters=i, random_state=1301).fit(X_scaled) preds = km.predict(X_scaled) print(f&#39;Score for number of cluster(s) {i}: {km.score(X_scaled):.3f}&#39;) km_scores.append(-km.score(X_scaled)) silhouette = silhouette_score(X_scaled,preds) km_silhouette.append(silhouette) print(f&#39;Silhouette score for number of cluster(s) {i}: {silhouette:.3f}&#39;) db = davies_bouldin_score(X_scaled,preds) km_db_score.append(db) print(f&#39;Davies Bouldin score for number of cluster(s) {i}: {db:.3f}&#39;) print(&#39;-&#39;*100) . Score for number of cluster(s) 2: -42548.672 Silhouette score for number of cluster(s) 2: 0.214 Davies Bouldin score for number of cluster(s) 2: 1.775 - Score for number of cluster(s) 3: -39063.495 Silhouette score for number of cluster(s) 3: 0.144 Davies Bouldin score for number of cluster(s) 3: 2.097 - Score for number of cluster(s) 4: -35986.918 Silhouette score for number of cluster(s) 4: 0.159 Davies Bouldin score for number of cluster(s) 4: 1.809 - Score for number of cluster(s) 5: -33699.043 Silhouette score for number of cluster(s) 5: 0.144 Davies Bouldin score for number of cluster(s) 5: 1.768 - Score for number of cluster(s) 6: -31973.251 Silhouette score for number of cluster(s) 6: 0.146 Davies Bouldin score for number of cluster(s) 6: 1.693 - Score for number of cluster(s) 7: -30552.998 Silhouette score for number of cluster(s) 7: 0.126 Davies Bouldin score for number of cluster(s) 7: 1.847 - Score for number of cluster(s) 8: -29361.568 Silhouette score for number of cluster(s) 8: 0.128 Davies Bouldin score for number of cluster(s) 8: 1.790 - Score for number of cluster(s) 9: -28198.302 Silhouette score for number of cluster(s) 9: 0.128 Davies Bouldin score for number of cluster(s) 9: 1.760 - Score for number of cluster(s) 10: -27444.897 Silhouette score for number of cluster(s) 10: 0.118 Davies Bouldin score for number of cluster(s) 10: 1.843 - . df_plot = pd.DataFrame({&#39;Number of clusters&#39;:[i for i in range(2,X.shape[1])],&#39;kmean score&#39;: km_scores}) alt.Chart(df_plot).mark_line(point=True).encode( x=alt.X(&#39;Number of clusters:N&#39;, axis=alt.Axis(labelAngle=0)), y=alt.Y(&#39;kmean score&#39;), tooltip=[alt.Tooltip(&#39;kmean score&#39;, format=&#39;.2f&#39;)] ).properties( title=&#39;Kmean score ifo number of cluster&#39; ).properties( title=&#39;The elbow method for determining number of clusters&#39;, width=400 ) . df_plot = pd.DataFrame({&#39;Number of clusters&#39;:[i for i in range(2,X.shape[1])],&#39;silhouette score&#39;: km_silhouette}) alt.Chart(df_plot).mark_line(point=True).encode( x=alt.X(&#39;Number of clusters:N&#39;, axis=alt.Axis(labelAngle=0)), y=alt.Y(&#39;silhouette score&#39;), tooltip=[alt.Tooltip(&#39;silhouette score&#39;, format=&#39;.2f&#39;)] ).properties( title=&#39;Silhouette score ifo number of cluster&#39;, width=400 ) . df_plot = pd.DataFrame({&#39;Number of clusters&#39;:[i for i in range(2, X.shape[1])],&#39;davies bouldin score&#39;: km_db_score}) alt.Chart(df_plot).mark_line(point=True).encode( x=alt.X(&#39;Number of clusters:N&#39;, axis=alt.Axis(labelAngle=0)), y=alt.Y(&#39;davies bouldin score&#39;), tooltip=[alt.Tooltip(&#39;davies bouldin score&#39;, format=&#39;.2f&#39;)] ).properties( title=&#39;Davies Bouldin score ifo number of cluster&#39;, width=400 ) . from sklearn.metrics import silhouette_samples, silhouette_score import matplotlib.cm as cm X = X.values range_n_clusters = [2, 3, 4, 5, 6] for n_clusters in range_n_clusters: # Create a subplot with 1 row and 2 columns fig, (ax1, ax2) = plt.subplots(1, 2) fig.set_size_inches(18, 7) # The 1st subplot is the silhouette plot # The silhouette coefficient can range from -1, 1 but in this example all # lie within [-0.1, 1] ax1.set_xlim([-0.1, 1]) # The (n_clusters+1)*10 is for inserting blank space between silhouette # plots of individual clusters, to demarcate them clearly. ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10]) # Initialize the clusterer with n_clusters value and a random generator # seed of 10 for reproducibility. clusterer = KMeans(n_clusters=n_clusters, random_state=10) cluster_labels = clusterer.fit_predict(X) # The silhouette_score gives the average value for all the samples. # This gives a perspective into the density and separation of the formed # clusters silhouette_avg = silhouette_score(X, cluster_labels) print(&quot;For n_clusters =&quot;, n_clusters, &quot;The average silhouette_score is :&quot;, silhouette_avg) # Compute the silhouette scores for each sample sample_silhouette_values = silhouette_samples(X, cluster_labels) y_lower = 10 for i in range(n_clusters): # Aggregate the silhouette scores for samples belonging to # cluster i, and sort them ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i] ith_cluster_silhouette_values.sort() size_cluster_i = ith_cluster_silhouette_values.shape[0] y_upper = y_lower + size_cluster_i color = cm.nipy_spectral(float(i) / n_clusters) ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values, facecolor=color, edgecolor=color, alpha=0.7) # Label the silhouette plots with their cluster numbers at the middle ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i)) # Compute the new y_lower for next plot y_lower = y_upper + 10 # 10 for the 0 samples ax1.set_title(&quot;The silhouette plot for the various clusters.&quot;) ax1.set_xlabel(&quot;The silhouette coefficient values&quot;) ax1.set_ylabel(&quot;Cluster label&quot;) # The vertical line for average silhouette score of all the values ax1.axvline(x=silhouette_avg, color=&quot;red&quot;, linestyle=&quot;--&quot;) ax1.set_yticks([]) # Clear the yaxis labels / ticks ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1]) # 2nd Plot showing the actual clusters formed colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters) ax2.scatter(X[:, 0], X[:, 1], marker=&#39;.&#39;, s=30, lw=0, alpha=0.7, c=colors, edgecolor=&#39;k&#39;) # Labeling the clusters centers = clusterer.cluster_centers_ # Draw white circles at cluster centers ax2.scatter(centers[:, 0], centers[:, 1], marker=&#39;o&#39;, c=&quot;white&quot;, alpha=1, s=200, edgecolor=&#39;k&#39;) for i, c in enumerate(centers): ax2.scatter(c[0], c[1], marker=&#39;$%d$&#39; % i, alpha=1, s=50, edgecolor=&#39;k&#39;) ax2.set_title(&quot;The visualization of the clustered data.&quot;) ax2.set_xlabel(&quot;Feature space for the 1st feature&quot;) ax2.set_ylabel(&quot;Feature space for the 2nd feature&quot;) plt.suptitle((&quot;Silhouette analysis for KMeans clustering on sample data &quot; &quot;with n_clusters = %d&quot; % n_clusters), fontsize=14, fontweight=&#39;bold&#39;) plt.show() . For n_clusters = 2 The average silhouette_score is : 0.5062782327345698 For n_clusters = 3 The average silhouette_score is : 0.4125412743885912 For n_clusters = 4 The average silhouette_score is : 0.3748324919186734 For n_clusters = 5 The average silhouette_score is : 0.3437053439455249 For n_clusters = 6 The average silhouette_score is : 0.313821767576001 . Make three clusters . Since we have a high Davies Boldin and low Silhoutte score for k=3 we select to cluster into three clusters. Another option could be to use the Gaussian Likelihood score. In this notebook another analysis reported also 3 clusters. . km = KMeans(n_clusters=3, random_state=1301).fit(X_scaled) preds = km.predict(X_scaled) pd.Series(preds).value_counts() # How many wines are in each cluster ? . 0 1801 1 1629 2 1468 dtype: int64 . df_km = pd.DataFrame(data={&#39;pca1&#39;:X_pca[:,0], &#39;pca2&#39;:X_pca [:,1], &#39;cluster&#39;:preds}) . # Add the scaled data with the input features for i,c in enumerate(all_features): df_km[c] = X_scaled[:,i] . domain = [0, 1, 2] range_ = [&#39;red&#39;, &#39;darkblue&#39;, &#39;green&#39;] selection = alt.selection_multi(fields=[&#39;cluster&#39;], bind=&#39;legend&#39;) pca = alt.Chart(df_km).mark_circle(size=20).encode( x=&#39;pca1&#39;, y=&#39;pca2&#39;, color=alt.Color(&#39;cluster:N&#39;, scale=alt.Scale(domain=domain, range=range_)), opacity=alt.condition(selection, alt.value(1), alt.value(0.1)), tooltip = list(all_features) ).add_selection( selection ) pca . Now we will plot the all the wines on a two dimensional plane. On the right, you get boxplots for every feature. The cool thing is now that with the mouse you can select by drawing a brush over the points (click, hold button and drag the mouse) and get immediate updates boxplots of the features of the selected wines. A such you can interactively gain some insight in what the cluster might mean. . brush = alt.selection(type=&#39;interval&#39;) domain = [0, 1, 2] range_ = [&#39;red&#39;, &#39;darkblue&#39;, &#39;green&#39;] points = alt.Chart(df_km).mark_circle(size=60).encode( x=&#39;pca1&#39;, y=&#39;pca2&#39;, color = alt.condition(brush, &#39;cluster:N&#39;, alt.value(&#39;lightgray&#39;)), tooltip = list(all_features) ).add_selection(brush) boxplots = alt.vconcat() for measure in all_features: boxplot = alt.Chart(df_km).mark_boxplot().encode( x =alt.X(measure, axis=alt.Axis(titleX=470, titleY=0)), ).transform_filter( brush ) boxplots &amp;= boxplot chart = alt.hconcat(points, boxplots) # chart.save(&#39;cluster_pca_n_3.html&#39;)) chart . AS you can&#39;t select al wine from one cluster with the rectangular brush, we make a plot for each cluster and boxplots of the features values of that cluster. . def plot_cluster(df, selected_columns, clusternr): points = alt.Chart(df).mark_circle(size=60).encode( x=alt.X(&#39;pca1&#39;, title=&#39;Principal component 1 (pca1)&#39;), y=alt.Y(&#39;pca2&#39;, title=&#39;Principal component 2 (pca1)&#39;), color = alt.condition(alt.FieldEqualPredicate(field=&#39;cluster&#39;, equal=clusternr), &#39;cluster:N&#39;, alt.value(&#39;lightgray&#39;)), tooltip = list(all_features)+[&#39;cluster&#39;], ) boxplots = alt.vconcat() for measure in [c for c in selected_columns]: boxplot = alt.Chart(df).mark_boxplot().encode( x =alt.X(measure, axis=alt.Axis(titleX=480, titleY=0)), ).transform_filter( alt.FieldEqualPredicate(field=&#39;cluster&#39;, equal=clusternr) ) boxplots &amp;= boxplot return points, boxplots . points, boxplots = plot_cluster(df_km, all_features, 0) c0 = alt.hconcat(points, boxplots).properties(title=&#39;Cluster 0&#39;) points, boxplots = plot_cluster(df_km, all_features, 1) c1 = alt.hconcat(points, boxplots).properties(title=&#39;Cluster 1&#39;) points, boxplots = plot_cluster(df_km, all_features, 2) c2 = alt.hconcat(points, boxplots).properties(title=&#39;Cluster 2&#39;) . alt.vconcat(c0, c1, c2) . I might still be difficult to explain the clusters. We will now build a multiclass classifier to predict the cluster from the features. Next we will use the Shapley values to explain the clusters. . Light gbm classifier . import lightgbm as lgb . params = lgb.LGBMClassifier().get_params() params . {&#39;boosting_type&#39;: &#39;gbdt&#39;, &#39;class_weight&#39;: None, &#39;colsample_bytree&#39;: 1.0, &#39;importance_type&#39;: &#39;split&#39;, &#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: -1, &#39;min_child_samples&#39;: 20, &#39;min_child_weight&#39;: 0.001, &#39;min_split_gain&#39;: 0.0, &#39;n_estimators&#39;: 100, &#39;n_jobs&#39;: -1, &#39;num_leaves&#39;: 31, &#39;objective&#39;: None, &#39;random_state&#39;: None, &#39;reg_alpha&#39;: 0.0, &#39;reg_lambda&#39;: 0.0, &#39;silent&#39;: True, &#39;subsample&#39;: 1.0, &#39;subsample_for_bin&#39;: 200000, &#39;subsample_freq&#39;: 0} . params[&#39;objective&#39;] = &#39;multiclass&#39; # the target to predict is the number of the cluster params[&#39;is_unbalance&#39;] = True params[&#39;n_jobs&#39;] = -1 params[&#39;random_state&#39;] = 1301 . mdl = lgb.LGBMClassifier(**params) . X = df.drop(columns=[&#39;quality&#39;, &#39;pca1&#39;, &#39;pca2&#39;, &#39;member&#39;]) . mdl.fit(X, preds) . LGBMClassifier(boosting_type=&#39;gbdt&#39;, class_weight=None, colsample_bytree=1.0, importance_type=&#39;split&#39;, is_unbalance=True, learning_rate=0.1, max_depth=-1, min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0, n_estimators=100, n_jobs=-1, num_leaves=31, objective=&#39;multiclass&#39;, random_state=1301, reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0, subsample_for_bin=200000, subsample_freq=0) . y_pred = mdl.predict_proba(X) . # Install the shap library to caculate the Shapley values !pip install shap . Requirement already satisfied: shap in /usr/local/lib/python3.6/dist-packages (0.35.0) Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from shap) (1.0.3) Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from shap) (0.22.2.post1) Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from shap) (1.4.1) Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from shap) (1.18.2) Requirement already satisfied: tqdm&gt;4.25.0 in /usr/local/lib/python3.6/dist-packages (from shap) (4.38.0) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas-&gt;shap) (2018.9) Requirement already satisfied: python-dateutil&gt;=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas-&gt;shap) (2.8.1) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn-&gt;shap) (0.14.1) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil&gt;=2.6.1-&gt;pandas-&gt;shap) (1.12.0) . import shap . explainer = shap.TreeExplainer(mdl) shap_values = explainer.shap_values(X) . Setting feature_perturbation = &#34;tree_path_dependent&#34; because no background data was given. . shap.summary_plot(shap_values, X, max_display=30) . From the Shapley values we see that the feature density has the highest impact on the model to predict the clusters. Let&#39;s have a look the Shapley values per cluster. The acidity features pH and fixed acidity has only impact on cluster 1 and 2 but almost none on cluster 0. . Shapley values for the three clusters . for cnr in df_km[&#39;cluster&#39;].unique(): shap.summary_plot(shap_values[cnr], X, max_display=30, show=False) plt.title(f&#39;Cluster {cnr}&#39;) plt.show() . To explain the clusters, we will plot the three clusters and the boxplot of the features ordered with the feature importance. . Explain cluster 0 . cnr = 0 feature_order = np.argsort(np.sum(np.abs(shap_values[cnr]), axis=0)) points, boxplots = plot_cluster(df_km, [X.columns[i] for i in feature_order][::-1][:6], 0) c0 = alt.hconcat(points, boxplots).properties(title=&#39;Cluster 0&#39;) c0 . cnr = 0 shap.summary_plot(shap_values[cnr], X, max_display=30, show=False) plt.title(f&#39;Cluster {cnr}&#39;) plt.show() . Cluster 0 can be describe as wines with: . high density | high total sulfur dioxide | high free sulfur dioxide | . Explain cluster 1 . X.columns . Index([&#39;fixed acidity&#39;, &#39;volatile acidity&#39;, &#39;citric acid&#39;, &#39;residual sugar&#39;, &#39;chlorides&#39;, &#39;free sulfur dioxide&#39;, &#39;total sulfur dioxide&#39;, &#39;density&#39;, &#39;pH&#39;, &#39;sulphates&#39;, &#39;alcohol&#39;, &#39;quality_weight&#39;], dtype=&#39;object&#39;) . cnr = 1 feature_order = np.argsort(np.sum(np.abs(shap_values[cnr]), axis=0)) points, boxplots = plot_cluster(df_km, [X.columns[i] for i in feature_order][::-1][:6], cnr) c1 = alt.hconcat(points, boxplots).properties(title=f&#39;Cluster {cnr}&#39;) c1 . cnr = 1 shap.summary_plot(shap_values[cnr], X, max_display=30, show=False) plt.title(f&#39;Cluster {cnr}&#39;) plt.show() . Cluster 1 contains wines with: . high pH | low fixed acidity | low density (opposite of cluster 0) | low citric acid | high on sulphates | . Explain cluster 2 . cnr = 2 feature_order = np.argsort(np.sum(np.abs(shap_values[cnr]), axis=0)) points, boxplots = plot_cluster(df_km, [X.columns[i] for i in feature_order][::-1][:6], cnr) c2 = alt.hconcat(points, boxplots).properties(title=f&#39;Cluster {cnr}&#39;) c2 . cnr = 2 shap.summary_plot(shap_values[cnr], X, max_display=30, show=False) plt.title(f&#39;Cluster {cnr}&#39;) plt.show() . Cluster 2 contains wines with: . high fixed acidity | low pH (opposite of cluster 1) | low density (opposite of cluster 0) | . How is the quality of the whines in each cluster ? . df_km[&#39;quality&#39;] = y df_km.groupby(&#39;cluster&#39;)[&#39;quality&#39;].describe() . count mean std min 25% 50% 75% max . cluster . 0 1801.0 | 5.606330 | 0.750514 | 3.0 | 5.0 | 6.0 | 6.0 | 8.0 | . 1 1629.0 | 6.150399 | 0.905265 | 3.0 | 6.0 | 6.0 | 7.0 | 9.0 | . 2 1468.0 | 5.908719 | 0.918554 | 3.0 | 5.0 | 6.0 | 6.0 | 9.0 | . # Let&#39;s plot the distributions of quality score per cluster alt.Chart(df_km).transform_density( density=&#39;quality&#39;, bandwidth=0.3, groupby=[&#39;cluster&#39;], extent= [3, 9], counts = True, steps=200 ).mark_area().encode( alt.X(&#39;value:Q&#39;), alt.Y(&#39;density:Q&#39;, stack=&#39;zero&#39;), alt.Color(&#39;cluster:N&#39;) ).properties( title=&#39;Distribution of quality of the wines per cluster&#39;, width=400, height=100 ) . We see that the quality has a similar distribution in each cluster. .",
            "url": "https://cast42.github.io/blog/datascience/python/clustering/altair/shap/2020/04/23/explain-clusters-to-business.html",
            "relUrl": "/datascience/python/clustering/altair/shap/2020/04/23/explain-clusters-to-business.html",
            "date": " • Apr 23, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Regional covid-19 mortality in Belgium per gender and age",
            "content": "# Import pandas for data wrangling and Altair for plotting import pandas as pd import altair as alt . df_tot_sc = pd.read_excel(&#39;https://epistat.sciensano.be/Data/COVID19BE.xlsx&#39;) . df_inhab = pd.read_excel(&#39;https://statbel.fgov.be/sites/default/files/files/opendata/bevolking%20naar%20woonplaats%2C%20nationaliteit%20burgelijke%20staat%20%2C%20leeftijd%20en%20geslacht/TF_SOC_POP_STRUCT_2019.xlsx&#39;) . df_inhab . CD_REFNIS TX_DESCR_NL TX_DESCR_FR CD_DSTR_REFNIS TX_ADM_DSTR_DESCR_NL TX_ADM_DSTR_DESCR_FR CD_PROV_REFNIS TX_PROV_DESCR_NL TX_PROV_DESCR_FR CD_RGN_REFNIS TX_RGN_DESCR_NL TX_RGN_DESCR_FR CD_SEX CD_NATLTY TX_NATLTY_NL TX_NATLTY_FR CD_CIV_STS TX_CIV_STS_NL TX_CIV_STS_FR CD_AGE MS_POPULATION . 0 11001 | Aartselaar | Aartselaar | 11000 | Arrondissement Antwerpen | Arrondissement d’Anvers | 10000.0 | Provincie Antwerpen | Province d’Anvers | 2000 | Vlaams Gewest | Région flamande | F | BEL | Belgen | Belges | 4 | Gescheiden | Divorcé | 69 | 11 | . 1 11001 | Aartselaar | Aartselaar | 11000 | Arrondissement Antwerpen | Arrondissement d’Anvers | 10000.0 | Provincie Antwerpen | Province d’Anvers | 2000 | Vlaams Gewest | Région flamande | F | BEL | Belgen | Belges | 4 | Gescheiden | Divorcé | 80 | 3 | . 2 11001 | Aartselaar | Aartselaar | 11000 | Arrondissement Antwerpen | Arrondissement d’Anvers | 10000.0 | Provincie Antwerpen | Province d’Anvers | 2000 | Vlaams Gewest | Région flamande | M | BEL | Belgen | Belges | 4 | Gescheiden | Divorcé | 30 | 2 | . 3 11001 | Aartselaar | Aartselaar | 11000 | Arrondissement Antwerpen | Arrondissement d’Anvers | 10000.0 | Provincie Antwerpen | Province d’Anvers | 2000 | Vlaams Gewest | Région flamande | F | BEL | Belgen | Belges | 4 | Gescheiden | Divorcé | 48 | 26 | . 4 11001 | Aartselaar | Aartselaar | 11000 | Arrondissement Antwerpen | Arrondissement d’Anvers | 10000.0 | Provincie Antwerpen | Province d’Anvers | 2000 | Vlaams Gewest | Région flamande | F | BEL | Belgen | Belges | 4 | Gescheiden | Divorcé | 76 | 2 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 463376 93090 | Viroinval | Viroinval | 93000 | Arrondissement Philippeville | Arrondissement de Philippeville | 90000.0 | Provincie Namen | Province de Namur | 3000 | Waals Gewest | Région wallonne | F | BEL | Belgen | Belges | 3 | Weduwstaat | Veuf | 73 | 10 | . 463377 93090 | Viroinval | Viroinval | 93000 | Arrondissement Philippeville | Arrondissement de Philippeville | 90000.0 | Provincie Namen | Province de Namur | 3000 | Waals Gewest | Région wallonne | M | BEL | Belgen | Belges | 3 | Weduwstaat | Veuf | 64 | 1 | . 463378 93090 | Viroinval | Viroinval | 93000 | Arrondissement Philippeville | Arrondissement de Philippeville | 90000.0 | Provincie Namen | Province de Namur | 3000 | Waals Gewest | Région wallonne | M | BEL | Belgen | Belges | 3 | Weduwstaat | Veuf | 86 | 3 | . 463379 93090 | Viroinval | Viroinval | 93000 | Arrondissement Philippeville | Arrondissement de Philippeville | 90000.0 | Provincie Namen | Province de Namur | 3000 | Waals Gewest | Région wallonne | M | ETR | niet-Belgen | non-Belges | 3 | Weduwstaat | Veuf | 74 | 1 | . 463380 93090 | Viroinval | Viroinval | 93000 | Arrondissement Philippeville | Arrondissement de Philippeville | 90000.0 | Provincie Namen | Province de Namur | 3000 | Waals Gewest | Région wallonne | M | BEL | Belgen | Belges | 3 | Weduwstaat | Veuf | 52 | 1 | . 463381 rows × 21 columns . inhab_provence = df_inhab[&#39;TX_PROV_DESCR_NL&#39;].dropna().unique() inhab_provence . array([&#39;Provincie Antwerpen&#39;, &#39;Provincie Vlaams-Brabant&#39;, &#39;Provincie Waals-Brabant&#39;, &#39;Provincie West-Vlaanderen&#39;, &#39;Provincie Oost-Vlaanderen&#39;, &#39;Provincie Henegouwen&#39;, &#39;Provincie Luik&#39;, &#39;Provincie Limburg&#39;, &#39;Provincie Luxemburg&#39;, &#39;Provincie Namen&#39;], dtype=object) . sc_provence = df_tot_sc[&#39;PROVINCE&#39;].unique() sc_provence . array([&#39;Brussels&#39;, &#39;Liège&#39;, &#39;Limburg&#39;, &#39;OostVlaanderen&#39;, &#39;VlaamsBrabant&#39;, &#39;Antwerpen&#39;, &#39;WestVlaanderen&#39;, &#39;BrabantWallon&#39;, &#39;Hainaut&#39;, &#39;Namur&#39;, nan, &#39;Luxembourg&#39;], dtype=object) . [p.split()[1] for p in inhab_provence] . [&#39;Antwerpen&#39;, &#39;Vlaams-Brabant&#39;, &#39;Waals-Brabant&#39;, &#39;West-Vlaanderen&#39;, &#39;Oost-Vlaanderen&#39;, &#39;Henegouwen&#39;, &#39;Luik&#39;, &#39;Limburg&#39;, &#39;Luxemburg&#39;, &#39;Namen&#39;] . map_statbel_provence_to_sc_provence = {&#39;Provincie Antwerpen&#39;:&#39;Antwerpen&#39;, &#39;Provincie Vlaams-Brabant&#39;:&#39;VlaamsBrabant&#39;, &#39;Provincie Waals-Brabant&#39;:&#39;BrabantWallon&#39;, &#39;Provincie West-Vlaanderen&#39;:&#39;WestVlaanderen&#39;, &#39;Provincie Oost-Vlaanderen&#39;:&#39;OostVlaanderen&#39;, &#39;Provincie Henegouwen&#39;:&#39;Hainaut&#39;, &#39;Provincie Luik&#39;:&#39;Liège&#39;, &#39;Provincie Limburg&#39;:&#39;Limburg&#39;, &#39;Provincie Luxemburg&#39;:&#39;Luxembourg&#39;, &#39;Provincie Namen&#39;:&#39;Namur&#39;} . df_inhab[&#39;sc_provence&#39;] = df_inhab[&#39;TX_PROV_DESCR_NL&#39;].map(map_statbel_provence_to_sc_provence) . df_tot_sc[&#39;AGEGROUP&#39;].unique() . array([&#39;10-19&#39;, &#39;20-29&#39;, &#39;30-39&#39;, &#39;40-49&#39;, &#39;50-59&#39;, &#39;70-79&#39;, &#39;60-69&#39;, &#39;0-9&#39;, &#39;90+&#39;, &#39;80-89&#39;, nan], dtype=object) . df_inhab[&#39;AGEGROUP&#39;] =pd.cut(df_inhab[&#39;CD_AGE&#39;], bins=[0,10,20,30,40,50,60,70,80,90,200], labels=[&#39;0-9&#39;,&#39;10-19&#39;,&#39;20-29&#39;,&#39;30-39&#39;,&#39;40-49&#39;,&#39;50-59&#39;,&#39;60-69&#39;,&#39;70-79&#39;,&#39;80-89&#39;,&#39;90+&#39;], include_lowest=True) . df_inhab_gender_prov = df_inhab.groupby([&#39;sc_provence&#39;, &#39;CD_SEX&#39;, &#39;AGEGROUP&#39;])[&#39;MS_POPULATION&#39;].sum().reset_index() . df_inhab_gender_prov_cases = pd.merge(df_inhab_gender_prov, df_tot_sc.dropna(), left_on=[&#39;sc_provence&#39;, &#39;AGEGROUP&#39;, &#39;CD_SEX&#39;], right_on=[&#39;PROVINCE&#39;, &#39;AGEGROUP&#39;, &#39;SEX&#39;]) . df_inhab_gender_prov_cases.head() . sc_provence CD_SEX AGEGROUP MS_POPULATION DATE PROVINCE REGION SEX CASES . 0 Antwerpen | F | 0-9 | 113851 | 2020-03-05 | Antwerpen | Flanders | F | 1 | . 1 Antwerpen | F | 0-9 | 113851 | 2020-03-18 | Antwerpen | Flanders | F | 1 | . 2 Antwerpen | F | 0-9 | 113851 | 2020-03-26 | Antwerpen | Flanders | F | 1 | . 3 Antwerpen | F | 0-9 | 113851 | 2020-03-30 | Antwerpen | Flanders | F | 1 | . 4 Antwerpen | F | 0-9 | 113851 | 2020-04-03 | Antwerpen | Flanders | F | 1 | . df_plot = df_inhab_gender_prov_cases.groupby([&#39;SEX&#39;, &#39;AGEGROUP&#39;, &#39;PROVINCE&#39;]).agg(CASES = (&#39;CASES&#39;, &#39;sum&#39;), MS_POPULATION=(&#39;MS_POPULATION&#39;, &#39;first&#39;)).reset_index() df_plot . SEX AGEGROUP PROVINCE CASES MS_POPULATION . 0 F | 0-9 | Antwerpen | 9 | 113851 | . 1 F | 0-9 | BrabantWallon | 3 | 23744 | . 2 F | 0-9 | Hainaut | 11 | 81075 | . 3 F | 0-9 | Limburg | 11 | 48102 | . 4 F | 0-9 | Liège | 19 | 67479 | . ... ... | ... | ... | ... | ... | . 195 M | 90+ | Luxembourg | 17 | 469 | . 196 M | 90+ | Namur | 27 | 827 | . 197 M | 90+ | OostVlaanderen | 102 | 3105 | . 198 M | 90+ | VlaamsBrabant | 129 | 2611 | . 199 M | 90+ | WestVlaanderen | 121 | 3292 | . 200 rows × 5 columns . df_plot[&#39;PROVINCE&#39;].unique() . array([&#39;Antwerpen&#39;, &#39;BrabantWallon&#39;, &#39;Hainaut&#39;, &#39;Limburg&#39;, &#39;Liège&#39;, &#39;Luxembourg&#39;, &#39;Namur&#39;, &#39;OostVlaanderen&#39;, &#39;VlaamsBrabant&#39;, &#39;WestVlaanderen&#39;], dtype=object) . alt.Chart(df_plot).mark_bar().encode(x=&#39;AGEGROUP:N&#39;, y=&#39;CASES&#39;, color=&#39;SEX:N&#39;, column=&#39;PROVINCE:N&#39;) . df_plot[&#39;percentage&#39;] = df_plot[&#39;CASES&#39;] / df_plot[&#39;MS_POPULATION&#39;] . alt.Chart(df_plot).mark_bar().encode(x=&#39;AGEGROUP:N&#39;, y=&#39;percentage&#39;, color=&#39;SEX:N&#39;, column=&#39;PROVINCE:N&#39;) . Let&#39;s add a colorscale the makes the male blue and female number pink. . color_scale = alt.Scale(domain=[&#39;M&#39;, &#39;F&#39;], range=[&#39;#1f77b4&#39;, &#39;#e377c2&#39;]) . alt.Chart(df_plot).mark_bar().encode( x=&#39;AGEGROUP:N&#39;, y=&#39;percentage&#39;, color=alt.Color(&#39;SEX:N&#39;, scale=color_scale, legend=None), column=&#39;PROVINCE:N&#39;) . The graph&#39;s get to wide. Let&#39;s use faceting to make two rows. . Inspired and based on https://altair-viz.github.io/gallery/us_population_pyramid_over_time.html . #slider = alt.binding_range(min=1850, max=2000, step=10) # select_province = alt.selection_single(name=&#39;PROVINCE&#39;, fields=[&#39;PROVINCE&#39;], # bind=slider, init={&#39;PROVINCE&#39;: &#39;Antwerpen&#39;}) color_scale = alt.Scale(domain=[&#39;Male&#39;, &#39;Female&#39;], range=[&#39;#1f77b4&#39;, &#39;#e377c2&#39;]) select_province = alt.selection_multi(fields=[&#39;PROVINCE&#39;], bind=&#39;legend&#39;) base = alt.Chart(df_plot).add_selection( select_province ).transform_filter( select_province ).transform_calculate( gender=alt.expr.if_(alt.datum.SEX == &#39;M&#39;, &#39;Male&#39;, &#39;Female&#39;) ).properties( width=250 ) left = base.transform_filter( alt.datum.gender == &#39;Female&#39; ).encode( y=alt.Y(&#39;AGEGROUP:O&#39;, axis=None), x=alt.X(&#39;percentage:Q&#39;, axis=alt.Axis(format=&#39;.0%&#39;), title=&#39;Percentage&#39;, sort=alt.SortOrder(&#39;descending&#39;), ), color=alt.Color(&#39;gender:N&#39;, scale=color_scale, legend=None), ).mark_bar().properties(title=&#39;Female&#39;) middle = base.encode( y=alt.Y(&#39;AGEGROUP:O&#39;, axis=None), text=alt.Text(&#39;AGEGROUP:O&#39;), ).mark_text().properties(width=20) right = base.transform_filter( alt.datum.gender == &#39;Male&#39; ).encode( y=alt.Y(&#39;AGEGROUP:O&#39;, axis=None), x=alt.X(&#39;percentage:Q&#39;, title=&#39;Percentage&#39;, axis=alt.Axis(format=&#39;.0%&#39;),), color=alt.Color(&#39;gender:N&#39;, scale=color_scale, legend=None) ).mark_bar().properties(title=&#39;Male&#39;) # legend = alt.Chart(df_plot).mark_text().encode( # y=alt.Y(&#39;PROVINCE:N&#39;, axis=None), # text=alt.Text(&#39;PROVINCE:N&#39;), # color=alt.Color(&#39;PROVINCE:N&#39;, legend=alt.Legend(title=&quot;Provincie&quot;)) # ) alt.concat(left, middle, right, spacing=5) #legend=alt.Legend(title=&quot;Species by color&quot;) . provinces = df_plot[&#39;PROVINCE&#39;].unique() select_province = alt.selection_single( name=&#39;Select&#39;, # name the selection &#39;Select&#39; fields=[&#39;PROVINCE&#39;], # limit selection to the Major_Genre field init={&#39;PROVINCE&#39;: &#39;Antwerpen&#39;}, # use first genre entry as initial value bind=alt.binding_select(options=provinces) # bind to a menu of unique provence values ) base = alt.Chart(df_plot).add_selection( select_province ).transform_filter( select_province ).transform_calculate( gender=alt.expr.if_(alt.datum.SEX == &#39;M&#39;, &#39;Male&#39;, &#39;Female&#39;) ).properties( width=250 ) left = base.transform_filter( alt.datum.gender == &#39;Female&#39; ).encode( y=alt.Y(&#39;AGEGROUP:O&#39;, axis=None), x=alt.X(&#39;percentage:Q&#39;, axis=alt.Axis(format=&#39;.0%&#39;), title=&#39;Percentage&#39;, sort=alt.SortOrder(&#39;descending&#39;), scale=alt.Scale(domain=(0.0, 0.1), clamp=True) ), color=alt.Color(&#39;gender:N&#39;, scale=color_scale, legend=None), tooltip=[alt.Tooltip(&#39;percentage&#39;, format=&#39;.1%&#39;)] ).mark_bar().properties(title=&#39;Female&#39;) middle = base.encode( y=alt.Y(&#39;AGEGROUP:O&#39;, axis=None), text=alt.Text(&#39;AGEGROUP:O&#39;), ).mark_text().properties(width=20) right = base.transform_filter( alt.datum.gender == &#39;Male&#39; ).encode( y=alt.Y(&#39;AGEGROUP:O&#39;, axis=None), x=alt.X(&#39;percentage:Q&#39;, title=&#39;Percentage&#39;, axis=alt.Axis(format=&#39;.1%&#39;), scale=alt.Scale(domain=(0.0, 0.1), clamp=True)), color=alt.Color(&#39;gender:N&#39;, scale=color_scale, legend=None), tooltip=[alt.Tooltip(&#39;percentage&#39;, format=&#39;.1%&#39;)] ).mark_bar().properties(title=&#39;Male&#39;) alt.concat(left, middle, right, spacing=5).properties(title=&#39;Percentage of covid-19 cases per province, gender and age grup in Belgium&#39;) . Mortality . # https://epistat.wiv-isp.be/covid/ # Dataset of mortality by date, age, sex, and region df_dead_sc = pd.read_csv(&#39;https://epistat.sciensano.be/Data/COVID19BE_MORT.csv&#39;) . df_dead_sc.head() . DATE REGION AGEGROUP SEX DEATHS . 0 2020-03-10 | Brussels | 85+ | F | 1 | . 1 2020-03-11 | Flanders | 85+ | F | 1 | . 2 2020-03-11 | Brussels | 75-84 | M | 1 | . 3 2020-03-11 | Brussels | 85+ | F | 1 | . 4 2020-03-12 | Brussels | 75-84 | M | 1 | . df_dead_sc[&#39;REGION&#39;].value_counts() . Wallonia 291 Flanders 275 Brussels 271 Name: REGION, dtype: int64 . df_dead_sc[&#39;AGEGROUP&#39;].value_counts() . 85+ 223 75-84 205 65-74 179 45-64 132 25-44 19 0-24 1 Name: AGEGROUP, dtype: int64 . df_inhab[&#39;AGEGROUP_sc&#39;] =pd.cut(df_inhab[&#39;CD_AGE&#39;], bins=[0,24,44,64,74,84,200], labels=[&#39;0-24&#39;,&#39;25-44&#39;,&#39;45-64&#39;,&#39;65-74&#39;,&#39;75-84&#39;,&#39;85+&#39;], include_lowest=True) . df_inhab.groupby(&#39;AGEGROUP_sc&#39;).agg(lowest_age=(&#39;CD_AGE&#39;, &#39;min&#39;), highest_age=(&#39;CD_AGE&#39;, max)) . lowest_age highest_age . AGEGROUP_sc . 0-24 0 | 24 | . 25-44 25 | 44 | . 45-64 45 | 64 | . 65-74 65 | 74 | . 75-84 75 | 84 | . 85+ 85 | 110 | . df_inhab.head() . CD_REFNIS TX_DESCR_NL TX_DESCR_FR CD_DSTR_REFNIS TX_ADM_DSTR_DESCR_NL TX_ADM_DSTR_DESCR_FR CD_PROV_REFNIS TX_PROV_DESCR_NL TX_PROV_DESCR_FR CD_RGN_REFNIS TX_RGN_DESCR_NL TX_RGN_DESCR_FR CD_SEX CD_NATLTY TX_NATLTY_NL TX_NATLTY_FR CD_CIV_STS TX_CIV_STS_NL TX_CIV_STS_FR CD_AGE MS_POPULATION sc_provence AGEGROUP AGEGROUP_sc . 0 11001 | Aartselaar | Aartselaar | 11000 | Arrondissement Antwerpen | Arrondissement d’Anvers | 10000.0 | Provincie Antwerpen | Province d’Anvers | 2000 | Vlaams Gewest | Région flamande | F | BEL | Belgen | Belges | 4 | Gescheiden | Divorcé | 69 | 11 | Antwerpen | 60-69 | 65-74 | . 1 11001 | Aartselaar | Aartselaar | 11000 | Arrondissement Antwerpen | Arrondissement d’Anvers | 10000.0 | Provincie Antwerpen | Province d’Anvers | 2000 | Vlaams Gewest | Région flamande | F | BEL | Belgen | Belges | 4 | Gescheiden | Divorcé | 80 | 3 | Antwerpen | 70-79 | 75-84 | . 2 11001 | Aartselaar | Aartselaar | 11000 | Arrondissement Antwerpen | Arrondissement d’Anvers | 10000.0 | Provincie Antwerpen | Province d’Anvers | 2000 | Vlaams Gewest | Région flamande | M | BEL | Belgen | Belges | 4 | Gescheiden | Divorcé | 30 | 2 | Antwerpen | 20-29 | 25-44 | . 3 11001 | Aartselaar | Aartselaar | 11000 | Arrondissement Antwerpen | Arrondissement d’Anvers | 10000.0 | Provincie Antwerpen | Province d’Anvers | 2000 | Vlaams Gewest | Région flamande | F | BEL | Belgen | Belges | 4 | Gescheiden | Divorcé | 48 | 26 | Antwerpen | 40-49 | 45-64 | . 4 11001 | Aartselaar | Aartselaar | 11000 | Arrondissement Antwerpen | Arrondissement d’Anvers | 10000.0 | Provincie Antwerpen | Province d’Anvers | 2000 | Vlaams Gewest | Région flamande | F | BEL | Belgen | Belges | 4 | Gescheiden | Divorcé | 76 | 2 | Antwerpen | 70-79 | 75-84 | . df_dead_sc[&#39;REGION&#39;].unique() . array([&#39;Brussels&#39;, &#39;Flanders&#39;, &#39;Wallonia&#39;], dtype=object) . df_inhab[&#39;TX_RGN_DESCR_NL&#39;].value_counts() . Vlaams Gewest 242865 Waals Gewest 199003 Brussels Hoofdstedelijk Gewest 21513 Name: TX_RGN_DESCR_NL, dtype: int64 . df_inhab_gender_prov = df_inhab.groupby([&#39;TX_RGN_DESCR_NL&#39;, &#39;CD_SEX&#39;, &#39;AGEGROUP_sc&#39;])[&#39;MS_POPULATION&#39;].sum().reset_index() . region_sc_to_region_inhad = {&#39;Flanders&#39;:&#39;Vlaams Gewest&#39;, &#39;Wallonia&#39;:&#39;Waals Gewest&#39;, &#39;Brussels&#39;:&#39;Brussels Hoofdstedelijk Gewest&#39;} . df_dead_sc[&#39;TX_RGN_DESCR_NL&#39;] = df_dead_sc[&#39;REGION&#39;].map(region_sc_to_region_inhad) . df_dead_sc.groupby([&#39;TX_RGN_DESCR_NL&#39;, &#39;AGEGROUP&#39;, &#39;SEX&#39;])[&#39;DEATHS&#39;].sum() . TX_RGN_DESCR_NL AGEGROUP SEX Brussels Hoofdstedelijk Gewest 25-44 F 1 M 4 45-64 F 21 M 43 65-74 F 42 M 71 75-84 F 128 M 170 85+ F 270 M 186 Vlaams Gewest 0-24 F 1 25-44 F 2 M 3 45-64 F 27 M 63 65-74 F 67 M 130 75-84 F 199 M 335 85+ F 232 M 309 Waals Gewest 25-44 F 5 M 4 45-64 F 41 M 89 65-74 F 98 M 186 75-84 F 290 M 300 85+ F 704 M 421 Name: DEATHS, dtype: int64 . df_dead_sc_region_agegroup_gender = df_dead_sc.groupby([&#39;TX_RGN_DESCR_NL&#39;, &#39;AGEGROUP&#39;, &#39;SEX&#39;])[&#39;DEATHS&#39;].sum().reset_index() . df_inhab_gender_prov_deaths = pd.merge(df_inhab_gender_prov, df_dead_sc_region_agegroup_gender, left_on=[&#39;TX_RGN_DESCR_NL&#39;, &#39;AGEGROUP_sc&#39;, &#39;CD_SEX&#39;], right_on=[&#39;TX_RGN_DESCR_NL&#39;, &#39;AGEGROUP&#39;, &#39;SEX&#39;]) . df_inhab_gender_prov_deaths[&#39;MS_POPULATION&#39;].sum() . 9077403 . df_inhab_gender_prov_deaths[&#39;DEATHS&#39;].sum() . 4442 . df_inhab_gender_prov_deaths . TX_RGN_DESCR_NL CD_SEX AGEGROUP_sc MS_POPULATION AGEGROUP SEX DEATHS . 0 Brussels Hoofdstedelijk Gewest | F | 25-44 | 197579 | 25-44 | F | 1 | . 1 Brussels Hoofdstedelijk Gewest | F | 45-64 | 137628 | 45-64 | F | 21 | . 2 Brussels Hoofdstedelijk Gewest | F | 65-74 | 45214 | 65-74 | F | 42 | . 3 Brussels Hoofdstedelijk Gewest | F | 75-84 | 30059 | 75-84 | F | 128 | . 4 Brussels Hoofdstedelijk Gewest | F | 85+ | 18811 | 85+ | F | 270 | . 5 Brussels Hoofdstedelijk Gewest | M | 25-44 | 194988 | 25-44 | M | 4 | . 6 Brussels Hoofdstedelijk Gewest | M | 45-64 | 140348 | 45-64 | M | 43 | . 7 Brussels Hoofdstedelijk Gewest | M | 65-74 | 36698 | 65-74 | M | 71 | . 8 Brussels Hoofdstedelijk Gewest | M | 75-84 | 19969 | 75-84 | M | 170 | . 9 Brussels Hoofdstedelijk Gewest | M | 85+ | 7918 | 85+ | M | 186 | . 10 Vlaams Gewest | F | 0-24 | 874891 | 0-24 | F | 1 | . 11 Vlaams Gewest | F | 25-44 | 820036 | 25-44 | F | 2 | . 12 Vlaams Gewest | F | 45-64 | 901554 | 45-64 | F | 27 | . 13 Vlaams Gewest | F | 65-74 | 353925 | 65-74 | F | 67 | . 14 Vlaams Gewest | F | 75-84 | 245981 | 75-84 | F | 199 | . 15 Vlaams Gewest | F | 85+ | 132649 | 85+ | F | 232 | . 16 Vlaams Gewest | M | 25-44 | 827281 | 25-44 | M | 3 | . 17 Vlaams Gewest | M | 45-64 | 917008 | 45-64 | M | 63 | . 18 Vlaams Gewest | M | 65-74 | 336242 | 65-74 | M | 130 | . 19 Vlaams Gewest | M | 75-84 | 193576 | 75-84 | M | 335 | . 20 Vlaams Gewest | M | 85+ | 69678 | 85+ | M | 309 | . 21 Waals Gewest | F | 25-44 | 457356 | 25-44 | F | 5 | . 22 Waals Gewest | F | 45-64 | 496668 | 45-64 | F | 41 | . 23 Waals Gewest | F | 65-74 | 199422 | 65-74 | F | 98 | . 24 Waals Gewest | F | 75-84 | 118224 | 75-84 | F | 290 | . 25 Waals Gewest | F | 85+ | 68502 | 85+ | F | 704 | . 26 Waals Gewest | M | 25-44 | 459444 | 25-44 | M | 4 | . 27 Waals Gewest | M | 45-64 | 487322 | 45-64 | M | 89 | . 28 Waals Gewest | M | 65-74 | 175508 | 65-74 | M | 186 | . 29 Waals Gewest | M | 75-84 | 82876 | 75-84 | M | 300 | . 30 Waals Gewest | M | 85+ | 30048 | 85+ | M | 421 | . df_inhab_gender_prov_deaths[&#39;percentage&#39;] = df_inhab_gender_prov_deaths[&#39;DEATHS&#39;]/df_inhab_gender_prov_deaths[&#39;MS_POPULATION&#39;] . df_plot = df_inhab_gender_prov_deaths . regions = df_plot[&#39;TX_RGN_DESCR_NL&#39;].unique() select_province = alt.selection_single( name=&#39;Select&#39;, # name the selection &#39;Select&#39; fields=[&#39;TX_RGN_DESCR_NL&#39;], # limit selection to the Major_Genre field init={&#39;TX_RGN_DESCR_NL&#39;: &#39;Vlaams Gewest&#39;}, # use first genre entry as initial value bind=alt.binding_select(options=regions) # bind to a menu of unique provence values ) base = alt.Chart(df_plot).add_selection( select_province ).transform_filter( select_province ).transform_calculate( gender=alt.expr.if_(alt.datum.SEX == &#39;M&#39;, &#39;Male&#39;, &#39;Female&#39;) ).properties( width=250 ) left = base.transform_filter( alt.datum.gender == &#39;Female&#39; ).encode( y=alt.Y(&#39;AGEGROUP:O&#39;, axis=None), x=alt.X(&#39;percentage:Q&#39;, axis=alt.Axis(format=&#39;.2%&#39;), title=&#39;Percentage&#39;, sort=alt.SortOrder(&#39;descending&#39;), # scale=alt.Scale(domain=(0.0, 0.02), clamp=True) ), color=alt.Color(&#39;gender:N&#39;, scale=color_scale, legend=None), tooltip=[alt.Tooltip(&#39;percentage&#39;, format=&#39;.2%&#39;)] ).mark_bar().properties(title=&#39;Female&#39;) middle = base.encode( y=alt.Y(&#39;AGEGROUP:O&#39;, axis=None), text=alt.Text(&#39;AGEGROUP:O&#39;), ).mark_text().properties(width=20) right = base.transform_filter( alt.datum.gender == &#39;Male&#39; ).encode( y=alt.Y(&#39;AGEGROUP:O&#39;, axis=None), # x=alt.X(&#39;percentage:Q&#39;, title=&#39;Percentage&#39;, axis=alt.Axis(format=&#39;.2%&#39;), scale=alt.Scale(domain=(0.0, 0.02), clamp=True)), x=alt.X(&#39;percentage:Q&#39;, title=&#39;Percentage&#39;, axis=alt.Axis(format=&#39;.2%&#39;)), color=alt.Color(&#39;gender:N&#39;, scale=color_scale, legend=None), tooltip=[alt.Tooltip(&#39;percentage&#39;, format=&#39;.2%&#39;)] ).mark_bar().properties(title=&#39;Male&#39;) alt.concat(left, middle, right, spacing=5).properties(title=&#39;Percentage of covid-19 deaths per province, gender and age group relative to number of inhabitants in Belgium&#39;) .",
            "url": "https://cast42.github.io/blog/cast42/jupyter/covid19/belgium/altair/2020/04/22/regional-covid19-mortality-belgium.html",
            "relUrl": "/cast42/jupyter/covid19/belgium/altair/2020/04/22/regional-covid19-mortality-belgium.html",
            "date": " • Apr 22, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Daily covid-19 Deaths compared to average deaths the last 10 years",
            "content": "# Import pandas for data wrangling and Altair for plotting import pandas as pd import altair as alt . The number of deadths per day from 2008 until 2018 can obtained from Statbel, the Belgium federal bureau of statistics: . df = pd.read_excel(&#39;https://statbel.fgov.be/sites/default/files/files/opendata/bevolking/TF_DEATHS.xlsx&#39;) # , skiprows=5, sheet_name=sheetnames . # Get a quick look to the data df.head() . DT_DATE MS_NUM_DEATHS . 0 2008-01-01 | 342 | . 1 2008-01-02 | 348 | . 2 2008-01-03 | 340 | . 3 2008-01-04 | 349 | . 4 2008-01-05 | 348 | . df[&#39;Jaar&#39;] = df[&#39;DT_DATE&#39;].dt.year df[&#39;Dag&#39;] = df[&#39;DT_DATE&#39;].dt.dayofyear . df_plot = df.groupby(&#39;Dag&#39;)[&#39;MS_NUM_DEATHS&#39;].mean().to_frame().reset_index() . # Let&#39;s make a quick plot alt.Chart(df_plot).mark_line().encode(x=&#39;Dag&#39;, y=&#39;MS_NUM_DEATHS&#39;).properties(width=600) . The John Hopkings University CSSE keeps track of the number of covid-19 deadths per day and country in a github repository: https://github.com/CSSEGISandData/COVID-19. We can easily obtain this data by reading it from github and filter out the cases for Belgium. . deaths_url = &#39;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv&#39; deaths = pd.read_csv(deaths_url, sep=&#39;,&#39;) . Filter out Belgium . deaths_be = deaths[deaths[&#39;Country/Region&#39;] == &#39;Belgium&#39;] . Inspect how the data is stored . deaths_be . Province/State Country/Region Lat Long 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 ... 4/9/20 4/10/20 4/11/20 4/12/20 4/13/20 4/14/20 4/15/20 4/16/20 4/17/20 4/18/20 . 23 NaN | Belgium | 50.8333 | 4.0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 2523 | 3019 | 3346 | 3600 | 3903 | 4157 | 4440 | 4857 | 5163 | 5453 | . 1 rows × 92 columns . Create dateframe for plotting . df_deaths = pd.DataFrame(data={&#39;Datum&#39;:pd.to_datetime(deaths_be.columns[4:]), &#39;Overlijdens&#39;:deaths_be.iloc[0].values[4:]}) . Check for Nan&#39;s . df_deaths[&#39;Overlijdens&#39;].isna().sum() . 0 . We need to do some type convertions. We cast &#39;Overlijdens&#39; to integer. Next, we add the number of the day. . df_deaths[&#39;Overlijdens&#39;] = df_deaths[&#39;Overlijdens&#39;].astype(int) df_deaths[&#39;Dag&#39;] = df_deaths[&#39;Datum&#39;].dt.dayofyear . Plot the data: . dead_2008_2018 = alt.Chart(df_plot).mark_line().encode(x=&#39;Dag&#39;, y=&#39;MS_NUM_DEATHS&#39;) dead_2008_2018 . Calculate the day-by-day change . df_deaths[&#39;Nieuwe covid-19 Sterfgevallen&#39;] = df_deaths[&#39;Overlijdens&#39;].diff() . # Check types df_deaths.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 88 entries, 0 to 87 Data columns (total 4 columns): # Column Non-Null Count Dtype -- -- 0 Datum 88 non-null datetime64[ns] 1 Overlijdens 88 non-null int32 2 Dag 88 non-null int64 3 Nieuwe covid-19 Sterfgevallen 87 non-null float64 dtypes: datetime64[ns](1), float64(1), int32(1), int64(1) memory usage: 2.5 KB . Plot covid-19 deaths in Belgium according to JHU CSSE. The plot shows a tooltip if you hover over the points. . dead_covid= alt.Chart(df_deaths).mark_line(point=True).encode( x=alt.X(&#39;Dag&#39;,scale=alt.Scale(domain=(1, 110), clamp=True)), y=&#39;Nieuwe covid-19 Sterfgevallen&#39;, color=alt.ColorValue(&#39;red&#39;), tooltip=[&#39;Dag&#39;, &#39;Nieuwe covid-19 Sterfgevallen&#39;]) dead_covid . Now we add average deaths per day in the last 10 year to the plot. . dead_2008_2018 + dead_covid . Take quick look to the datatable: . df.head() . DT_DATE MS_NUM_DEATHS Jaar Dag . 0 2008-01-01 | 342 | 2008 | 1 | . 1 2008-01-02 | 348 | 2008 | 2 | . 2 2008-01-03 | 340 | 2008 | 3 | . 3 2008-01-04 | 349 | 2008 | 4 | . 4 2008-01-05 | 348 | 2008 | 5 | . The column &#39;DT_DATE&#39; is a string. We convert it to a datatime so we can add it to the tooltip. . df[&#39;Datum&#39;] = pd.to_datetime(df[&#39;DT_DATE&#39;]) . Now we are prepared to make the final graph. We use the Altair mark_errorband(extend=&#39;ci&#39;) to bootstrap 95% confidence band around the average number of deaths per day. . line = alt.Chart(df).mark_line().encode( x=alt.X(&#39;Dag&#39;, scale=alt.Scale( domain=(1, 120), clamp=True )), y=&#39;mean(MS_NUM_DEATHS)&#39; ) # Bootstrapped 95% confidence interval band = alt.Chart(df).mark_errorband(extent=&#39;ci&#39;).encode( x=alt.X(&#39;Dag&#39;, scale=alt.Scale(domain=(1, 120), clamp=True)), y=alt.Y(&#39;MS_NUM_DEATHS&#39;, title=&#39;Overlijdens per dag&#39;), ) dead_covid= alt.Chart(df_deaths).mark_line(point=True).encode( x=alt.X(&#39;Dag&#39;,scale=alt.Scale(domain=(1, 120), clamp=True)), y=&#39;Nieuwe covid-19 Sterfgevallen&#39;, color=alt.ColorValue(&#39;red&#39;), tooltip=[&#39;Dag&#39;, &#39;Nieuwe covid-19 Sterfgevallen&#39;, &#39;Datum&#39;] ) (band + line + dead_covid).properties(width=1024, title=&#39;Gemiddeld aantal overlijdens over 10 jaar versus overlijdens door covid-19 in Belgie&#39;) . Source date from sciensano . In this section, we compare the graph obtained with data obtained from sciensano. . df_sc = pd.read_csv(&#39;https://epistat.sciensano.be/Data/COVID19BE_MORT.csv&#39;) . df_sc.head() . DATE REGION AGEGROUP SEX DEATHS . 0 2020-03-10 | Brussels | 85+ | F | 1 | . 1 2020-03-11 | Flanders | 85+ | F | 1 | . 2 2020-03-11 | Brussels | 75-84 | M | 1 | . 3 2020-03-11 | Brussels | 85+ | F | 1 | . 4 2020-03-12 | Brussels | 75-84 | M | 1 | . df_dead_day = df_sc.groupby(&#39;DATE&#39;)[&#39;DEATHS&#39;].sum().reset_index() df_dead_day[&#39;Datum&#39;] = pd.to_datetime(df_dead_day[&#39;DATE&#39;]) df_dead_day[&#39;Dag&#39;] = df_dead_day[&#39;Datum&#39;].dt.dayofyear . line = alt.Chart(df).mark_line().encode( x=alt.X(&#39;Dag&#39;, title=&#39;Dag van het jaar&#39;, scale=alt.Scale( domain=(1, 120), clamp=True )), y=&#39;mean(MS_NUM_DEATHS)&#39; ) # Bootstrapped 95% confidence interval band = alt.Chart(df).mark_errorband(extent=&#39;ci&#39;).encode( x=alt.X(&#39;Dag&#39;, scale=alt.Scale(domain=(1, 120), clamp=True)), y=alt.Y(&#39;MS_NUM_DEATHS&#39;, title=&#39;Overlijdens per dag&#39;), ) dead_covid= alt.Chart(df_dead_day).mark_line(point=True).encode( x=alt.X(&#39;Dag&#39;,scale=alt.Scale(domain=(1, 120), clamp=True)), y=&#39;DEATHS&#39;, color=alt.ColorValue(&#39;red&#39;), tooltip=[&#39;Dag&#39;, &#39;DEATHS&#39;, &#39;Datum&#39;] ) (band + line + dead_covid).properties(width=750, title=&#39;Gemiddeld aantal overlijdens over 10 jaar versus overlijdens door covid-19 in Belgie&#39;) . Obviously, data form 16-17-18 April 2020 is not final yet. Also, the amounts are smaller then those from JHU. . Obtain more detail (for another blogpost...) . df_tot_sc = pd.read_excel(&#39;https://epistat.sciensano.be/Data/COVID19BE.xlsx&#39;) . df_tot_sc . DATE PROVINCE REGION AGEGROUP SEX CASES . 0 2020-03-01 | Brussels | Brussels | 10-19 | M | 1 | . 1 2020-03-01 | Brussels | Brussels | 10-19 | F | 1 | . 2 2020-03-01 | Brussels | Brussels | 20-29 | M | 1 | . 3 2020-03-01 | Brussels | Brussels | 30-39 | F | 1 | . 4 2020-03-01 | Brussels | Brussels | 40-49 | F | 1 | . ... ... | ... | ... | ... | ... | ... | . 6875 NaN | OostVlaanderen | Flanders | NaN | F | 4 | . 6876 NaN | VlaamsBrabant | Flanders | 40-49 | M | 3 | . 6877 NaN | VlaamsBrabant | Flanders | 40-49 | F | 2 | . 6878 NaN | VlaamsBrabant | Flanders | 50-59 | M | 1 | . 6879 NaN | WestVlaanderen | Flanders | 50-59 | M | 3 | . 6880 rows × 6 columns . We know that there are a lot of reional differences: . df_plot = df_tot_sc.groupby([&#39;DATE&#39;, &#39;PROVINCE&#39;])[&#39;CASES&#39;].sum().reset_index() . df_plot . DATE PROVINCE CASES . 0 2020-03-01 | Brussels | 6 | . 1 2020-03-01 | Limburg | 1 | . 2 2020-03-01 | Liège | 2 | . 3 2020-03-01 | OostVlaanderen | 1 | . 4 2020-03-01 | VlaamsBrabant | 6 | . ... ... | ... | ... | . 505 2020-04-17 | OostVlaanderen | 44 | . 506 2020-04-17 | VlaamsBrabant | 42 | . 507 2020-04-17 | WestVlaanderen | 30 | . 508 2020-04-18 | Brussels | 1 | . 509 2020-04-18 | Hainaut | 1 | . 510 rows × 3 columns . df_plot[&#39;DATE&#39;] = pd.to_datetime(df_plot[&#39;DATE&#39;]) . base = alt.Chart(df_plot, title=&#39;Number of cases in Belgium per day and province&#39;).mark_line(point=True).encode( x=alt.X(&#39;DATE:T&#39;, title=&#39;Datum&#39;), y=alt.Y(&#39;CASES&#39;, title=&#39;Cases per day&#39;), color=&#39;PROVINCE&#39;, tooltip=[&#39;DATE&#39;, &#39;CASES&#39;, &#39;PROVINCE&#39;] ).properties(width=600) base . From the above graph we see a much lower number of cases in Luxembourg, Namur, Waals Brabant. . !pwd . &#39;pwd&#39; is not recognized as an internal or external command, operable program or batch file. . !dir . Volume in drive C is Windows Volume Serial Number is 7808-E933 Directory of C: Users lnh6dt5 AppData Local Temp Mxt121 tmp home_lnh6dt5 blog _notebooks 19/04/2020 14:14 &lt;DIR&gt; . 19/04/2020 14:14 &lt;DIR&gt; .. 19/04/2020 10:37 &lt;DIR&gt; .ipynb_checkpoints 19/04/2020 10:17 23.473 2020-01-28-Altair.ipynb 19/04/2020 10:34 9.228 2020-01-29-bullet-chart-altair.ipynb 19/04/2020 10:26 41.041 2020-02-15-breakins.ipynb 19/04/2020 09:43 30.573 2020-02-20-test.ipynb 19/04/2020 09:49 1.047 2020-04-18-first-test.ipynb 19/04/2020 14:14 1.237.674 2020-04-19-deads-last-ten-year-vs-covid.ipynb 19/04/2020 09:43 &lt;DIR&gt; my_icons 19/04/2020 09:43 771 README.md 7 File(s) 1.343.807 bytes 4 Dir(s) 89.905.336.320 bytes free .",
            "url": "https://cast42.github.io/blog/cast42/jupyter/covid19/belgium/altair/2020/04/19/deads-last-ten-year-vs-covid.html",
            "relUrl": "/cast42/jupyter/covid19/belgium/altair/2020/04/19/deads-last-ten-year-vs-covid.html",
            "date": " • Apr 19, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "First test post",
            "content": "import pandas as pd import altair as alt . # Check if this get published .",
            "url": "https://cast42.github.io/blog/jupyter/2020/04/18/first-test.html",
            "relUrl": "/jupyter/2020/04/18/first-test.html",
            "date": " • Apr 18, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://cast42.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Evolution of burglary in Leuven. Is the trend downwards ?",
            "content": "The local police shared a graph with the number of break-ins in Leuven per year. The article shows a graph with a downwards trendline. Can we conclude that the number of breakins is showing a downward trend based on those numbers? Let&#39;s construct a dataframe with the data from the graph. . import numpy as np import pandas as pd import altair as alt df = pd.DataFrame({&#39;year_int&#39;:[y for y in range(2006, 2020)], &#39;breakins&#39;:[1133,834,953,891,1006,1218,992,1079,1266,1112,713,669,730,644]}) df[&#39;year&#39;] = pd.to_datetime(df[&#39;year_int&#39;], format=&#39;%Y&#39;) . points = alt.Chart(df).mark_line(point=True).encode( x=&#39;year&#39;, y=&#39;breakins&#39;, tooltip=&#39;breakins&#39; ) points + points.transform_regression(&#39;year&#39;, &#39;breakins&#39;).mark_line( color=&#39;green&#39; ).properties( title=&#39;Regression trend on the number breakins per year in Leuven&#39; ) . The article claims that the number of breakins stabilizes the last years. Let&#39;s perform a local regression to check that. . # https://opendatascience.com/local-regression-in-python # Loess: https://gist.github.com/AllenDowney/818f6153ef316aee80467c51faee80f8 points + points.transform_loess(&#39;year&#39;, &#39;breakins&#39;).mark_line( color=&#39;green&#39; ).properties( title=&#39;Local regression trend on the number breakins per year in Leuven&#39; ) . But what about the trend line? Are we sure the trend is negative ? Bring in the code based on the blogpost The hacker&#39;s guide to uncertainty estimates to estimate the uncertainty.: . # Code from: https://erikbern.com/2018/10/08/the-hackers-guide-to-uncertainty-estimates.html import scipy.optimize import random def model(xs, k, m): return k * xs + m def neg_log_likelihood(tup, xs, ys): # Since sigma &gt; 0, we use use log(sigma) as the parameter instead. # That way we have an unconstrained problem. k, m, log_sigma = tup sigma = np.exp(log_sigma) delta = model(xs, k, m) - ys return len(xs)/2*np.log(2*np.pi*sigma**2) + np.dot(delta, delta) / (2*sigma**2) def confidence_bands(xs, ys, nr_bootstrap): curves = [] xys = list(zip(xs, ys)) for i in range(nr_bootstrap): # sample with replacement bootstrap = [random.choice(xys) for _ in xys] xs_bootstrap = np.array([x for x, y in bootstrap]) ys_bootstrap = np.array([y for x, y in bootstrap]) k_hat, m_hat, log_sigma_hat = scipy.optimize.minimize( neg_log_likelihood, (0, 0, 0), args=(xs_bootstrap, ys_bootstrap) ).x curves.append( model(xs, k_hat, m_hat) + # Note what&#39;s going on here: we&#39;re _adding_ the random term # to the predictions! np.exp(log_sigma_hat) * np.random.normal(size=xs.shape) ) lo, hi = np.percentile(curves, (2.5, 97.5), axis=0) return lo, hi . # Make a plot with a confidence band df[&#39;lo&#39;], df[&#39;hi&#39;] = confidence_bands(df.index, df[&#39;breakins&#39;], 100) ci = alt.Chart(df).mark_area().encode( x=alt.X(&#39;year:T&#39;, title=&#39;&#39;), y=alt.Y(&#39;lo:Q&#39;), y2=alt.Y2(&#39;hi:Q&#39;, title=&#39;&#39;), color=alt.value(&#39;lightblue&#39;), opacity=alt.value(0.6) ) chart = alt.Chart(df).mark_line(point=True).encode( x=&#39;year&#39;, y=&#39;breakins&#39;, tooltip=&#39;breakins&#39; ) ci + chart + chart.transform_regression(&#39;year&#39;, &#39;breakins&#39;).mark_line( color=&#39;red&#39; ).properties( title=&#39;95% Confidence band of the number of breakins per year in Leuven&#39; ) . On the above chart, we see that a possitive trend might be possible as well. . Linear regression . Let&#39;s perform a linear regression with statsmodel to calculate the confidence interval on the slope of the regression line. . import statsmodels.formula.api as smf . results = smf.ols(&#39;breakins ~ index&#39;, data=df.reset_index()).fit() . results.params . Intercept 1096.314286 index -23.169231 dtype: float64 . The most likely slope of the trend line is 23.17 breakins per year. But how sure are we that the trend is heading down ? . results.summary() . C: Users lnh6dt5 AppData Local Continuum anaconda3 lib site-packages scipy stats stats.py:1535: UserWarning: kurtosistest only valid for n&gt;=20 ... continuing anyway, n=14 &#34;anyway, n=%i&#34; % int(n)) . OLS Regression Results Dep. Variable: breakins | R-squared: 0.223 | . Model: OLS | Adj. R-squared: 0.159 | . Method: Least Squares | F-statistic: 3.451 | . Date: Sun, 19 Apr 2020 | Prob (F-statistic): 0.0879 | . Time: 10:26:45 | Log-Likelihood: -92.105 | . No. Observations: 14 | AIC: 188.2 | . Df Residuals: 12 | BIC: 189.5 | . Df Model: 1 | | . Covariance Type: nonrobust | | . | coef std err t P&gt;|t| [0.025 0.975] . Intercept 1096.3143 | 95.396 | 11.492 | 0.000 | 888.465 | 1304.164 | . index -23.1692 | 12.472 | -1.858 | 0.088 | -50.344 | 4.006 | . Omnibus: 1.503 | Durbin-Watson: 1.035 | . Prob(Omnibus): 0.472 | Jarque-Bera (JB): 1.196 | . Skew: 0.577 | Prob(JB): 0.550 | . Kurtosis: 2.153 | Cond. No. 14.7 | . Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified. The analysis reveals that the slope of the best fitting regression line is 23 breakins less per year. However, the confidence interval of the trend is between -50.344 and 4.006. Also the p)value of the regression coefficient is 0.088. Meaning we have eight percent chance that the negative trend is by accident. Hence, based on the current data we are not 95% percent sure the trend is downwards. Hence we can not conclude, based on this data, that there is a negative trend. This corresponds with the width of the 95% certainty band drawn that allows for an upward trend line: . # Here are the confidence intervals of the regression results.conf_int() . 0 1 . Intercept 888.464586 | 1304.163986 | . index -50.344351 | 4.005889 | . y_low = results.params[&#39;Intercept&#39;] # ?ost likely value of the intercept y_high = results.params[&#39;Intercept&#39;] + results.conf_int()[1][&#39;index&#39;] * df.shape[0] # Value of upward trend for the last year df_upward_trend = pd.DataFrame({&#39;year&#39;:[df[&#39;year&#39;].min(), df[&#39;year&#39;].max()], &#39;breakins&#39;:[y_low, y_high]}) possible_upwards_trend = alt.Chart(df_upward_trend).mark_line( color=&#39;green&#39;, strokeDash=[10,10] ).encode( x=&#39;year:T&#39;, y=alt.Y(&#39;breakins:Q&#39;, title=&#39;Number of breakins per year&#39;) ) points = alt.Chart(df).mark_line(point=True).encode(x=&#39;year&#39;, y=&#39;breakins&#39;, tooltip=&#39;breakins&#39;) (ci + points + points.transform_regression(&#39;year&#39;, &#39;breakins&#39;).mark_line(color=&#39;red&#39;) + possible_upwards_trend).properties( title=&#39;Trend analysis on the number of breakins per year in Leuven, Belgium&#39; ) . In the above graph, we see that a slight positive trend (green dashed line) is in the 95% confidence band on the regression coefficient. We are not sure that the trend on the number of breakins is downwards. .",
            "url": "https://cast42.github.io/blog/cast42/jupyter/altair/2020/02/15/breakins.html",
            "relUrl": "/cast42/jupyter/altair/2020/02/15/breakins.html",
            "date": " • Feb 15, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Bullet chart in python Altair",
            "content": "In the article &quot;Bullet Charts - What Is It And How To Use It&quot; I learned about Bullet charts. It&#39;s a specific kind of barchart that must convey the state of a measure or KPI. The goal is to see in a glance if the target is met. Here is an example bullet chart from the article: . # This causes issues to: # from IPython.display import Image # Image(&#39;https://jscharting.com/blog/bullet-charts/images/bullet_components.png&#39;) . . # &lt;img src=&quot;https://jscharting.com/blog/bullet-charts/images/bullet_components.png&quot; alt=&quot;Bullet chart&quot; style=&quot;width: 200px;&quot;/&gt; . Below is some Python code that generates bullets graphs using the Altair library. . import altair as alt import pandas as pd df = pd.DataFrame.from_records([ {&quot;title&quot;:&quot;Revenue&quot;,&quot;subtitle&quot;:&quot;US$, in thousands&quot;,&quot;ranges&quot;:[150,225,300],&quot;measures&quot;:[220,270],&quot;markers&quot;:[250]}, {&quot;title&quot;:&quot;Profit&quot;,&quot;subtitle&quot;:&quot;%&quot;,&quot;ranges&quot;:[20,25,30],&quot;measures&quot;:[21,23],&quot;markers&quot;:[26]}, {&quot;title&quot;:&quot;Order Size&quot;,&quot;subtitle&quot;:&quot;US$, average&quot;,&quot;ranges&quot;:[350,500,600],&quot;measures&quot;:[100,320],&quot;markers&quot;:[550]}, {&quot;title&quot;:&quot;New Customers&quot;,&quot;subtitle&quot;:&quot;count&quot;,&quot;ranges&quot;:[1400,2000,2500],&quot;measures&quot;:[1000,1650],&quot;markers&quot;:[2100]}, {&quot;title&quot;:&quot;Satisfaction&quot;,&quot;subtitle&quot;:&quot;out of 5&quot;,&quot;ranges&quot;:[3.5,4.25,5],&quot;measures&quot;:[3.2,4.7],&quot;markers&quot;:[4.4]} ]) alt.layer( alt.Chart().mark_bar(color=&#39;#eee&#39;).encode(alt.X(&quot;ranges[2]:Q&quot;, scale=alt.Scale(nice=False), title=None)), alt.Chart().mark_bar(color=&#39;#ddd&#39;).encode(x=&quot;ranges[1]:Q&quot;), alt.Chart().mark_bar(color=&#39;#bbb&#39;).encode(x=&quot;ranges[0]:Q&quot;), alt.Chart().mark_bar(color=&#39;steelblue&#39;, size=10).encode(x=&#39;measures[0]:Q&#39;), alt.Chart().mark_tick(color=&#39;black&#39;, size=12).encode(x=&#39;markers[0]:Q&#39;), data=df ).facet( row=alt.Row(&quot;title:O&quot;, title=&#39;&#39;) ).resolve_scale( x=&#39;independent&#39; ) .",
            "url": "https://cast42.github.io/blog/cast42/jupyter/data%20visualisation/altair/2020/01/29/bullet-chart-altair.html",
            "relUrl": "/cast42/jupyter/data%20visualisation/altair/2020/01/29/bullet-chart-altair.html",
            "date": " • Jan 29, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Simple Notebook with interactive Altair graph",
            "content": "Test fast template notebook posts . import altair as alt . Make an Altair graph . from vega_datasets import data iris = data.iris() alt.Chart(iris).mark_point().encode( x=&#39;petalLength&#39;, y=&#39;petalWidth&#39;, color=&#39;species&#39;, tooltip=&#39;species&#39; ).interactive() . Enjoy :) .",
            "url": "https://cast42.github.io/blog/cast42/jupyter/covid19/belgium/altair/2020/01/28/Altair.html",
            "relUrl": "/cast42/jupyter/covid19/belgium/altair/2020/01/28/Altair.html",
            "date": " • Jan 28, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://cast42.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is Lode Nachtergaele. I’m an engineer by training, and currently hold the position as data scientist at Colruyt Group (a retailer in Belgium), where I spend time on writing notebooks in Python. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://cast42.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://cast42.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}