{
  
    
        "post0": {
            "title": "Comparing Rt numbers for Belgium on 09-12-2020",
            "content": "Model Based on URL Rt Date . by Niel Hens | Cases | https://gjbex.github.io/DSI_UHasselt_covid_dashboard/ | 0.96 | 20-12-5 | . Cori et al. (2013) | Hospitalisations | https://covid-19.sciensano.be/sites/default/files/Covid19/COVID-19_Weekly_report_NL.pdf | 0.798 | 20-11-27/ till 20-12-3 | . RKI | Hospitalisations | https://datastudio.google.com/embed/u/0/reporting/c14a5cfc-cab7-4812-848c-0369173148ab/page/ZwmOB | 0.97 | 20-12-09 | . rtlive | Cases | https://rtlive.de/global.html | 0.80 | 20-12-09 | . epiforecast | Cases and Deaths | https://epiforecasts.io/covid/posts/national/belgium/ | 0.5 | 20-12-07 | . Huisman et al. (2020) | Cases | https://ibz-shiny.ethz.ch/covid-19-re-international/ | 1.01 | 20-11-24 | . Huisman et al. (2020) | Hospitalisations | https://ibz-shiny.ethz.ch/covid-19-re-international/ | 0.84 | 20-11-24 | . RKI | Cases | https://twitter.com/BartMesuere/status/1336565641764089856 | 0.99 | 20-12-08 | . Deforche (2020) | Hospitalisations and Deaths | https://twitter.com/houterkabouter/status/1336582281994055680 | 0.85 | 20-12-09 | . SEIR | Hospitalisations and Deaths | https://twitter.com/vdwnico/status/1336557572254552065 | 1.5 | 20-12-09 | .",
            "url": "https://cast42.github.io/blog/covid19/2020/12/09/Comparing-Rt-numbers-for-Belgium.html",
            "relUrl": "/covid19/2020/12/09/Comparing-Rt-numbers-for-Belgium.html",
            "date": " ‚Ä¢ Dec 9, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Estimating the effective reproduction number in Belgium with the RKI method",
            "content": "Every day Bart Mesuere tweets a nice dashboard with current numbers about Covid-19 in Belgium. This was the tweet on Wednesday 20/11/04: Gisteren waren er 877 nieuwe ziekenhuisopnames, een nieuw record na enkele betere dagen. Er liggen nu 7485 (+254) mensen in het ziekenhuis, waarvan 1351 (+49) op IC. Op zat. 31/10 waren er 5551 nieuwe bevestigde besmettingen, in de meeste provincies lijken we over de piek heen. pic.twitter.com/EOvnATQUaa . &mdash; Bart Mesuere (@BartMesuere) November 4, 2020 . It&#39;s nice to see that the effective reproduction number ($Re(t)$) is again below one. That means the power of virus is declining and the number of infection will start to lower. This occured first on Tuesday 2020/11/3: Gisteren waren er 596 nieuwe ziekenhuisopnames. Er liggen nu 7231 (+408) mensen in het ziekenhuis, waarvan 1302 (+79) op IC dit is meer dan tijdens de 1e golf. Op 30/10 waren er 14700 nieuwe bevestigde besmettingen. In Brussel en Brabant zien we ondertussen een duidelijke daling. pic.twitter.com/eWmhX20jQG . &mdash; Bart Mesuere (@BartMesuere) November 3, 2020 . I estimated the $Re(t)$ earlier with rt.live model in this notebook. There the $Re(t)$ was still estimated to be above one. Michael Osthege replied with a simulation results with furter improved model: Sneak preview of the results we got. @cast42 how do they compare with yours? Would love to see a contribution of your data loading routines to https://t.co/fCZDWUFd2k ! pic.twitter.com/QLEqZeV1qx . &mdash; Michael Osthege (@theCake) November 2, 2020 . In that estimation, the $Re(t)$ was also not yet heading below one at the end of october. . In this notebook, we will implement a calculation based on the method of the Robert Koch Institute. The method is described and programmed in R in this blog post. . In that blogpost there&#39;s a link to a website with estimations for most places in the world The estimation for Belgium is here . . According to that calculation, $Re(t)$ is already below zero for some days. . Load libraries and data . import numpy as np import pandas as pd . df_tests = pd.read_csv(&#39;https://epistat.sciensano.be/Data/COVID19BE_tests.csv&#39;, parse_dates=[&#39;DATE&#39;]) . df_cases = pd.read_csv(&#39;https://epistat.sciensano.be/Data/COVID19BE_CASES_AGESEX.csv&#39;, parse_dates=[&#39;DATE&#39;]) df_cases . DATE PROVINCE REGION AGEGROUP SEX CASES . 0 2020-03-01 | Antwerpen | Flanders | 40-49 | M | 1 | . 1 2020-03-01 | Brussels | Brussels | 10-19 | F | 1 | . 2 2020-03-01 | Brussels | Brussels | 10-19 | M | 1 | . 3 2020-03-01 | Brussels | Brussels | 20-29 | M | 1 | . 4 2020-03-01 | Brussels | Brussels | 30-39 | F | 1 | . ... ... | ... | ... | ... | ... | ... | . 36279 NaT | VlaamsBrabant | Flanders | 40-49 | M | 3 | . 36280 NaT | VlaamsBrabant | Flanders | 50-59 | M | 1 | . 36281 NaT | WestVlaanderen | Flanders | 20-29 | F | 1 | . 36282 NaT | WestVlaanderen | Flanders | 50-59 | M | 3 | . 36283 NaT | NaN | NaN | NaN | NaN | 1 | . 36284 rows √ó 6 columns . Reformat data into Rtlive format . df_cases_per_day = (df_cases .dropna(subset=[&#39;DATE&#39;]) .assign(region=&#39;Belgium&#39;) .groupby([&#39;region&#39;, &#39;DATE&#39;], as_index=False) .agg(cases=(&#39;CASES&#39;, &#39;sum&#39;)) .rename(columns={&#39;DATE&#39;:&#39;date&#39;}) .set_index([&quot;region&quot;, &quot;date&quot;]) ) . What&#39;s in our basetable: . df_cases_per_day . cases . region date . Belgium 2020-03-01 19 | . 2020-03-02 19 | . 2020-03-03 34 | . 2020-03-04 53 | . 2020-03-05 81 | . ... ... | . 2020-11-01 2660 | . 2020-11-02 13345 | . 2020-11-03 11167 | . 2020-11-04 4019 | . 2020-11-05 5 | . 250 rows √ó 1 columns . Let&#39;s plot the number of cases in function of the time. . ax = df_cases_per_day.loc[&#39;Belgium&#39;].plot(figsize=(18,6)) ax.set(ylabel=&#39;Number of cases&#39;, title=&#39;Number of cases for covid-19 and number of positives in Belgium&#39;); . We see that the last days are not yet complete. Let&#39;s cut off the last two days of reporting. . import datetime from dateutil.relativedelta import relativedelta . Calculate the date two days ago: . datetime.date(2020, 11, 3) . datetime.date(2020, 11, 3) . # today_minus_two = datetime.date.today() + relativedelta(days=-2) today_minus_two = datetime.date(2020, 11, 3) # Fix the day today_minus_two.strftime(&quot;%Y-%m-%d&quot;) . &#39;2020-11-03&#39; . Replot the cases: . ax = df_cases_per_day.loc[&#39;Belgium&#39;][:today_minus_two].plot(figsize=(18,6)) ax.set(ylabel=&#39;Number of cases&#39;, title=&#39;Number of cases for covid-19 and number of positives in Belgium&#39;); . Select the Belgium region: . region = &#39;Belgium&#39; df = df_cases_per_day.loc[region][:today_minus_two] df . cases . date . 2020-03-01 19 | . 2020-03-02 19 | . 2020-03-03 34 | . 2020-03-04 53 | . 2020-03-05 81 | . ... ... | . 2020-10-30 15185 | . 2020-10-31 6243 | . 2020-11-01 2660 | . 2020-11-02 13345 | . 2020-11-03 11167 | . 248 rows √ó 1 columns . Check the types of the columns: . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; DatetimeIndex: 248 entries, 2020-03-01 to 2020-11-03 Data columns (total 1 columns): # Column Non-Null Count Dtype -- -- 0 cases 248 non-null int64 dtypes: int64(1) memory usage: 3.9 KB . Robert Koch Institute method . A basic method to calculate the effective reproduction number is described (among others) in this blogpost. I included the relevant paragraph: . In a recent report (an der Heiden and Hamouda 2020) the RKI described their method for computing R as part of the COVID-19 outbreak as follows (p. 13): For a constant generation time of 4 days, one obtains R as the ratio of new infections in two consecutive time periods each consisting of 4 days. Mathematically, this estimation could be formulated as part of a statistical model: . $$y_{s+4} | y_{s} sim Po(R cdot y_{s}), s= 1,2,3,4$$ . where $y_{1}, ldots, y_{4}$ are considered as fixed. From this we obtain . $$ hat{R}_{RKI} = sum_{s=1}^{4} y_{s+4} / sum_{s=1}^{4} y_{s}$$ . Somewhat arbitrary, we denote by $Re(t)$ the above estimate for R when $s=1$ corresponds to time $t-8$, i.e. we assign the obtained value to the last of the 8 values used in the computation. . In Python, we define a lambda function that we apply on a rolling window. Since indexes start from zero, we calculate: . $$ hat{R}_{RKI} = sum_{s=0}^{3} y_{s+4} / sum_{s=0}^{3} y_{s}$$ . rt = lambda y: np.sum(y[4:])/np.sum(y[:4]) . df.rolling(8).apply(rt) . cases . date . 2020-03-01 NaN | . 2020-03-02 NaN | . 2020-03-03 NaN | . 2020-03-04 NaN | . 2020-03-05 NaN | . ... ... | . 2020-10-30 1.273703 | . 2020-10-31 0.929291 | . 2020-11-01 0.601838 | . 2020-11-02 0.499806 | . 2020-11-03 0.475685 | . 248 rows √ó 1 columns . The first values are Nan because the window is in the past. If we plot the result, it looks like this: . ax = df.rolling(8).apply(rt).plot(figsize=(16,4), label=&#39;Re(t)&#39;) ax.set(ylabel=&#39;Re(t)&#39;, title=&#39;Effective reproduction number estimated with RKI method&#39;) ax.legend([&#39;Re(t)&#39;]); . To avoid the spikes due to weekend reporting issue, I first applied a rolling mean on a window of 7 days: . ax = df.rolling(7).mean().rolling(8).apply(rt).plot(figsize=(16,4), label=&#39;Re(t)&#39;) ax.set(ylabel=&#39;Re(t)&#39;, title=&#39;Effective reproduction number estimated with RKI method after rolling mean on window of 7 days&#39;) ax.legend([&#39;Re(t)&#39;]); . Interactive visualisation in Altair . import altair as alt alt.Chart(df.rolling(7).mean().rolling(8).apply(rt).fillna(0).reset_index()).mark_line().encode( x=alt.X(&#39;date:T&#39;), y=alt.Y(&#39;cases&#39;, title=&#39;Re(t)&#39;), tooltip=[&#39;date:T&#39;, alt.Tooltip(&#39;cases&#39;, format=&#39;.2f&#39;)] ).transform_filter( alt.datum.date &gt; alt.expr.toDate(&#39;2020-03-13&#39;) ).properties( width=600, title=&#39;Effective reproduction number in Belgium based on Robert-Koch Institute method&#39; ) . Making the final visualisation in Altair . In the interactive Altair figure below, we show the $Re(t)$ for the last 14 days. We reduce the rolling mean window to three to see faster reactions. . #collapse df_plot = df.rolling(7).mean().rolling(8).apply(rt).fillna(0).reset_index() last_value = str(df_plot.iloc[-1][&#39;cases&#39;].round(2)) + &#39; ‚Üì&#39; first_value = str(df_plot[df_plot[&#39;date&#39;] == &#39;2020-10-21&#39;].iloc[0][&#39;cases&#39;].round(2)) # + &#39; ‚Üë&#39; today_minus_15 = datetime.datetime.today() + relativedelta(days=-15) today_minus_15_str = today_minus_15.strftime(&quot;%Y-%m-%d&quot;) line = alt.Chart(df_plot).mark_line(point=True).encode( x=alt.X(&#39;date:T&#39;, axis=alt.Axis(title=&#39;Datum&#39;, grid=False)), y=alt.Y(&#39;cases&#39;, axis=alt.Axis(title=&#39;Re(t)&#39;, grid=False, labels=False, titlePadding=40)), tooltip=[&#39;date:T&#39;, alt.Tooltip(&#39;cases&#39;, title=&#39;Re(t)&#39;, format=&#39;.2f&#39;)] ).transform_filter( alt.datum.date &gt; alt.expr.toDate(today_minus_15_str) ).properties( width=600, height=100 ) hline = alt.Chart(pd.DataFrame({&#39;cases&#39;: [1]})).mark_rule().encode(y=&#39;cases&#39;) label_right = alt.Chart(df_plot).mark_text( align=&#39;left&#39;, dx=5, dy=-10 , size=15 ).encode( x=alt.X(&#39;max(date):T&#39;, title=None), text=alt.value(last_value), ) label_left = alt.Chart(df_plot).mark_text( align=&#39;right&#39;, dx=-5, dy=-40, size=15 ).encode( x=alt.X(&#39;min(date):T&#39;, title=None), text=alt.value(first_value), ).transform_filter( alt.datum.date &gt; alt.expr.toDate(today_minus_15_str) ) source = alt.Chart( {&quot;values&quot;: [{&quot;text&quot;: &quot;Data source: Sciensano&quot;}]} ).mark_text(size=12, align=&#39;left&#39;, dx=-57).encode( text=&quot;text:N&quot; ) alt.vconcat(line + label_left + label_right + hline, source).configure( background=&#39;#D9E9F0&#39; ).configure_view( stroke=None, # Remove box around graph ).configure_axisY( ticks=False, grid=False, domain=False ).configure_axisX( grid=False, domain=False ).properties(title={ &quot;text&quot;: [&#39;Effective reproduction number for the last 14 days in Belgium&#39;], &quot;subtitle&quot;: [f&#39;Estimation based on the number of cases until {today_minus_two.strftime(&quot;%Y-%m-%d&quot;)} after example of Robert Koch Institute with serial interval of 4&#39;], } ) # .configure_axisY( # labelPadding=50, # ) . . To check the calculation, here are the last for values for the number of cases after applying the mean window of 7: . df.rolling(7).mean().iloc[-8:-4] . cases . date . 2020-10-27 16067.571429 | . 2020-10-28 16135.857143 | . 2020-10-29 15744.571429 | . 2020-10-30 15218.000000 | . Those must be added together: . df.rolling(7).mean().iloc[-8:-4].sum() . cases 63166.0 dtype: float64 . And here are the four values, starting four days ago: . df.rolling(7).mean().iloc[-4:] . cases . date . 2020-10-31 14459.428571 | . 2020-11-01 14140.428571 | . 2020-11-02 13213.428571 | . 2020-11-03 11641.428571 | . These are added together: . df.rolling(7).mean().iloc[-4:].sum() . cases 53454.714286 dtype: float64 . And now we divide those two sums to get the $Re(t)$ of 2020-11-03: . df.rolling(7).mean().iloc[-4:].sum()/df.rolling(7).mean().iloc[-8:-4].sum() . cases 0.846258 dtype: float64 . This matches (as expected) the value in the graph. Let&#39;s compare with three other sources: . Alas it does not match the calculation reported by Bart Mesuere on 2020-11-03 based on the RKI model that reports 0.96: Gisteren waren er 596 nieuwe ziekenhuisopnames. Er liggen nu 7231 (+408) mensen in het ziekenhuis, waarvan 1302 (+79) op IC dit is meer dan tijdens de 1e golf. Op 30/10 waren er 14700 nieuwe bevestigde besmettingen. In Brussel en Brabant zien we ondertussen een duidelijke daling. pic.twitter.com/eWmhX20jQG . &mdash; Bart Mesuere (@BartMesuere) November 3, 2020 | Also, the more elaborated model from rtliveglobal is not yet that optimistic. Mind that model rtlive start estimating the $Re(t)$ from the number of tests instead of the number of cases. It might be that other reporting delays are involved. . | epiforecast.io is already below 1 since beginning of November. . | Another possiblity is that I made somewhere a mistake. If you spot it, please let me know. .",
            "url": "https://cast42.github.io/blog/cast42/covid19/belgium/2020/11/05/RKI-rte-calculation.html",
            "relUrl": "/cast42/covid19/belgium/2020/11/05/RKI-rte-calculation.html",
            "date": " ‚Ä¢ Nov 5, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Estimating the effective reproduction number in Belgium",
            "content": "In this post we estimate the effective reproduction number of COVID-19 in the northern and southern part of Belgium. We apply the Bayesian model of rt.live on Belgian data of COVID-19 tests provided by the goverment. . Install needed packages and software . import numpy as np import pandas as pd . The current version of pymc3, installed by default in Colab, is version 3.3.7. The requirements for the Bayesian model of rt.live stipulates a more recent version. We first uninstall verions 3.3.7 and then install a version v3.9.3. . !pip uninstall -y pymc3 . Uninstalling pymc3-3.7: Successfully uninstalled pymc3-3.7 . !pip install pymc3&gt;=3.9.2 !pip install arviz&gt;=0.9.0 . import warnings warnings.simplefilter(action=&quot;ignore&quot;, category=FutureWarning) import pymc3 as pm import arviz as az import numpy as np import pandas as pd from scipy import stats as sps import theano import theano.tensor as tt from theano.tensor.signal.conv import conv2d import seaborn as sns sns.set_context(&#39;talk&#39;) from scipy import stats from matplotlib import pyplot as plt . print(&#39;Running on PyMC3 v{}&#39;.format(pm.__version__)) . Running on PyMC3 v3.9.3 . Now that we are running a recent version of pymc3, we can install the model: . !pip install git+https://github.com/rtcovidlive/covid-model.git . Unfortunately, this does not work. I pasted the code in this notebook as a workaround. . #collapse import warnings warnings.simplefilter(action=&quot;ignore&quot;, category=FutureWarning) import pymc3 as pm import arviz as az import numpy as np import pandas as pd from scipy import stats as sps import theano import theano.tensor as tt from theano.tensor.signal.conv import conv2d # from covid.patients import get_delay_distribution class GenerativeModel: version = &quot;1.0.0&quot; def __init__(self, region: str, observed: pd.DataFrame, buffer_days=10): &quot;&quot;&quot; Takes a region (ie State) name and observed new positive and total test counts per day. buffer_days is the default number of blank days we pad on the leading edge of the time series because infections occur long before reports and we need to infer values on those days &quot;&quot;&quot; first_index = observed.positive.ne(0).argmax() observed = observed.iloc[first_index:] new_index = pd.date_range( start=observed.index[0] - pd.Timedelta(days=buffer_days), end=observed.index[-1], freq=&quot;D&quot;, ) observed = observed.reindex(new_index, fill_value=0) self._trace = None self._inference_data = None self.model = None self.observed = observed self.region = region @property def n_divergences(self): &quot;&quot;&quot; Returns the number of divergences from the current trace &quot;&quot;&quot; assert self.trace != None, &quot;Must run sample() first!&quot; return self.trace[&quot;diverging&quot;].nonzero()[0].size @property def inference_data(self): &quot;&quot;&quot; Returns an Arviz InferenceData object &quot;&quot;&quot; assert self.trace, &quot;Must run sample() first!&quot; with self.model: posterior_predictive = pm.sample_posterior_predictive(self.trace) _inference_data = az.from_pymc3( trace=self.trace, posterior_predictive=posterior_predictive, ) _inference_data.posterior.attrs[&quot;model_version&quot;] = self.version return _inference_data @property def trace(self): &quot;&quot;&quot; Returns the trace from a sample() call. &quot;&quot;&quot; assert self._trace, &quot;Must run sample() first!&quot; return self._trace def _scale_to_positives(self, data): &quot;&quot;&quot; Scales a time series to have the same mean as the observed positives time series. This is useful because many of the series we infer are relative to their true values so we make them comparable by putting them on the same scale. &quot;&quot;&quot; scale_factor = self.observed.positive.mean() / np.mean(data) return scale_factor * data def _get_generation_time_interval(self): &quot;&quot;&quot; Create a discrete P(Generation Interval) Source: https://www.ijidonline.com/article/S1201-9712(20)30119-3/pdf &quot;&quot;&quot; mean_si = 4.7 std_si = 2.9 mu_si = np.log(mean_si ** 2 / np.sqrt(std_si ** 2 + mean_si ** 2)) sigma_si = np.sqrt(np.log(std_si ** 2 / mean_si ** 2 + 1)) dist = sps.lognorm(scale=np.exp(mu_si), s=sigma_si) # Discretize the Generation Interval up to 20 days max g_range = np.arange(0, 20) gt = pd.Series(dist.cdf(g_range), index=g_range) gt = gt.diff().fillna(0) gt /= gt.sum() gt = gt.values return gt def _get_convolution_ready_gt(self, len_observed): &quot;&quot;&quot; Speeds up theano.scan by pre-computing the generation time interval vector. Thank you to Junpeng Lao for this optimization. Please see the outbreak simulation math here: https://staff.math.su.se/hoehle/blog/2020/04/15/effectiveR0.html &quot;&quot;&quot; gt = self._get_generation_time_interval() convolution_ready_gt = np.zeros((len_observed - 1, len_observed)) for t in range(1, len_observed): begin = np.maximum(0, t - len(gt) + 1) slice_update = gt[1 : t - begin + 1][::-1] convolution_ready_gt[ t - 1, begin : begin + len(slice_update) ] = slice_update convolution_ready_gt = theano.shared(convolution_ready_gt) return convolution_ready_gt def build(self): &quot;&quot;&quot; Builds and returns the Generative model. Also sets self.model &quot;&quot;&quot; # p_delay = get_delay_distribution() p_delay = pd.read_csv(&#39;https://raw.githubusercontent.com/rtcovidlive/covid-model/master/data/p_delay.csv&#39;) nonzero_days = self.observed.total.gt(0) len_observed = len(self.observed) convolution_ready_gt = self._get_convolution_ready_gt(len_observed) x = np.arange(len_observed)[:, None] coords = { &quot;date&quot;: self.observed.index.values, &quot;nonzero_date&quot;: self.observed.index.values[self.observed.total.gt(0)], } with pm.Model(coords=coords) as self.model: # Let log_r_t walk randomly with a fixed prior of ~0.035. Think # of this number as how quickly r_t can react. log_r_t = pm.GaussianRandomWalk( &quot;log_r_t&quot;, sigma=0.035, dims=[&quot;date&quot;] ) r_t = pm.Deterministic(&quot;r_t&quot;, pm.math.exp(log_r_t), dims=[&quot;date&quot;]) # For a given seed population and R_t curve, we calculate the # implied infection curve by simulating an outbreak. While this may # look daunting, it&#39;s simply a way to recreate the outbreak # simulation math inside the model: # https://staff.math.su.se/hoehle/blog/2020/04/15/effectiveR0.html seed = pm.Exponential(&quot;seed&quot;, 1 / 0.02) y0 = tt.zeros(len_observed) y0 = tt.set_subtensor(y0[0], seed) outputs, _ = theano.scan( fn=lambda t, gt, y, r_t: tt.set_subtensor(y[t], tt.sum(r_t * y * gt)), sequences=[tt.arange(1, len_observed), convolution_ready_gt], outputs_info=y0, non_sequences=r_t, n_steps=len_observed - 1, ) infections = pm.Deterministic(&quot;infections&quot;, outputs[-1], dims=[&quot;date&quot;]) # Convolve infections to confirmed positive reports based on a known # p_delay distribution. See patients.py for details on how we calculate # this distribution. test_adjusted_positive = pm.Deterministic( &quot;test_adjusted_positive&quot;, conv2d( tt.reshape(infections, (1, len_observed)), tt.reshape(p_delay, (1, len(p_delay))), border_mode=&quot;full&quot;, )[0, :len_observed], dims=[&quot;date&quot;] ) # Picking an exposure with a prior that exposure never goes below # 0.1 * max_tests. The 0.1 only affects early values of Rt when # testing was minimal or when data errors cause underreporting # of tests. tests = pm.Data(&quot;tests&quot;, self.observed.total.values, dims=[&quot;date&quot;]) exposure = pm.Deterministic( &quot;exposure&quot;, pm.math.clip(tests, self.observed.total.max() * 0.1, 1e9), dims=[&quot;date&quot;] ) # Test-volume adjust reported cases based on an assumed exposure # Note: this is similar to the exposure parameter in a Poisson # regression. positive = pm.Deterministic( &quot;positive&quot;, exposure * test_adjusted_positive, dims=[&quot;date&quot;] ) # Save data as part of trace so we can access in inference_data observed_positive = pm.Data(&quot;observed_positive&quot;, self.observed.positive.values, dims=[&quot;date&quot;]) nonzero_observed_positive = pm.Data(&quot;nonzero_observed_positive&quot;, self.observed.positive[nonzero_days.values].values, dims=[&quot;nonzero_date&quot;]) positive_nonzero = pm.NegativeBinomial( &quot;nonzero_positive&quot;, mu=positive[nonzero_days.values], alpha=pm.Gamma(&quot;alpha&quot;, mu=6, sigma=1), observed=nonzero_observed_positive, dims=[&quot;nonzero_date&quot;] ) return self.model def sample( self, cores=4, chains=4, tune=700, draws=200, target_accept=0.95, init=&quot;jitter+adapt_diag&quot;, ): &quot;&quot;&quot; Runs the PyMC3 model and stores the trace result in self.trace &quot;&quot;&quot; if self.model is None: self.build() with self.model: self._trace = pm.sample( draws=draws, cores=cores, chains=chains, target_accept=target_accept, tune=tune, init=init, ) return self . . Get the data . Read the data from sciensano: . df_tests = pd.read_csv(&#39;https://epistat.sciensano.be/Data/COVID19BE_tests.csv&#39;, parse_dates=[&#39;DATE&#39;]) . What is in this dataframe ? . df_tests . DATE PROVINCE REGION TESTS_ALL TESTS_ALL_POS . 0 2020-03-01 | Antwerpen | Flanders | 18 | 0 | . 1 2020-03-01 | BrabantWallon | Wallonia | 8 | 0 | . 2 2020-03-01 | Brussels | Brussels | 4 | 0 | . 3 2020-03-01 | Hainaut | Wallonia | 5 | 0 | . 4 2020-03-01 | Li√®ge | Wallonia | 8 | 0 | . ... ... | ... | ... | ... | ... | . 2935 2020-10-31 | NaN | NaN | 58 | 13 | . 2936 2020-10-31 | Namur | Wallonia | 864 | 387 | . 2937 2020-10-31 | OostVlaanderen | Flanders | 927 | 106 | . 2938 2020-10-31 | VlaamsBrabant | Flanders | 1600 | 259 | . 2939 2020-10-31 | WestVlaanderen | Flanders | 512 | 82 | . 2940 rows √ó 5 columns . We see that we have the number of tests (TESTS_ALL) and the number of positive tests (TEST_ALL_POS) per date, province and region. In this post, we will analyse the three regions: Brussels, Flanders and Wallonia. . Preprocessing . Are the any Nan ? . df_tests.isnull().mean() . DATE 0.000000 PROVINCE 0.083333 REGION 0.083333 TESTS_ALL 0.000000 TESTS_ALL_POS 0.000000 dtype: float64 . About eight procent of the lines do not have a PROVINCE nor REGION assigned. What should we do with those ? Ignore them ? Let&#39;s look how many there are: . ax = df_tests[df_tests[&#39;REGION&#39;].isnull()].groupby([&#39;DATE&#39;,], dropna=False).sum().plot(figsize=(18, 4)) ax.set(title=&#39;Number of covid-19 tests per day not attributed to a region in Belgium&#39;, ylabel=&#39;Number of tests&#39;); . Here we create a function that distributes the non attributed tests according to the number of tests in each region. For example suppose on a day there are 10, 20 and 150 test in Brussels, Flanders and Wallonia respectively. Suppose there are 10 test unattributed in Flanders. Then we add 10 (10/(10+20+150)) = 10 (10/180) = 100/180 = 0.55 test to Flanders. The total number of test of Flanders becomes 10.55. We round that to the nearest integer: 11. And we do the same for the other regions Brussels and Wallonia. So we distribute the 10 unattributed tests weighted according to the number of tests in each region. . def redistribute(g, col): gdata = g.groupby(&#39;REGION&#39;)[col].sum() gdata.loc[&#39;Brussels&#39;] += gdata.loc[&#39;Nan&#39;] * (gdata.loc[&#39;Brussels&#39;]/(gdata.loc[&#39;Brussels&#39;] + gdata.loc[&#39;Flanders&#39;] + gdata.loc[&#39;Wallonia&#39;])) gdata.loc[&#39;Flanders&#39;] += gdata.loc[&#39;Nan&#39;] * (gdata.loc[&#39;Flanders&#39;]/(gdata.loc[&#39;Brussels&#39;] + gdata.loc[&#39;Flanders&#39;] + gdata.loc[&#39;Wallonia&#39;])) gdata.loc[&#39;Wallonia&#39;] += gdata.loc[&#39;Nan&#39;] * (gdata.loc[&#39;Wallonia&#39;]/(gdata.loc[&#39;Brussels&#39;] + gdata.loc[&#39;Flanders&#39;] + gdata.loc[&#39;Wallonia&#39;])) gdata.drop(index=&#39;Nan&#39;, inplace=True) gdata = np.round(gdata.fillna(0)).astype(int) return gdata . # Redistribute the nan for the column TESTS_ALL df_tests_all = (df_tests .fillna(&#39;Nan&#39;) .groupby([&#39;DATE&#39;]) .apply(redistribute, &#39;TESTS_ALL&#39;) .stack() .reset_index() .rename(columns={&#39;DATE&#39;:&#39;date&#39;, &#39;REGION&#39;:&#39;region&#39;, 0:&#39;total&#39;}) ) . # Redistribute the nan for the column TESTS_ALL_POS df_tests_positive = (df_tests .fillna(&#39;Nan&#39;) .groupby([&#39;DATE&#39;]) .apply(redistribute, &#39;TESTS_ALL_POS&#39;) .stack() .reset_index() .rename(columns={&#39;DATE&#39;:&#39;date&#39;, &#39;REGION&#39;:&#39;region&#39;, 0:&#39;positive&#39;}) ) . # Combine the total number of tests and the number of positive tests into a basetable df_tests_per_region_day = pd.concat([df_tests_all, df_tests_positive[&#39;positive&#39;]], axis=1).set_index([&#39;region&#39;, &#39;date&#39;]) . # Check if the basetable is ok assert df_tests_per_region_day.isnull().sum().sum() == 0, &#39;There are nan in the basetable&#39; . df_tests_per_region_day . total positive . region date . Brussels 2020-03-01 4 | 0 | . Flanders 2020-03-01 57 | 0 | . Wallonia 2020-03-01 21 | 0 | . Brussels 2020-03-02 17 | 3 | . Flanders 2020-03-02 259 | 7 | . ... ... | ... | . 2020-10-30 38154 | 6531 | . Wallonia 2020-10-30 21414 | 9011 | . Brussels 2020-10-31 1246 | 392 | . Flanders 2020-10-31 3542 | 531 | . Wallonia 2020-10-31 2827 | 951 | . 735 rows √ó 2 columns . # What regions do we have in the table ? df_tests_per_region_day.index.get_level_values(0).unique().to_list() . [&#39;Brussels&#39;, &#39;Flanders&#39;, &#39;Wallonia&#39;] . Re(t) for Flanders . region = &#39;Flanders&#39; . ax = df_tests_per_region_day.loc[region].plot(figsize=(18,6)) ax.set(title=f&#39;Number of tests for covid-19 and number of positives in {region}&#39;); . import datetime from dateutil.relativedelta import relativedelta . # Remove last two days because tests are not yet fully reported today_minus_two = datetime.datetime.today() + relativedelta(days=-2) today_minus_two.strftime(&quot;%Y-%m-%d&quot;) . &#39;2020-10-30&#39; . ax = df_tests_per_region_day.loc[region][:today_minus_two].plot(figsize=(18,6)) ax.set(title=f&#39;Number of tests for covid-19 and number of positives in {region}&#39;); . # Fit the model on the data df = df_tests_per_region_day.loc[region][:today_minus_two] gm = GenerativeModel(region, df) gm.sample() . Only 200 samples in chain. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [alpha, seed, log_r_t] . . 100.00% [3600/3600 1:10:58&lt;00:00 Sampling 4 chains, 0 divergences] Sampling 4 chains for 700 tune and 200 draw iterations (2_800 + 800 draws total) took 4260 seconds. . &lt;__main__.GenerativeModel at 0x7f382109add8&gt; . #collapse def summarize_inference_data(inference_data: az.InferenceData): &quot;&quot;&quot; Summarizes an inference_data object into the form that we publish on rt.live &quot;&quot;&quot; posterior = inference_data.posterior hdi_mass = 80 hpdi = az.hdi(posterior.r_t, hdi_prob=hdi_mass / 100).r_t observed_positive = inference_data.constant_data.observed_positive.to_series() scale_to_positives = lambda data: observed_positive.mean() / np.mean(data) * data tests = inference_data.constant_data.tests.to_series() normalized_positive = observed_positive / tests.clip(0.1 * tests.max()) summary = pd.DataFrame( data={ &quot;mean&quot;: posterior.r_t.mean([&quot;draw&quot;, &quot;chain&quot;]), &quot;median&quot;: posterior.r_t.median([&quot;chain&quot;, &quot;draw&quot;]), f&quot;lower_{hdi_mass}&quot;: hpdi[:, 0], f&quot;upper_{hdi_mass}&quot;: hpdi[:, 1], &quot;infections&quot;: scale_to_positives( posterior.infections.mean([&quot;draw&quot;, &quot;chain&quot;]) ), &quot;test_adjusted_positive&quot;: scale_to_positives( posterior.test_adjusted_positive.mean([&quot;draw&quot;, &quot;chain&quot;]) ), &quot;test_adjusted_positive_raw&quot;: scale_to_positives(normalized_positive), &quot;positive&quot;: observed_positive, &quot;tests&quot;: tests, }, index=pd.Index(posterior.date.values, name=&quot;date&quot;), ) return summary . . result = summarize_inference_data(gm.inference_data) . . 100.00% [800/800 00:12&lt;00:00] fig, ax = plt.subplots(figsize=(12, 8)) result.infections.plot(c=&quot;C2&quot;, label=&quot;Expected primary infections&quot;) result.test_adjusted_positive.plot(c=&quot;C0&quot;, label=&quot;Expected positive tests if tests were constant&quot;) result.test_adjusted_positive_raw.plot(c=&quot;C1&quot;, alpha=.5, label=&quot;Expected positive tests&quot;, style=&quot;--&quot;) gm.observed.positive.plot(c=&quot;C7&quot;, alpha=.7, label=&quot;Reported positive tests&quot;) fig.set_facecolor(&quot;w&quot;) ax.legend(); ax.set(title=f&quot;rt.live model inference for {region}&quot;, ylabel=&quot;number of cases&quot;) sns.despine(); . fig, ax = plt.subplots(figsize=(12, 8)) ax.set(title=f&quot;Effective reproduction number for {region}&quot;, ylabel=&quot;$R_e(t)$&quot;) samples = gm.trace[&quot;r_t&quot;] x = result.index cmap = plt.get_cmap(&quot;Reds&quot;) percs = np.linspace(51, 99, 40) colors = (percs - np.min(percs)) / (np.max(percs) - np.min(percs)) samples = samples.T result[&quot;median&quot;].plot(c=&quot;k&quot;, ls=&#39;-&#39;) for i, p in enumerate(percs[::-1]): upper = np.percentile(samples, p, axis=1) lower = np.percentile(samples, 100-p, axis=1) color_val = colors[i] ax.fill_between(x, upper, lower, color=cmap(color_val), alpha=.8) ax.axhline(1.0, c=&quot;k&quot;, lw=1, linestyle=&quot;--&quot;) sns.despine(); . Re(t) for Wallonia . region = &#39;Wallonia&#39; . ax = df_tests_per_region_day.loc[region].plot(figsize=(18,6)) ax.set(title=f&#39;Number of tests for covid-19 and number of positives in {region}&#39;); . # Remove last two days because tests are not yet fully reported today_minus_two = datetime.datetime.today() + relativedelta(days=-2) today_minus_two.strftime(&quot;%Y-%m-%d&quot;) . &#39;2020-10-30&#39; . ax = df_tests_per_region_day.loc[region][:today_minus_two].plot(figsize=(18,6)) ax.set(title=f&#39;Number of tests for covid-19 and number of positives in {region}&#39;); . df = df_tests_per_region_day.loc[region][:today_minus_two] gm = GenerativeModel(region, df) gm.sample() . Only 200 samples in chain. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Multiprocess sampling (4 chains in 4 jobs) NUTS: [alpha, seed, log_r_t] . . 100.00% [3600/3600 1:12:49&lt;00:00 Sampling 4 chains, 0 divergences] Sampling 4 chains for 700 tune and 200 draw iterations (2_800 + 800 draws total) took 4371 seconds. . &lt;__main__.GenerativeModel at 0x7f380ac35c88&gt; . result = summarize_inference_data(gm.inference_data) . . 100.00% [800/800 00:12&lt;00:00] fig, ax = plt.subplots(figsize=(12, 8)) result.infections.plot(c=&quot;C2&quot;, label=&quot;Expected primary infections&quot;) result.test_adjusted_positive.plot(c=&quot;C0&quot;, label=&quot;Expected positive tests if tests were constant&quot;) result.test_adjusted_positive_raw.plot(c=&quot;C1&quot;, alpha=.5, label=&quot;Expected positive tests&quot;, style=&quot;--&quot;) gm.observed.positive.plot(c=&quot;C7&quot;, alpha=.7, label=&quot;Reported positive tests&quot;) fig.set_facecolor(&quot;w&quot;) ax.legend(); ax.set(title=f&quot;rt.live model inference for {region}&quot;, ylabel=&quot;number of cases&quot;) sns.despine(); . fig, ax = plt.subplots(figsize=(12, 8)) ax.set(title=f&quot;Effective reproduction number for {region}&quot;, ylabel=&quot;$R_e(t)$&quot;) samples = gm.trace[&quot;r_t&quot;] x = result.index cmap = plt.get_cmap(&quot;Reds&quot;) percs = np.linspace(51, 99, 40) colors = (percs - np.min(percs)) / (np.max(percs) - np.min(percs)) samples = samples.T result[&quot;median&quot;].plot(c=&quot;k&quot;, ls=&#39;-&#39;) for i, p in enumerate(percs[::-1]): upper = np.percentile(samples, p, axis=1) lower = np.percentile(samples, 100-p, axis=1) color_val = colors[i] ax.fill_between(x, upper, lower, color=cmap(color_val), alpha=.8) ax.axhline(1.0, c=&quot;k&quot;, lw=1, linestyle=&quot;--&quot;) sns.despine(); .",
            "url": "https://cast42.github.io/blog/cast42/covid19/belgium/2020/11/01/rt-be-region.html",
            "relUrl": "/cast42/covid19/belgium/2020/11/01/rt-be-region.html",
            "date": " ‚Ä¢ Nov 1, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "My talk at data science leuven",
            "content": "The video is recorded on Youtube: . You can see the slides here: slides . The talk itself is based on this notebook that I published on this blog yesterday and that I used to demo during the talk. . The host of the conference was Istvan Hajnal. He tweeted the following: 5000 Portuguese white wines on slide 1. This presentation is going places. @cast42 of @colruyt at our 26th #dsleuven meetup. üç∏ pic.twitter.com/RN28wTWgca . &mdash; Data Science Leuven (@dsleuven) April 23, 2020 . He also took the R out of my family name NachteRgaele. Troubles with R, it&#39;s becoming a story of my life... :joy: Behind the scene Kris Peeters calmly took the heat of doing the live streaming. :+1: Almost Pydata quality! Big thanks to the whole Data Science Leuven team that is doing all this on voluntary basis. . Standing on the shoulders of the giants . This talks was not possible without the awesome Altair visualisation library made by Jake VanderPlas. Secondly, it builds upon the open source Shap library made by Scott Lundberg. Those two libraries had a major impact on my daily work as datascientist at Colruyt group. They inspired me in trying to give back to the open source community with this talk. :metal: . If you want to learn how to use Altair I recommend the tutorial made by Vincent Warmerdam on his calm code site: https://calmcode.io/altair/introduction.htm . I would also like to thank my collegues at work who endured the dry-run of this talk and who made the suggestion to try to use a classifier to explain the clustering result. Top team! . Awesome fastpages . Finally, this blog is build with the awesome fastpages. I can now share a rendered Jupyter notebook, with working interactive demos, that can be opened in My binder or Google Colab with one click on a button. This means that readers can directly tinker around with the code and methods discussed in the talk. All you need is a browser and an internet connection. So thank you Jeremy Howard, Hamel Husain, and the fastdotai team for pulling this off. Thank you Hamel Husain for your Github Actions. I will cast for two how awesome this all is. .",
            "url": "https://cast42.github.io/blog/cast42/dsleuven/clustering/altair/shap/2020/04/24/talk-datascience-leuven-explain-clustering.html",
            "relUrl": "/cast42/dsleuven/clustering/altair/shap/2020/04/24/talk-datascience-leuven-explain-clustering.html",
            "date": " ‚Ä¢ Apr 24, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Explain clusters to business with Altair and Shapley values",
            "content": "# Load python libraries for data handling and plotting import numpy as np import pandas as pd import altair as alt import matplotlib.pyplot as plt . # Load the data set with wines from the internet df = pd.read_csv(&#39;https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv&#39;, sep=&#39;;&#39;) . # Show the first lines of the data set to get an idea what&#39;s in there. df.head() . fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality . 0 7.0 | 0.27 | 0.36 | 20.7 | 0.045 | 45.0 | 170.0 | 1.0010 | 3.00 | 0.45 | 8.8 | 6 | . 1 6.3 | 0.30 | 0.34 | 1.6 | 0.049 | 14.0 | 132.0 | 0.9940 | 3.30 | 0.49 | 9.5 | 6 | . 2 8.1 | 0.28 | 0.40 | 6.9 | 0.050 | 30.0 | 97.0 | 0.9951 | 3.26 | 0.44 | 10.1 | 6 | . 3 7.2 | 0.23 | 0.32 | 8.5 | 0.058 | 47.0 | 186.0 | 0.9956 | 3.19 | 0.40 | 9.9 | 6 | . 4 7.2 | 0.23 | 0.32 | 8.5 | 0.058 | 47.0 | 186.0 | 0.9956 | 3.19 | 0.40 | 9.9 | 6 | . # How many wines and features do we have ? df.shape . (4898, 12) . df.columns . Index([&#39;fixed acidity&#39;, &#39;volatile acidity&#39;, &#39;citric acid&#39;, &#39;residual sugar&#39;, &#39;chlorides&#39;, &#39;free sulfur dioxide&#39;, &#39;total sulfur dioxide&#39;, &#39;density&#39;, &#39;pH&#39;, &#39;sulphates&#39;, &#39;alcohol&#39;, &#39;quality&#39;], dtype=&#39;object&#39;) . # Define the standard X (feature matrix) and target series y (not used in here) X = df.drop(columns=&#39;quality&#39;) all_features = X.columns y = df[&#39;quality&#39;] . df.describe() . fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol quality . count 4898.000000 | 4898.000000 | 4898.000000 | 4898.000000 | 4898.000000 | 4898.000000 | 4898.000000 | 4898.000000 | 4898.000000 | 4898.000000 | 4898.000000 | 4898.000000 | . mean 6.854788 | 0.278241 | 0.334192 | 6.391415 | 0.045772 | 35.308085 | 138.360657 | 0.994027 | 3.188267 | 0.489847 | 10.514267 | 5.877909 | . std 0.843868 | 0.100795 | 0.121020 | 5.072058 | 0.021848 | 17.007137 | 42.498065 | 0.002991 | 0.151001 | 0.114126 | 1.230621 | 0.885639 | . min 3.800000 | 0.080000 | 0.000000 | 0.600000 | 0.009000 | 2.000000 | 9.000000 | 0.987110 | 2.720000 | 0.220000 | 8.000000 | 3.000000 | . 25% 6.300000 | 0.210000 | 0.270000 | 1.700000 | 0.036000 | 23.000000 | 108.000000 | 0.991723 | 3.090000 | 0.410000 | 9.500000 | 5.000000 | . 50% 6.800000 | 0.260000 | 0.320000 | 5.200000 | 0.043000 | 34.000000 | 134.000000 | 0.993740 | 3.180000 | 0.470000 | 10.400000 | 6.000000 | . 75% 7.300000 | 0.320000 | 0.390000 | 9.900000 | 0.050000 | 46.000000 | 167.000000 | 0.996100 | 3.280000 | 0.550000 | 11.400000 | 6.000000 | . max 14.200000 | 1.100000 | 1.660000 | 65.800000 | 0.346000 | 289.000000 | 440.000000 | 1.038980 | 3.820000 | 1.080000 | 14.200000 | 9.000000 | . alt.Chart(df).mark_point().encode(x=&#39;fixed acidity&#39;, y=&#39;volatile acidity&#39;, color=&#39;quality:N&#39;) . # Add interactivity alt.Chart(df).mark_point().encode(x=&#39;fixed acidity&#39;, y=&#39;volatile acidity&#39;, color=&#39;quality:N&#39;, tooltip=[&#39;alcohol&#39;, &#39;quality&#39;]).interactive() . Scaling . From the describe, we see that domains of the feature differ widely. Feature &#39;fixed acidity&#39; has mean 6.854788, while feature &#39;volatile acidity&#39; has mean 0.278241. This is a sign that are on a different scale. We scale the features first so that we can use them together. . # When features have different scale we have to scale them so that we can use them together from sklearn.preprocessing import RobustScaler from sklearn.preprocessing import StandardScaler . scaler = StandardScaler() # scaler = RobustScaler() # Take the robust scaler when data contains outliers that you want to remove . X_scaled = scaler.fit_transform(X) . PCA . Perform principal component analysis to get an idea of the dimensionality of the wine dataset. Next reduce to two dimension so that we can make a 2D scatter plot were each dot is a wine from the set. . from sklearn.decomposition import PCA pca = PCA().fit(X_scaled) . df_plot = pd.DataFrame({&#39;Component Number&#39;: 1+np.arange(X.shape[1]), &#39;Cumulative explained variance&#39;: np.cumsum(pca.explained_variance_ratio_)}) alt.Chart(df_plot).mark_bar().encode( x=alt.X(&#39;Component Number:N&#39;, axis=alt.Axis(labelAngle=0)), y=alt.Y(&#39;Cumulative explained variance&#39;, axis=alt.Axis(format=&#39;%&#39;, title=&#39;Percentage&#39;)), tooltip=[alt.Tooltip(&#39;Cumulative explained variance&#39;, format=&#39;.0%&#39;)] ).properties( title=&#39;Cumulative explained variance&#39; ).properties( width=400 ) . We see that almost all features are needed to explain all the variance. Only with 9 comoponent we can explain 97% of the variance. This means that we should use all features from the dataset and feature selection is not necessary in this case. . PCA with 2 Dimensions . Perform dimension reduction to two dimentions with PCA to be able to draw all the wines in one graph (not for the clustering!) . twod_pca = PCA(n_components=2) X_pca = twod_pca.fit_transform(X_scaled) . # Add first to PCA components to the dataset so we can plot it df[&#39;pca1&#39;] = X_pca[:,0] df[&#39;pca2&#39;] = X_pca[:,1] . df[&#39;member&#39;] = 1 df.groupby(&#39;quality&#39;)[&#39;member&#39;].transform(&#39;count&#39;).div(df.shape[0]) . 0 0.448755 1 0.448755 2 0.448755 3 0.448755 4 0.448755 ... 4893 0.448755 4894 0.297468 4895 0.448755 4896 0.179665 4897 0.448755 Name: member, Length: 4898, dtype: float64 . selection = alt.selection_multi(fields=[&#39;quality&#39;], bind=&#39;legend&#39;) df[&#39;quality_weight&#39;] = df[&#39;quality&#39;].map(df[&#39;quality&#39;].value_counts(normalize=True).to_dict()) # Draw 20% stratified sample alt.Chart(df.sample(1000, weights=&#39;quality_weight&#39;)).mark_circle(size=60).encode( x=alt.X(&#39;pca1&#39;, title=&#39;First component&#39;), y=alt.Y(&#39;pca2&#39;, title=&#39;Second component&#39;), color=alt.Color(&#39;quality:N&#39;), tooltip=[&#39;quality&#39;], opacity=alt.condition(selection, alt.value(1), alt.value(0.2)) ).properties( title=&#39;PCA analyse&#39;, width=600, height=400 ).add_selection( selection ) . df_twod_pca = pd.DataFrame(data=twod_pca.components_.T, columns=[&#39;pca1&#39;, &#39;pca2&#39;], index=X.columns) . pca1 = alt.Chart(df_twod_pca.reset_index()).mark_bar().encode( y=alt.Y(&#39;index:O&#39;, title=None), x=&#39;pca1&#39;, color=alt.Color(&#39;pca1&#39;, scale=alt.Scale(scheme=&#39;viridis&#39;)), tooltip = [alt.Tooltip(&#39;index&#39;, title=&#39;Feature&#39;), alt.Tooltip(&#39;pca1&#39;, format=&#39;.2f&#39;)] ) pca2 = alt.Chart(df_twod_pca.reset_index()).mark_bar().encode( y=alt.Y(&#39;index:O&#39;, title=None), x=&#39;pca2&#39;, color=alt.Color(&#39;pca2&#39;, scale=alt.Scale(scheme=&#39;viridis&#39;)), tooltip = [alt.Tooltip(&#39;index&#39;, title=&#39;Feature&#39;), alt.Tooltip(&#39;pca2&#39;, format=&#39;.2f&#39;)] ) (pca1 &amp; pca2).properties( title=&#39;Loadings of the first two principal components&#39; ) . Kmeans clustering . Determine the number of cluster for Kmeans clustering by lopping from 2 to 11 cluster. Calculate for each result the within-cluster variation (inertia), the silhoutte) and the Davies-Bouldin index. Use the elbow method to determine the number of clusters. The elbow method seeks the value of k after which the clustering quality improves only marginally. . from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score, davies_bouldin_score . km_scores= [] km_silhouette = [] km_db_score = [] for i in range(2,X.shape[1]): km = KMeans(n_clusters=i, random_state=1301).fit(X_scaled) preds = km.predict(X_scaled) print(f&#39;Score for number of cluster(s) {i}: {km.score(X_scaled):.3f}&#39;) km_scores.append(-km.score(X_scaled)) silhouette = silhouette_score(X_scaled,preds) km_silhouette.append(silhouette) print(f&#39;Silhouette score for number of cluster(s) {i}: {silhouette:.3f}&#39;) db = davies_bouldin_score(X_scaled,preds) km_db_score.append(db) print(f&#39;Davies Bouldin score for number of cluster(s) {i}: {db:.3f}&#39;) print(&#39;-&#39;*100) . Score for number of cluster(s) 2: -42548.672 Silhouette score for number of cluster(s) 2: 0.214 Davies Bouldin score for number of cluster(s) 2: 1.775 - Score for number of cluster(s) 3: -39063.495 Silhouette score for number of cluster(s) 3: 0.144 Davies Bouldin score for number of cluster(s) 3: 2.097 - Score for number of cluster(s) 4: -35986.918 Silhouette score for number of cluster(s) 4: 0.159 Davies Bouldin score for number of cluster(s) 4: 1.809 - Score for number of cluster(s) 5: -33699.043 Silhouette score for number of cluster(s) 5: 0.144 Davies Bouldin score for number of cluster(s) 5: 1.768 - Score for number of cluster(s) 6: -31973.251 Silhouette score for number of cluster(s) 6: 0.146 Davies Bouldin score for number of cluster(s) 6: 1.693 - Score for number of cluster(s) 7: -30552.998 Silhouette score for number of cluster(s) 7: 0.126 Davies Bouldin score for number of cluster(s) 7: 1.847 - Score for number of cluster(s) 8: -29361.568 Silhouette score for number of cluster(s) 8: 0.128 Davies Bouldin score for number of cluster(s) 8: 1.790 - Score for number of cluster(s) 9: -28198.302 Silhouette score for number of cluster(s) 9: 0.128 Davies Bouldin score for number of cluster(s) 9: 1.760 - Score for number of cluster(s) 10: -27444.897 Silhouette score for number of cluster(s) 10: 0.118 Davies Bouldin score for number of cluster(s) 10: 1.843 - . df_plot = pd.DataFrame({&#39;Number of clusters&#39;:[i for i in range(2,X.shape[1])],&#39;kmean score&#39;: km_scores}) alt.Chart(df_plot).mark_line(point=True).encode( x=alt.X(&#39;Number of clusters:N&#39;, axis=alt.Axis(labelAngle=0)), y=alt.Y(&#39;kmean score&#39;), tooltip=[alt.Tooltip(&#39;kmean score&#39;, format=&#39;.2f&#39;)] ).properties( title=&#39;Kmean score ifo number of cluster&#39; ).properties( title=&#39;The elbow method for determining number of clusters&#39;, width=400 ) . df_plot = pd.DataFrame({&#39;Number of clusters&#39;:[i for i in range(2,X.shape[1])],&#39;silhouette score&#39;: km_silhouette}) alt.Chart(df_plot).mark_line(point=True).encode( x=alt.X(&#39;Number of clusters:N&#39;, axis=alt.Axis(labelAngle=0)), y=alt.Y(&#39;silhouette score&#39;), tooltip=[alt.Tooltip(&#39;silhouette score&#39;, format=&#39;.2f&#39;)] ).properties( title=&#39;Silhouette score ifo number of cluster&#39;, width=400 ) . df_plot = pd.DataFrame({&#39;Number of clusters&#39;:[i for i in range(2, X.shape[1])],&#39;davies bouldin score&#39;: km_db_score}) alt.Chart(df_plot).mark_line(point=True).encode( x=alt.X(&#39;Number of clusters:N&#39;, axis=alt.Axis(labelAngle=0)), y=alt.Y(&#39;davies bouldin score&#39;), tooltip=[alt.Tooltip(&#39;davies bouldin score&#39;, format=&#39;.2f&#39;)] ).properties( title=&#39;Davies Bouldin score ifo number of cluster&#39;, width=400 ) . from sklearn.metrics import silhouette_samples, silhouette_score import matplotlib.cm as cm X = X.values range_n_clusters = [2, 3, 4, 5, 6] for n_clusters in range_n_clusters: # Create a subplot with 1 row and 2 columns fig, (ax1, ax2) = plt.subplots(1, 2) fig.set_size_inches(18, 7) # The 1st subplot is the silhouette plot # The silhouette coefficient can range from -1, 1 but in this example all # lie within [-0.1, 1] ax1.set_xlim([-0.1, 1]) # The (n_clusters+1)*10 is for inserting blank space between silhouette # plots of individual clusters, to demarcate them clearly. ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10]) # Initialize the clusterer with n_clusters value and a random generator # seed of 10 for reproducibility. clusterer = KMeans(n_clusters=n_clusters, random_state=10) cluster_labels = clusterer.fit_predict(X) # The silhouette_score gives the average value for all the samples. # This gives a perspective into the density and separation of the formed # clusters silhouette_avg = silhouette_score(X, cluster_labels) print(&quot;For n_clusters =&quot;, n_clusters, &quot;The average silhouette_score is :&quot;, silhouette_avg) # Compute the silhouette scores for each sample sample_silhouette_values = silhouette_samples(X, cluster_labels) y_lower = 10 for i in range(n_clusters): # Aggregate the silhouette scores for samples belonging to # cluster i, and sort them ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i] ith_cluster_silhouette_values.sort() size_cluster_i = ith_cluster_silhouette_values.shape[0] y_upper = y_lower + size_cluster_i color = cm.nipy_spectral(float(i) / n_clusters) ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values, facecolor=color, edgecolor=color, alpha=0.7) # Label the silhouette plots with their cluster numbers at the middle ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i)) # Compute the new y_lower for next plot y_lower = y_upper + 10 # 10 for the 0 samples ax1.set_title(&quot;The silhouette plot for the various clusters.&quot;) ax1.set_xlabel(&quot;The silhouette coefficient values&quot;) ax1.set_ylabel(&quot;Cluster label&quot;) # The vertical line for average silhouette score of all the values ax1.axvline(x=silhouette_avg, color=&quot;red&quot;, linestyle=&quot;--&quot;) ax1.set_yticks([]) # Clear the yaxis labels / ticks ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1]) # 2nd Plot showing the actual clusters formed colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters) ax2.scatter(X[:, 0], X[:, 1], marker=&#39;.&#39;, s=30, lw=0, alpha=0.7, c=colors, edgecolor=&#39;k&#39;) # Labeling the clusters centers = clusterer.cluster_centers_ # Draw white circles at cluster centers ax2.scatter(centers[:, 0], centers[:, 1], marker=&#39;o&#39;, c=&quot;white&quot;, alpha=1, s=200, edgecolor=&#39;k&#39;) for i, c in enumerate(centers): ax2.scatter(c[0], c[1], marker=&#39;$%d$&#39; % i, alpha=1, s=50, edgecolor=&#39;k&#39;) ax2.set_title(&quot;The visualization of the clustered data.&quot;) ax2.set_xlabel(&quot;Feature space for the 1st feature&quot;) ax2.set_ylabel(&quot;Feature space for the 2nd feature&quot;) plt.suptitle((&quot;Silhouette analysis for KMeans clustering on sample data &quot; &quot;with n_clusters = %d&quot; % n_clusters), fontsize=14, fontweight=&#39;bold&#39;) plt.show() . For n_clusters = 2 The average silhouette_score is : 0.5062782327345698 For n_clusters = 3 The average silhouette_score is : 0.4125412743885912 For n_clusters = 4 The average silhouette_score is : 0.3748324919186734 For n_clusters = 5 The average silhouette_score is : 0.3437053439455249 For n_clusters = 6 The average silhouette_score is : 0.313821767576001 . Make three clusters . Since we have a high Davies Boldin and low Silhoutte score for k=3 we select to cluster into three clusters. Another option could be to use the Gaussian Likelihood score. In this notebook another analysis reported also 3 clusters. . km = KMeans(n_clusters=3, random_state=1301).fit(X_scaled) preds = km.predict(X_scaled) pd.Series(preds).value_counts() # How many wines are in each cluster ? . 0 1801 1 1629 2 1468 dtype: int64 . df_km = pd.DataFrame(data={&#39;pca1&#39;:X_pca[:,0], &#39;pca2&#39;:X_pca [:,1], &#39;cluster&#39;:preds}) . # Add the scaled data with the input features for i,c in enumerate(all_features): df_km[c] = X_scaled[:,i] . domain = [0, 1, 2] range_ = [&#39;red&#39;, &#39;darkblue&#39;, &#39;green&#39;] selection = alt.selection_multi(fields=[&#39;cluster&#39;], bind=&#39;legend&#39;) pca = alt.Chart(df_km).mark_circle(size=20).encode( x=&#39;pca1&#39;, y=&#39;pca2&#39;, color=alt.Color(&#39;cluster:N&#39;, scale=alt.Scale(domain=domain, range=range_)), opacity=alt.condition(selection, alt.value(1), alt.value(0.1)), tooltip = list(all_features) ).add_selection( selection ) pca . Now we will plot the all the wines on a two dimensional plane. On the right, you get boxplots for every feature. The cool thing is now that with the mouse you can select by drawing a brush over the points (click, hold button and drag the mouse) and get immediate updates boxplots of the features of the selected wines. A such you can interactively gain some insight in what the cluster might mean. . brush = alt.selection(type=&#39;interval&#39;) domain = [0, 1, 2] range_ = [&#39;red&#39;, &#39;darkblue&#39;, &#39;green&#39;] points = alt.Chart(df_km).mark_circle(size=60).encode( x=&#39;pca1&#39;, y=&#39;pca2&#39;, color = alt.condition(brush, &#39;cluster:N&#39;, alt.value(&#39;lightgray&#39;)), tooltip = list(all_features) ).add_selection(brush) boxplots = alt.vconcat() for measure in all_features: boxplot = alt.Chart(df_km).mark_boxplot().encode( x =alt.X(measure, axis=alt.Axis(titleX=470, titleY=0)), ).transform_filter( brush ) boxplots &amp;= boxplot chart = alt.hconcat(points, boxplots) # chart.save(&#39;cluster_pca_n_3.html&#39;)) chart . AS you can&#39;t select al wine from one cluster with the rectangular brush, we make a plot for each cluster and boxplots of the features values of that cluster. . def plot_cluster(df, selected_columns, clusternr): points = alt.Chart(df).mark_circle(size=60).encode( x=alt.X(&#39;pca1&#39;, title=&#39;Principal component 1 (pca1)&#39;), y=alt.Y(&#39;pca2&#39;, title=&#39;Principal component 2 (pca1)&#39;), color = alt.condition(alt.FieldEqualPredicate(field=&#39;cluster&#39;, equal=clusternr), &#39;cluster:N&#39;, alt.value(&#39;lightgray&#39;)), tooltip = list(all_features)+[&#39;cluster&#39;], ) boxplots = alt.vconcat() for measure in [c for c in selected_columns]: boxplot = alt.Chart(df).mark_boxplot().encode( x =alt.X(measure, axis=alt.Axis(titleX=480, titleY=0)), ).transform_filter( alt.FieldEqualPredicate(field=&#39;cluster&#39;, equal=clusternr) ) boxplots &amp;= boxplot return points, boxplots . points, boxplots = plot_cluster(df_km, all_features, 0) c0 = alt.hconcat(points, boxplots).properties(title=&#39;Cluster 0&#39;) points, boxplots = plot_cluster(df_km, all_features, 1) c1 = alt.hconcat(points, boxplots).properties(title=&#39;Cluster 1&#39;) points, boxplots = plot_cluster(df_km, all_features, 2) c2 = alt.hconcat(points, boxplots).properties(title=&#39;Cluster 2&#39;) . alt.vconcat(c0, c1, c2) . I might still be difficult to explain the clusters. We will now build a multiclass classifier to predict the cluster from the features. Next we will use the Shapley values to explain the clusters. . Light gbm classifier . import lightgbm as lgb . params = lgb.LGBMClassifier().get_params() params . {&#39;boosting_type&#39;: &#39;gbdt&#39;, &#39;class_weight&#39;: None, &#39;colsample_bytree&#39;: 1.0, &#39;importance_type&#39;: &#39;split&#39;, &#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: -1, &#39;min_child_samples&#39;: 20, &#39;min_child_weight&#39;: 0.001, &#39;min_split_gain&#39;: 0.0, &#39;n_estimators&#39;: 100, &#39;n_jobs&#39;: -1, &#39;num_leaves&#39;: 31, &#39;objective&#39;: None, &#39;random_state&#39;: None, &#39;reg_alpha&#39;: 0.0, &#39;reg_lambda&#39;: 0.0, &#39;silent&#39;: True, &#39;subsample&#39;: 1.0, &#39;subsample_for_bin&#39;: 200000, &#39;subsample_freq&#39;: 0} . params[&#39;objective&#39;] = &#39;multiclass&#39; # the target to predict is the number of the cluster params[&#39;is_unbalance&#39;] = True params[&#39;n_jobs&#39;] = -1 params[&#39;random_state&#39;] = 1301 . mdl = lgb.LGBMClassifier(**params) . X = df.drop(columns=[&#39;quality&#39;, &#39;pca1&#39;, &#39;pca2&#39;, &#39;member&#39;]) . mdl.fit(X, preds) . LGBMClassifier(boosting_type=&#39;gbdt&#39;, class_weight=None, colsample_bytree=1.0, importance_type=&#39;split&#39;, is_unbalance=True, learning_rate=0.1, max_depth=-1, min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0, n_estimators=100, n_jobs=-1, num_leaves=31, objective=&#39;multiclass&#39;, random_state=1301, reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0, subsample_for_bin=200000, subsample_freq=0) . y_pred = mdl.predict_proba(X) . # Install the shap library to caculate the Shapley values !pip install shap . Requirement already satisfied: shap in /usr/local/lib/python3.6/dist-packages (0.35.0) Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from shap) (1.0.3) Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from shap) (0.22.2.post1) Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from shap) (1.4.1) Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from shap) (1.18.2) Requirement already satisfied: tqdm&gt;4.25.0 in /usr/local/lib/python3.6/dist-packages (from shap) (4.38.0) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas-&gt;shap) (2018.9) Requirement already satisfied: python-dateutil&gt;=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas-&gt;shap) (2.8.1) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn-&gt;shap) (0.14.1) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil&gt;=2.6.1-&gt;pandas-&gt;shap) (1.12.0) . import shap . explainer = shap.TreeExplainer(mdl) shap_values = explainer.shap_values(X) . Setting feature_perturbation = &#34;tree_path_dependent&#34; because no background data was given. . shap.summary_plot(shap_values, X, max_display=30) . From the Shapley values we see that the feature density has the highest impact on the model to predict the clusters. Let&#39;s have a look the Shapley values per cluster. The acidity features pH and fixed acidity has only impact on cluster 1 and 2 but almost none on cluster 0. . Shapley values for the three clusters . for cnr in df_km[&#39;cluster&#39;].unique(): shap.summary_plot(shap_values[cnr], X, max_display=30, show=False) plt.title(f&#39;Cluster {cnr}&#39;) plt.show() . To explain the clusters, we will plot the three clusters and the boxplot of the features ordered with the feature importance. . Explain cluster 0 . cnr = 0 feature_order = np.argsort(np.sum(np.abs(shap_values[cnr]), axis=0)) points, boxplots = plot_cluster(df_km, [X.columns[i] for i in feature_order][::-1][:6], 0) c0 = alt.hconcat(points, boxplots).properties(title=&#39;Cluster 0&#39;) c0 . cnr = 0 shap.summary_plot(shap_values[cnr], X, max_display=30, show=False) plt.title(f&#39;Cluster {cnr}&#39;) plt.show() . Cluster 0 can be describe as wines with: . high density | high total sulfur dioxide | high free sulfur dioxide | . Explain cluster 1 . X.columns . Index([&#39;fixed acidity&#39;, &#39;volatile acidity&#39;, &#39;citric acid&#39;, &#39;residual sugar&#39;, &#39;chlorides&#39;, &#39;free sulfur dioxide&#39;, &#39;total sulfur dioxide&#39;, &#39;density&#39;, &#39;pH&#39;, &#39;sulphates&#39;, &#39;alcohol&#39;, &#39;quality_weight&#39;], dtype=&#39;object&#39;) . cnr = 1 feature_order = np.argsort(np.sum(np.abs(shap_values[cnr]), axis=0)) points, boxplots = plot_cluster(df_km, [X.columns[i] for i in feature_order][::-1][:6], cnr) c1 = alt.hconcat(points, boxplots).properties(title=f&#39;Cluster {cnr}&#39;) c1 . cnr = 1 shap.summary_plot(shap_values[cnr], X, max_display=30, show=False) plt.title(f&#39;Cluster {cnr}&#39;) plt.show() . Cluster 1 contains wines with: . high pH | low fixed acidity | low density (opposite of cluster 0) | low citric acid | high on sulphates | . Explain cluster 2 . cnr = 2 feature_order = np.argsort(np.sum(np.abs(shap_values[cnr]), axis=0)) points, boxplots = plot_cluster(df_km, [X.columns[i] for i in feature_order][::-1][:6], cnr) c2 = alt.hconcat(points, boxplots).properties(title=f&#39;Cluster {cnr}&#39;) c2 . cnr = 2 shap.summary_plot(shap_values[cnr], X, max_display=30, show=False) plt.title(f&#39;Cluster {cnr}&#39;) plt.show() . Cluster 2 contains wines with: . high fixed acidity | low pH (opposite of cluster 1) | low density (opposite of cluster 0) | . How is the quality of the whines in each cluster ? . df_km[&#39;quality&#39;] = y df_km.groupby(&#39;cluster&#39;)[&#39;quality&#39;].describe() . count mean std min 25% 50% 75% max . cluster . 0 1801.0 | 5.606330 | 0.750514 | 3.0 | 5.0 | 6.0 | 6.0 | 8.0 | . 1 1629.0 | 6.150399 | 0.905265 | 3.0 | 6.0 | 6.0 | 7.0 | 9.0 | . 2 1468.0 | 5.908719 | 0.918554 | 3.0 | 5.0 | 6.0 | 6.0 | 9.0 | . # Let&#39;s plot the distributions of quality score per cluster alt.Chart(df_km).transform_density( density=&#39;quality&#39;, bandwidth=0.3, groupby=[&#39;cluster&#39;], extent= [3, 9], counts = True, steps=200 ).mark_area().encode( alt.X(&#39;value:Q&#39;), alt.Y(&#39;density:Q&#39;, stack=&#39;zero&#39;), alt.Color(&#39;cluster:N&#39;) ).properties( title=&#39;Distribution of quality of the wines per cluster&#39;, width=400, height=100 ) . We see that the quality has a similar distribution in each cluster. .",
            "url": "https://cast42.github.io/blog/datascience/python/clustering/altair/shap/2020/04/23/explain-clusters-to-business.html",
            "relUrl": "/datascience/python/clustering/altair/shap/2020/04/23/explain-clusters-to-business.html",
            "date": " ‚Ä¢ Apr 23, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Regional covid-19 mortality in Belgium per gender and age",
            "content": "# Import pandas for data wrangling and Altair for plotting import pandas as pd import altair as alt . df_tot_sc = pd.read_excel(&#39;https://epistat.sciensano.be/Data/COVID19BE.xlsx&#39;) . df_inhab = pd.read_excel(&#39;https://statbel.fgov.be/sites/default/files/files/opendata/bevolking%20naar%20woonplaats%2C%20nationaliteit%20burgelijke%20staat%20%2C%20leeftijd%20en%20geslacht/TF_SOC_POP_STRUCT_2019.xlsx&#39;) . df_inhab . CD_REFNIS TX_DESCR_NL TX_DESCR_FR CD_DSTR_REFNIS TX_ADM_DSTR_DESCR_NL TX_ADM_DSTR_DESCR_FR CD_PROV_REFNIS TX_PROV_DESCR_NL TX_PROV_DESCR_FR CD_RGN_REFNIS TX_RGN_DESCR_NL TX_RGN_DESCR_FR CD_SEX CD_NATLTY TX_NATLTY_NL TX_NATLTY_FR CD_CIV_STS TX_CIV_STS_NL TX_CIV_STS_FR CD_AGE MS_POPULATION . 0 11001 | Aartselaar | Aartselaar | 11000 | Arrondissement Antwerpen | Arrondissement d‚ÄôAnvers | 10000.0 | Provincie Antwerpen | Province d‚ÄôAnvers | 2000 | Vlaams Gewest | R√©gion flamande | F | BEL | Belgen | Belges | 4 | Gescheiden | Divorc√© | 69 | 11 | . 1 11001 | Aartselaar | Aartselaar | 11000 | Arrondissement Antwerpen | Arrondissement d‚ÄôAnvers | 10000.0 | Provincie Antwerpen | Province d‚ÄôAnvers | 2000 | Vlaams Gewest | R√©gion flamande | F | BEL | Belgen | Belges | 4 | Gescheiden | Divorc√© | 80 | 3 | . 2 11001 | Aartselaar | Aartselaar | 11000 | Arrondissement Antwerpen | Arrondissement d‚ÄôAnvers | 10000.0 | Provincie Antwerpen | Province d‚ÄôAnvers | 2000 | Vlaams Gewest | R√©gion flamande | M | BEL | Belgen | Belges | 4 | Gescheiden | Divorc√© | 30 | 2 | . 3 11001 | Aartselaar | Aartselaar | 11000 | Arrondissement Antwerpen | Arrondissement d‚ÄôAnvers | 10000.0 | Provincie Antwerpen | Province d‚ÄôAnvers | 2000 | Vlaams Gewest | R√©gion flamande | F | BEL | Belgen | Belges | 4 | Gescheiden | Divorc√© | 48 | 26 | . 4 11001 | Aartselaar | Aartselaar | 11000 | Arrondissement Antwerpen | Arrondissement d‚ÄôAnvers | 10000.0 | Provincie Antwerpen | Province d‚ÄôAnvers | 2000 | Vlaams Gewest | R√©gion flamande | F | BEL | Belgen | Belges | 4 | Gescheiden | Divorc√© | 76 | 2 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 463376 93090 | Viroinval | Viroinval | 93000 | Arrondissement Philippeville | Arrondissement de Philippeville | 90000.0 | Provincie Namen | Province de Namur | 3000 | Waals Gewest | R√©gion wallonne | F | BEL | Belgen | Belges | 3 | Weduwstaat | Veuf | 73 | 10 | . 463377 93090 | Viroinval | Viroinval | 93000 | Arrondissement Philippeville | Arrondissement de Philippeville | 90000.0 | Provincie Namen | Province de Namur | 3000 | Waals Gewest | R√©gion wallonne | M | BEL | Belgen | Belges | 3 | Weduwstaat | Veuf | 64 | 1 | . 463378 93090 | Viroinval | Viroinval | 93000 | Arrondissement Philippeville | Arrondissement de Philippeville | 90000.0 | Provincie Namen | Province de Namur | 3000 | Waals Gewest | R√©gion wallonne | M | BEL | Belgen | Belges | 3 | Weduwstaat | Veuf | 86 | 3 | . 463379 93090 | Viroinval | Viroinval | 93000 | Arrondissement Philippeville | Arrondissement de Philippeville | 90000.0 | Provincie Namen | Province de Namur | 3000 | Waals Gewest | R√©gion wallonne | M | ETR | niet-Belgen | non-Belges | 3 | Weduwstaat | Veuf | 74 | 1 | . 463380 93090 | Viroinval | Viroinval | 93000 | Arrondissement Philippeville | Arrondissement de Philippeville | 90000.0 | Provincie Namen | Province de Namur | 3000 | Waals Gewest | R√©gion wallonne | M | BEL | Belgen | Belges | 3 | Weduwstaat | Veuf | 52 | 1 | . 463381 rows √ó 21 columns . inhab_provence = df_inhab[&#39;TX_PROV_DESCR_NL&#39;].dropna().unique() inhab_provence . array([&#39;Provincie Antwerpen&#39;, &#39;Provincie Vlaams-Brabant&#39;, &#39;Provincie Waals-Brabant&#39;, &#39;Provincie West-Vlaanderen&#39;, &#39;Provincie Oost-Vlaanderen&#39;, &#39;Provincie Henegouwen&#39;, &#39;Provincie Luik&#39;, &#39;Provincie Limburg&#39;, &#39;Provincie Luxemburg&#39;, &#39;Provincie Namen&#39;], dtype=object) . sc_provence = df_tot_sc[&#39;PROVINCE&#39;].unique() sc_provence . array([&#39;Brussels&#39;, &#39;Li√®ge&#39;, &#39;Limburg&#39;, &#39;OostVlaanderen&#39;, &#39;VlaamsBrabant&#39;, &#39;Antwerpen&#39;, &#39;WestVlaanderen&#39;, &#39;BrabantWallon&#39;, &#39;Hainaut&#39;, &#39;Namur&#39;, nan, &#39;Luxembourg&#39;], dtype=object) . [p.split()[1] for p in inhab_provence] . [&#39;Antwerpen&#39;, &#39;Vlaams-Brabant&#39;, &#39;Waals-Brabant&#39;, &#39;West-Vlaanderen&#39;, &#39;Oost-Vlaanderen&#39;, &#39;Henegouwen&#39;, &#39;Luik&#39;, &#39;Limburg&#39;, &#39;Luxemburg&#39;, &#39;Namen&#39;] . map_statbel_provence_to_sc_provence = {&#39;Provincie Antwerpen&#39;:&#39;Antwerpen&#39;, &#39;Provincie Vlaams-Brabant&#39;:&#39;VlaamsBrabant&#39;, &#39;Provincie Waals-Brabant&#39;:&#39;BrabantWallon&#39;, &#39;Provincie West-Vlaanderen&#39;:&#39;WestVlaanderen&#39;, &#39;Provincie Oost-Vlaanderen&#39;:&#39;OostVlaanderen&#39;, &#39;Provincie Henegouwen&#39;:&#39;Hainaut&#39;, &#39;Provincie Luik&#39;:&#39;Li√®ge&#39;, &#39;Provincie Limburg&#39;:&#39;Limburg&#39;, &#39;Provincie Luxemburg&#39;:&#39;Luxembourg&#39;, &#39;Provincie Namen&#39;:&#39;Namur&#39;} . df_inhab[&#39;sc_provence&#39;] = df_inhab[&#39;TX_PROV_DESCR_NL&#39;].map(map_statbel_provence_to_sc_provence) . df_tot_sc[&#39;AGEGROUP&#39;].unique() . array([&#39;10-19&#39;, &#39;20-29&#39;, &#39;30-39&#39;, &#39;40-49&#39;, &#39;50-59&#39;, &#39;70-79&#39;, &#39;60-69&#39;, &#39;0-9&#39;, &#39;90+&#39;, &#39;80-89&#39;, nan], dtype=object) . df_inhab[&#39;AGEGROUP&#39;] =pd.cut(df_inhab[&#39;CD_AGE&#39;], bins=[0,10,20,30,40,50,60,70,80,90,200], labels=[&#39;0-9&#39;,&#39;10-19&#39;,&#39;20-29&#39;,&#39;30-39&#39;,&#39;40-49&#39;,&#39;50-59&#39;,&#39;60-69&#39;,&#39;70-79&#39;,&#39;80-89&#39;,&#39;90+&#39;], include_lowest=True) . df_inhab_gender_prov = df_inhab.groupby([&#39;sc_provence&#39;, &#39;CD_SEX&#39;, &#39;AGEGROUP&#39;])[&#39;MS_POPULATION&#39;].sum().reset_index() . df_inhab_gender_prov_cases = pd.merge(df_inhab_gender_prov, df_tot_sc.dropna(), left_on=[&#39;sc_provence&#39;, &#39;AGEGROUP&#39;, &#39;CD_SEX&#39;], right_on=[&#39;PROVINCE&#39;, &#39;AGEGROUP&#39;, &#39;SEX&#39;]) . df_inhab_gender_prov_cases.head() . sc_provence CD_SEX AGEGROUP MS_POPULATION DATE PROVINCE REGION SEX CASES . 0 Antwerpen | F | 0-9 | 113851 | 2020-03-05 | Antwerpen | Flanders | F | 1 | . 1 Antwerpen | F | 0-9 | 113851 | 2020-03-18 | Antwerpen | Flanders | F | 1 | . 2 Antwerpen | F | 0-9 | 113851 | 2020-03-26 | Antwerpen | Flanders | F | 1 | . 3 Antwerpen | F | 0-9 | 113851 | 2020-03-30 | Antwerpen | Flanders | F | 1 | . 4 Antwerpen | F | 0-9 | 113851 | 2020-04-03 | Antwerpen | Flanders | F | 1 | . df_plot = df_inhab_gender_prov_cases.groupby([&#39;SEX&#39;, &#39;AGEGROUP&#39;, &#39;PROVINCE&#39;]).agg(CASES = (&#39;CASES&#39;, &#39;sum&#39;), MS_POPULATION=(&#39;MS_POPULATION&#39;, &#39;first&#39;)).reset_index() df_plot . SEX AGEGROUP PROVINCE CASES MS_POPULATION . 0 F | 0-9 | Antwerpen | 9 | 113851 | . 1 F | 0-9 | BrabantWallon | 3 | 23744 | . 2 F | 0-9 | Hainaut | 11 | 81075 | . 3 F | 0-9 | Limburg | 11 | 48102 | . 4 F | 0-9 | Li√®ge | 19 | 67479 | . ... ... | ... | ... | ... | ... | . 195 M | 90+ | Luxembourg | 17 | 469 | . 196 M | 90+ | Namur | 27 | 827 | . 197 M | 90+ | OostVlaanderen | 102 | 3105 | . 198 M | 90+ | VlaamsBrabant | 129 | 2611 | . 199 M | 90+ | WestVlaanderen | 121 | 3292 | . 200 rows √ó 5 columns . df_plot[&#39;PROVINCE&#39;].unique() . array([&#39;Antwerpen&#39;, &#39;BrabantWallon&#39;, &#39;Hainaut&#39;, &#39;Limburg&#39;, &#39;Li√®ge&#39;, &#39;Luxembourg&#39;, &#39;Namur&#39;, &#39;OostVlaanderen&#39;, &#39;VlaamsBrabant&#39;, &#39;WestVlaanderen&#39;], dtype=object) . alt.Chart(df_plot).mark_bar().encode(x=&#39;AGEGROUP:N&#39;, y=&#39;CASES&#39;, color=&#39;SEX:N&#39;, column=&#39;PROVINCE:N&#39;) . df_plot[&#39;percentage&#39;] = df_plot[&#39;CASES&#39;] / df_plot[&#39;MS_POPULATION&#39;] . alt.Chart(df_plot).mark_bar().encode(x=&#39;AGEGROUP:N&#39;, y=&#39;percentage&#39;, color=&#39;SEX:N&#39;, column=&#39;PROVINCE:N&#39;) . Let&#39;s add a colorscale the makes the male blue and female number pink. . color_scale = alt.Scale(domain=[&#39;M&#39;, &#39;F&#39;], range=[&#39;#1f77b4&#39;, &#39;#e377c2&#39;]) . alt.Chart(df_plot).mark_bar().encode( x=&#39;AGEGROUP:N&#39;, y=&#39;percentage&#39;, color=alt.Color(&#39;SEX:N&#39;, scale=color_scale, legend=None), column=&#39;PROVINCE:N&#39;) . The graph&#39;s get to wide. Let&#39;s use faceting to make two rows. . Inspired and based on https://altair-viz.github.io/gallery/us_population_pyramid_over_time.html . #slider = alt.binding_range(min=1850, max=2000, step=10) # select_province = alt.selection_single(name=&#39;PROVINCE&#39;, fields=[&#39;PROVINCE&#39;], # bind=slider, init={&#39;PROVINCE&#39;: &#39;Antwerpen&#39;}) color_scale = alt.Scale(domain=[&#39;Male&#39;, &#39;Female&#39;], range=[&#39;#1f77b4&#39;, &#39;#e377c2&#39;]) select_province = alt.selection_multi(fields=[&#39;PROVINCE&#39;], bind=&#39;legend&#39;) base = alt.Chart(df_plot).add_selection( select_province ).transform_filter( select_province ).transform_calculate( gender=alt.expr.if_(alt.datum.SEX == &#39;M&#39;, &#39;Male&#39;, &#39;Female&#39;) ).properties( width=250 ) left = base.transform_filter( alt.datum.gender == &#39;Female&#39; ).encode( y=alt.Y(&#39;AGEGROUP:O&#39;, axis=None), x=alt.X(&#39;percentage:Q&#39;, axis=alt.Axis(format=&#39;.0%&#39;), title=&#39;Percentage&#39;, sort=alt.SortOrder(&#39;descending&#39;), ), color=alt.Color(&#39;gender:N&#39;, scale=color_scale, legend=None), ).mark_bar().properties(title=&#39;Female&#39;) middle = base.encode( y=alt.Y(&#39;AGEGROUP:O&#39;, axis=None), text=alt.Text(&#39;AGEGROUP:O&#39;), ).mark_text().properties(width=20) right = base.transform_filter( alt.datum.gender == &#39;Male&#39; ).encode( y=alt.Y(&#39;AGEGROUP:O&#39;, axis=None), x=alt.X(&#39;percentage:Q&#39;, title=&#39;Percentage&#39;, axis=alt.Axis(format=&#39;.0%&#39;),), color=alt.Color(&#39;gender:N&#39;, scale=color_scale, legend=None) ).mark_bar().properties(title=&#39;Male&#39;) # legend = alt.Chart(df_plot).mark_text().encode( # y=alt.Y(&#39;PROVINCE:N&#39;, axis=None), # text=alt.Text(&#39;PROVINCE:N&#39;), # color=alt.Color(&#39;PROVINCE:N&#39;, legend=alt.Legend(title=&quot;Provincie&quot;)) # ) alt.concat(left, middle, right, spacing=5) #legend=alt.Legend(title=&quot;Species by color&quot;) . provinces = df_plot[&#39;PROVINCE&#39;].unique() select_province = alt.selection_single( name=&#39;Select&#39;, # name the selection &#39;Select&#39; fields=[&#39;PROVINCE&#39;], # limit selection to the Major_Genre field init={&#39;PROVINCE&#39;: &#39;Antwerpen&#39;}, # use first genre entry as initial value bind=alt.binding_select(options=provinces) # bind to a menu of unique provence values ) base = alt.Chart(df_plot).add_selection( select_province ).transform_filter( select_province ).transform_calculate( gender=alt.expr.if_(alt.datum.SEX == &#39;M&#39;, &#39;Male&#39;, &#39;Female&#39;) ).properties( width=250 ) left = base.transform_filter( alt.datum.gender == &#39;Female&#39; ).encode( y=alt.Y(&#39;AGEGROUP:O&#39;, axis=None), x=alt.X(&#39;percentage:Q&#39;, axis=alt.Axis(format=&#39;.0%&#39;), title=&#39;Percentage&#39;, sort=alt.SortOrder(&#39;descending&#39;), scale=alt.Scale(domain=(0.0, 0.1), clamp=True) ), color=alt.Color(&#39;gender:N&#39;, scale=color_scale, legend=None), tooltip=[alt.Tooltip(&#39;percentage&#39;, format=&#39;.1%&#39;)] ).mark_bar().properties(title=&#39;Female&#39;) middle = base.encode( y=alt.Y(&#39;AGEGROUP:O&#39;, axis=None), text=alt.Text(&#39;AGEGROUP:O&#39;), ).mark_text().properties(width=20) right = base.transform_filter( alt.datum.gender == &#39;Male&#39; ).encode( y=alt.Y(&#39;AGEGROUP:O&#39;, axis=None), x=alt.X(&#39;percentage:Q&#39;, title=&#39;Percentage&#39;, axis=alt.Axis(format=&#39;.1%&#39;), scale=alt.Scale(domain=(0.0, 0.1), clamp=True)), color=alt.Color(&#39;gender:N&#39;, scale=color_scale, legend=None), tooltip=[alt.Tooltip(&#39;percentage&#39;, format=&#39;.1%&#39;)] ).mark_bar().properties(title=&#39;Male&#39;) alt.concat(left, middle, right, spacing=5).properties(title=&#39;Percentage of covid-19 cases per province, gender and age grup in Belgium&#39;) . Mortality . # https://epistat.wiv-isp.be/covid/ # Dataset of mortality by date, age, sex, and region df_dead_sc = pd.read_csv(&#39;https://epistat.sciensano.be/Data/COVID19BE_MORT.csv&#39;) . df_dead_sc.head() . DATE REGION AGEGROUP SEX DEATHS . 0 2020-03-10 | Brussels | 85+ | F | 1 | . 1 2020-03-11 | Flanders | 85+ | F | 1 | . 2 2020-03-11 | Brussels | 75-84 | M | 1 | . 3 2020-03-11 | Brussels | 85+ | F | 1 | . 4 2020-03-12 | Brussels | 75-84 | M | 1 | . df_dead_sc[&#39;REGION&#39;].value_counts() . Wallonia 291 Flanders 275 Brussels 271 Name: REGION, dtype: int64 . df_dead_sc[&#39;AGEGROUP&#39;].value_counts() . 85+ 223 75-84 205 65-74 179 45-64 132 25-44 19 0-24 1 Name: AGEGROUP, dtype: int64 . df_inhab[&#39;AGEGROUP_sc&#39;] =pd.cut(df_inhab[&#39;CD_AGE&#39;], bins=[0,24,44,64,74,84,200], labels=[&#39;0-24&#39;,&#39;25-44&#39;,&#39;45-64&#39;,&#39;65-74&#39;,&#39;75-84&#39;,&#39;85+&#39;], include_lowest=True) . df_inhab.groupby(&#39;AGEGROUP_sc&#39;).agg(lowest_age=(&#39;CD_AGE&#39;, &#39;min&#39;), highest_age=(&#39;CD_AGE&#39;, max)) . lowest_age highest_age . AGEGROUP_sc . 0-24 0 | 24 | . 25-44 25 | 44 | . 45-64 45 | 64 | . 65-74 65 | 74 | . 75-84 75 | 84 | . 85+ 85 | 110 | . df_inhab.head() . CD_REFNIS TX_DESCR_NL TX_DESCR_FR CD_DSTR_REFNIS TX_ADM_DSTR_DESCR_NL TX_ADM_DSTR_DESCR_FR CD_PROV_REFNIS TX_PROV_DESCR_NL TX_PROV_DESCR_FR CD_RGN_REFNIS TX_RGN_DESCR_NL TX_RGN_DESCR_FR CD_SEX CD_NATLTY TX_NATLTY_NL TX_NATLTY_FR CD_CIV_STS TX_CIV_STS_NL TX_CIV_STS_FR CD_AGE MS_POPULATION sc_provence AGEGROUP AGEGROUP_sc . 0 11001 | Aartselaar | Aartselaar | 11000 | Arrondissement Antwerpen | Arrondissement d‚ÄôAnvers | 10000.0 | Provincie Antwerpen | Province d‚ÄôAnvers | 2000 | Vlaams Gewest | R√©gion flamande | F | BEL | Belgen | Belges | 4 | Gescheiden | Divorc√© | 69 | 11 | Antwerpen | 60-69 | 65-74 | . 1 11001 | Aartselaar | Aartselaar | 11000 | Arrondissement Antwerpen | Arrondissement d‚ÄôAnvers | 10000.0 | Provincie Antwerpen | Province d‚ÄôAnvers | 2000 | Vlaams Gewest | R√©gion flamande | F | BEL | Belgen | Belges | 4 | Gescheiden | Divorc√© | 80 | 3 | Antwerpen | 70-79 | 75-84 | . 2 11001 | Aartselaar | Aartselaar | 11000 | Arrondissement Antwerpen | Arrondissement d‚ÄôAnvers | 10000.0 | Provincie Antwerpen | Province d‚ÄôAnvers | 2000 | Vlaams Gewest | R√©gion flamande | M | BEL | Belgen | Belges | 4 | Gescheiden | Divorc√© | 30 | 2 | Antwerpen | 20-29 | 25-44 | . 3 11001 | Aartselaar | Aartselaar | 11000 | Arrondissement Antwerpen | Arrondissement d‚ÄôAnvers | 10000.0 | Provincie Antwerpen | Province d‚ÄôAnvers | 2000 | Vlaams Gewest | R√©gion flamande | F | BEL | Belgen | Belges | 4 | Gescheiden | Divorc√© | 48 | 26 | Antwerpen | 40-49 | 45-64 | . 4 11001 | Aartselaar | Aartselaar | 11000 | Arrondissement Antwerpen | Arrondissement d‚ÄôAnvers | 10000.0 | Provincie Antwerpen | Province d‚ÄôAnvers | 2000 | Vlaams Gewest | R√©gion flamande | F | BEL | Belgen | Belges | 4 | Gescheiden | Divorc√© | 76 | 2 | Antwerpen | 70-79 | 75-84 | . df_dead_sc[&#39;REGION&#39;].unique() . array([&#39;Brussels&#39;, &#39;Flanders&#39;, &#39;Wallonia&#39;], dtype=object) . df_inhab[&#39;TX_RGN_DESCR_NL&#39;].value_counts() . Vlaams Gewest 242865 Waals Gewest 199003 Brussels Hoofdstedelijk Gewest 21513 Name: TX_RGN_DESCR_NL, dtype: int64 . df_inhab_gender_prov = df_inhab.groupby([&#39;TX_RGN_DESCR_NL&#39;, &#39;CD_SEX&#39;, &#39;AGEGROUP_sc&#39;])[&#39;MS_POPULATION&#39;].sum().reset_index() . region_sc_to_region_inhad = {&#39;Flanders&#39;:&#39;Vlaams Gewest&#39;, &#39;Wallonia&#39;:&#39;Waals Gewest&#39;, &#39;Brussels&#39;:&#39;Brussels Hoofdstedelijk Gewest&#39;} . df_dead_sc[&#39;TX_RGN_DESCR_NL&#39;] = df_dead_sc[&#39;REGION&#39;].map(region_sc_to_region_inhad) . df_dead_sc.groupby([&#39;TX_RGN_DESCR_NL&#39;, &#39;AGEGROUP&#39;, &#39;SEX&#39;])[&#39;DEATHS&#39;].sum() . TX_RGN_DESCR_NL AGEGROUP SEX Brussels Hoofdstedelijk Gewest 25-44 F 1 M 4 45-64 F 21 M 43 65-74 F 42 M 71 75-84 F 128 M 170 85+ F 270 M 186 Vlaams Gewest 0-24 F 1 25-44 F 2 M 3 45-64 F 27 M 63 65-74 F 67 M 130 75-84 F 199 M 335 85+ F 232 M 309 Waals Gewest 25-44 F 5 M 4 45-64 F 41 M 89 65-74 F 98 M 186 75-84 F 290 M 300 85+ F 704 M 421 Name: DEATHS, dtype: int64 . df_dead_sc_region_agegroup_gender = df_dead_sc.groupby([&#39;TX_RGN_DESCR_NL&#39;, &#39;AGEGROUP&#39;, &#39;SEX&#39;])[&#39;DEATHS&#39;].sum().reset_index() . df_inhab_gender_prov_deaths = pd.merge(df_inhab_gender_prov, df_dead_sc_region_agegroup_gender, left_on=[&#39;TX_RGN_DESCR_NL&#39;, &#39;AGEGROUP_sc&#39;, &#39;CD_SEX&#39;], right_on=[&#39;TX_RGN_DESCR_NL&#39;, &#39;AGEGROUP&#39;, &#39;SEX&#39;]) . df_inhab_gender_prov_deaths[&#39;MS_POPULATION&#39;].sum() . 9077403 . df_inhab_gender_prov_deaths[&#39;DEATHS&#39;].sum() . 4442 . df_inhab_gender_prov_deaths . TX_RGN_DESCR_NL CD_SEX AGEGROUP_sc MS_POPULATION AGEGROUP SEX DEATHS . 0 Brussels Hoofdstedelijk Gewest | F | 25-44 | 197579 | 25-44 | F | 1 | . 1 Brussels Hoofdstedelijk Gewest | F | 45-64 | 137628 | 45-64 | F | 21 | . 2 Brussels Hoofdstedelijk Gewest | F | 65-74 | 45214 | 65-74 | F | 42 | . 3 Brussels Hoofdstedelijk Gewest | F | 75-84 | 30059 | 75-84 | F | 128 | . 4 Brussels Hoofdstedelijk Gewest | F | 85+ | 18811 | 85+ | F | 270 | . 5 Brussels Hoofdstedelijk Gewest | M | 25-44 | 194988 | 25-44 | M | 4 | . 6 Brussels Hoofdstedelijk Gewest | M | 45-64 | 140348 | 45-64 | M | 43 | . 7 Brussels Hoofdstedelijk Gewest | M | 65-74 | 36698 | 65-74 | M | 71 | . 8 Brussels Hoofdstedelijk Gewest | M | 75-84 | 19969 | 75-84 | M | 170 | . 9 Brussels Hoofdstedelijk Gewest | M | 85+ | 7918 | 85+ | M | 186 | . 10 Vlaams Gewest | F | 0-24 | 874891 | 0-24 | F | 1 | . 11 Vlaams Gewest | F | 25-44 | 820036 | 25-44 | F | 2 | . 12 Vlaams Gewest | F | 45-64 | 901554 | 45-64 | F | 27 | . 13 Vlaams Gewest | F | 65-74 | 353925 | 65-74 | F | 67 | . 14 Vlaams Gewest | F | 75-84 | 245981 | 75-84 | F | 199 | . 15 Vlaams Gewest | F | 85+ | 132649 | 85+ | F | 232 | . 16 Vlaams Gewest | M | 25-44 | 827281 | 25-44 | M | 3 | . 17 Vlaams Gewest | M | 45-64 | 917008 | 45-64 | M | 63 | . 18 Vlaams Gewest | M | 65-74 | 336242 | 65-74 | M | 130 | . 19 Vlaams Gewest | M | 75-84 | 193576 | 75-84 | M | 335 | . 20 Vlaams Gewest | M | 85+ | 69678 | 85+ | M | 309 | . 21 Waals Gewest | F | 25-44 | 457356 | 25-44 | F | 5 | . 22 Waals Gewest | F | 45-64 | 496668 | 45-64 | F | 41 | . 23 Waals Gewest | F | 65-74 | 199422 | 65-74 | F | 98 | . 24 Waals Gewest | F | 75-84 | 118224 | 75-84 | F | 290 | . 25 Waals Gewest | F | 85+ | 68502 | 85+ | F | 704 | . 26 Waals Gewest | M | 25-44 | 459444 | 25-44 | M | 4 | . 27 Waals Gewest | M | 45-64 | 487322 | 45-64 | M | 89 | . 28 Waals Gewest | M | 65-74 | 175508 | 65-74 | M | 186 | . 29 Waals Gewest | M | 75-84 | 82876 | 75-84 | M | 300 | . 30 Waals Gewest | M | 85+ | 30048 | 85+ | M | 421 | . df_inhab_gender_prov_deaths[&#39;percentage&#39;] = df_inhab_gender_prov_deaths[&#39;DEATHS&#39;]/df_inhab_gender_prov_deaths[&#39;MS_POPULATION&#39;] . df_plot = df_inhab_gender_prov_deaths . regions = df_plot[&#39;TX_RGN_DESCR_NL&#39;].unique() select_province = alt.selection_single( name=&#39;Select&#39;, # name the selection &#39;Select&#39; fields=[&#39;TX_RGN_DESCR_NL&#39;], # limit selection to the Major_Genre field init={&#39;TX_RGN_DESCR_NL&#39;: &#39;Vlaams Gewest&#39;}, # use first genre entry as initial value bind=alt.binding_select(options=regions) # bind to a menu of unique provence values ) base = alt.Chart(df_plot).add_selection( select_province ).transform_filter( select_province ).transform_calculate( gender=alt.expr.if_(alt.datum.SEX == &#39;M&#39;, &#39;Male&#39;, &#39;Female&#39;) ).properties( width=250 ) left = base.transform_filter( alt.datum.gender == &#39;Female&#39; ).encode( y=alt.Y(&#39;AGEGROUP:O&#39;, axis=None), x=alt.X(&#39;percentage:Q&#39;, axis=alt.Axis(format=&#39;.2%&#39;), title=&#39;Percentage&#39;, sort=alt.SortOrder(&#39;descending&#39;), # scale=alt.Scale(domain=(0.0, 0.02), clamp=True) ), color=alt.Color(&#39;gender:N&#39;, scale=color_scale, legend=None), tooltip=[alt.Tooltip(&#39;percentage&#39;, format=&#39;.2%&#39;)] ).mark_bar().properties(title=&#39;Female&#39;) middle = base.encode( y=alt.Y(&#39;AGEGROUP:O&#39;, axis=None), text=alt.Text(&#39;AGEGROUP:O&#39;), ).mark_text().properties(width=20) right = base.transform_filter( alt.datum.gender == &#39;Male&#39; ).encode( y=alt.Y(&#39;AGEGROUP:O&#39;, axis=None), # x=alt.X(&#39;percentage:Q&#39;, title=&#39;Percentage&#39;, axis=alt.Axis(format=&#39;.2%&#39;), scale=alt.Scale(domain=(0.0, 0.02), clamp=True)), x=alt.X(&#39;percentage:Q&#39;, title=&#39;Percentage&#39;, axis=alt.Axis(format=&#39;.2%&#39;)), color=alt.Color(&#39;gender:N&#39;, scale=color_scale, legend=None), tooltip=[alt.Tooltip(&#39;percentage&#39;, format=&#39;.2%&#39;)] ).mark_bar().properties(title=&#39;Male&#39;) alt.concat(left, middle, right, spacing=5).properties(title=&#39;Percentage of covid-19 deaths per province, gender and age group relative to number of inhabitants in Belgium&#39;) .",
            "url": "https://cast42.github.io/blog/cast42/jupyter/covid19/belgium/altair/2020/04/22/regional-covid19-mortality-belgium.html",
            "relUrl": "/cast42/jupyter/covid19/belgium/altair/2020/04/22/regional-covid19-mortality-belgium.html",
            "date": " ‚Ä¢ Apr 22, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Daily covid-19 Deaths compared to average deaths the last 10 years",
            "content": "# Import pandas for data wrangling and Altair for plotting import pandas as pd import altair as alt . The number of deadths per day from 2008 until 2018 can obtained from Statbel, the Belgium federal bureau of statistics: . df = pd.read_excel(&#39;https://statbel.fgov.be/sites/default/files/files/opendata/bevolking/TF_DEATHS.xlsx&#39;) # , skiprows=5, sheet_name=sheetnames . # Get a quick look to the data df.head() . DT_DATE MS_NUM_DEATHS . 0 2008-01-01 | 342 | . 1 2008-01-02 | 348 | . 2 2008-01-03 | 340 | . 3 2008-01-04 | 349 | . 4 2008-01-05 | 348 | . df[&#39;Jaar&#39;] = df[&#39;DT_DATE&#39;].dt.year df[&#39;Dag&#39;] = df[&#39;DT_DATE&#39;].dt.dayofyear . df_plot = df.groupby(&#39;Dag&#39;)[&#39;MS_NUM_DEATHS&#39;].mean().to_frame().reset_index() . # Let&#39;s make a quick plot alt.Chart(df_plot).mark_line().encode(x=&#39;Dag&#39;, y=&#39;MS_NUM_DEATHS&#39;).properties(width=600) . The John Hopkings University CSSE keeps track of the number of covid-19 deadths per day and country in a github repository: https://github.com/CSSEGISandData/COVID-19. We can easily obtain this data by reading it from github and filter out the cases for Belgium. . deaths_url = &#39;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv&#39; deaths = pd.read_csv(deaths_url, sep=&#39;,&#39;) . Filter out Belgium . deaths_be = deaths[deaths[&#39;Country/Region&#39;] == &#39;Belgium&#39;] . Inspect how the data is stored . deaths_be . Province/State Country/Region Lat Long 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 ... 4/9/20 4/10/20 4/11/20 4/12/20 4/13/20 4/14/20 4/15/20 4/16/20 4/17/20 4/18/20 . 23 NaN | Belgium | 50.8333 | 4.0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 2523 | 3019 | 3346 | 3600 | 3903 | 4157 | 4440 | 4857 | 5163 | 5453 | . 1 rows √ó 92 columns . Create dateframe for plotting . df_deaths = pd.DataFrame(data={&#39;Datum&#39;:pd.to_datetime(deaths_be.columns[4:]), &#39;Overlijdens&#39;:deaths_be.iloc[0].values[4:]}) . Check for Nan&#39;s . df_deaths[&#39;Overlijdens&#39;].isna().sum() . 0 . We need to do some type convertions. We cast &#39;Overlijdens&#39; to integer. Next, we add the number of the day. . df_deaths[&#39;Overlijdens&#39;] = df_deaths[&#39;Overlijdens&#39;].astype(int) df_deaths[&#39;Dag&#39;] = df_deaths[&#39;Datum&#39;].dt.dayofyear . Plot the data: . dead_2008_2018 = alt.Chart(df_plot).mark_line().encode(x=&#39;Dag&#39;, y=&#39;MS_NUM_DEATHS&#39;) dead_2008_2018 . Calculate the day-by-day change . df_deaths[&#39;Nieuwe covid-19 Sterfgevallen&#39;] = df_deaths[&#39;Overlijdens&#39;].diff() . # Check types df_deaths.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 88 entries, 0 to 87 Data columns (total 4 columns): # Column Non-Null Count Dtype -- -- 0 Datum 88 non-null datetime64[ns] 1 Overlijdens 88 non-null int32 2 Dag 88 non-null int64 3 Nieuwe covid-19 Sterfgevallen 87 non-null float64 dtypes: datetime64[ns](1), float64(1), int32(1), int64(1) memory usage: 2.5 KB . Plot covid-19 deaths in Belgium according to JHU CSSE. The plot shows a tooltip if you hover over the points. . dead_covid= alt.Chart(df_deaths).mark_line(point=True).encode( x=alt.X(&#39;Dag&#39;,scale=alt.Scale(domain=(1, 110), clamp=True)), y=&#39;Nieuwe covid-19 Sterfgevallen&#39;, color=alt.ColorValue(&#39;red&#39;), tooltip=[&#39;Dag&#39;, &#39;Nieuwe covid-19 Sterfgevallen&#39;]) dead_covid . Now we add average deaths per day in the last 10 year to the plot. . dead_2008_2018 + dead_covid . Take quick look to the datatable: . df.head() . DT_DATE MS_NUM_DEATHS Jaar Dag . 0 2008-01-01 | 342 | 2008 | 1 | . 1 2008-01-02 | 348 | 2008 | 2 | . 2 2008-01-03 | 340 | 2008 | 3 | . 3 2008-01-04 | 349 | 2008 | 4 | . 4 2008-01-05 | 348 | 2008 | 5 | . The column &#39;DT_DATE&#39; is a string. We convert it to a datatime so we can add it to the tooltip. . df[&#39;Datum&#39;] = pd.to_datetime(df[&#39;DT_DATE&#39;]) . Now we are prepared to make the final graph. We use the Altair mark_errorband(extend=&#39;ci&#39;) to bootstrap 95% confidence band around the average number of deaths per day. . line = alt.Chart(df).mark_line().encode( x=alt.X(&#39;Dag&#39;, scale=alt.Scale( domain=(1, 120), clamp=True )), y=&#39;mean(MS_NUM_DEATHS)&#39; ) # Bootstrapped 95% confidence interval band = alt.Chart(df).mark_errorband(extent=&#39;ci&#39;).encode( x=alt.X(&#39;Dag&#39;, scale=alt.Scale(domain=(1, 120), clamp=True)), y=alt.Y(&#39;MS_NUM_DEATHS&#39;, title=&#39;Overlijdens per dag&#39;), ) dead_covid= alt.Chart(df_deaths).mark_line(point=True).encode( x=alt.X(&#39;Dag&#39;,scale=alt.Scale(domain=(1, 120), clamp=True)), y=&#39;Nieuwe covid-19 Sterfgevallen&#39;, color=alt.ColorValue(&#39;red&#39;), tooltip=[&#39;Dag&#39;, &#39;Nieuwe covid-19 Sterfgevallen&#39;, &#39;Datum&#39;] ) (band + line + dead_covid).properties(width=1024, title=&#39;Gemiddeld aantal overlijdens over 10 jaar versus overlijdens door covid-19 in Belgie&#39;) . Source date from sciensano . In this section, we compare the graph obtained with data obtained from sciensano. . df_sc = pd.read_csv(&#39;https://epistat.sciensano.be/Data/COVID19BE_MORT.csv&#39;) . df_sc.head() . DATE REGION AGEGROUP SEX DEATHS . 0 2020-03-10 | Brussels | 85+ | F | 1 | . 1 2020-03-11 | Flanders | 85+ | F | 1 | . 2 2020-03-11 | Brussels | 75-84 | M | 1 | . 3 2020-03-11 | Brussels | 85+ | F | 1 | . 4 2020-03-12 | Brussels | 75-84 | M | 1 | . df_dead_day = df_sc.groupby(&#39;DATE&#39;)[&#39;DEATHS&#39;].sum().reset_index() df_dead_day[&#39;Datum&#39;] = pd.to_datetime(df_dead_day[&#39;DATE&#39;]) df_dead_day[&#39;Dag&#39;] = df_dead_day[&#39;Datum&#39;].dt.dayofyear . line = alt.Chart(df).mark_line().encode( x=alt.X(&#39;Dag&#39;, title=&#39;Dag van het jaar&#39;, scale=alt.Scale( domain=(1, 120), clamp=True )), y=&#39;mean(MS_NUM_DEATHS)&#39; ) # Bootstrapped 95% confidence interval band = alt.Chart(df).mark_errorband(extent=&#39;ci&#39;).encode( x=alt.X(&#39;Dag&#39;, scale=alt.Scale(domain=(1, 120), clamp=True)), y=alt.Y(&#39;MS_NUM_DEATHS&#39;, title=&#39;Overlijdens per dag&#39;), ) dead_covid= alt.Chart(df_dead_day).mark_line(point=True).encode( x=alt.X(&#39;Dag&#39;,scale=alt.Scale(domain=(1, 120), clamp=True)), y=&#39;DEATHS&#39;, color=alt.ColorValue(&#39;red&#39;), tooltip=[&#39;Dag&#39;, &#39;DEATHS&#39;, &#39;Datum&#39;] ) (band + line + dead_covid).properties(width=750, title=&#39;Gemiddeld aantal overlijdens over 10 jaar versus overlijdens door covid-19 in Belgie&#39;) . Obviously, data form 16-17-18 April 2020 is not final yet. Also, the amounts are smaller then those from JHU. . Obtain more detail (for another blogpost...) . df_tot_sc = pd.read_excel(&#39;https://epistat.sciensano.be/Data/COVID19BE.xlsx&#39;) . df_tot_sc . DATE PROVINCE REGION AGEGROUP SEX CASES . 0 2020-03-01 | Brussels | Brussels | 10-19 | M | 1 | . 1 2020-03-01 | Brussels | Brussels | 10-19 | F | 1 | . 2 2020-03-01 | Brussels | Brussels | 20-29 | M | 1 | . 3 2020-03-01 | Brussels | Brussels | 30-39 | F | 1 | . 4 2020-03-01 | Brussels | Brussels | 40-49 | F | 1 | . ... ... | ... | ... | ... | ... | ... | . 6875 NaN | OostVlaanderen | Flanders | NaN | F | 4 | . 6876 NaN | VlaamsBrabant | Flanders | 40-49 | M | 3 | . 6877 NaN | VlaamsBrabant | Flanders | 40-49 | F | 2 | . 6878 NaN | VlaamsBrabant | Flanders | 50-59 | M | 1 | . 6879 NaN | WestVlaanderen | Flanders | 50-59 | M | 3 | . 6880 rows √ó 6 columns . We know that there are a lot of reional differences: . df_plot = df_tot_sc.groupby([&#39;DATE&#39;, &#39;PROVINCE&#39;])[&#39;CASES&#39;].sum().reset_index() . df_plot . DATE PROVINCE CASES . 0 2020-03-01 | Brussels | 6 | . 1 2020-03-01 | Limburg | 1 | . 2 2020-03-01 | Li√®ge | 2 | . 3 2020-03-01 | OostVlaanderen | 1 | . 4 2020-03-01 | VlaamsBrabant | 6 | . ... ... | ... | ... | . 505 2020-04-17 | OostVlaanderen | 44 | . 506 2020-04-17 | VlaamsBrabant | 42 | . 507 2020-04-17 | WestVlaanderen | 30 | . 508 2020-04-18 | Brussels | 1 | . 509 2020-04-18 | Hainaut | 1 | . 510 rows √ó 3 columns . df_plot[&#39;DATE&#39;] = pd.to_datetime(df_plot[&#39;DATE&#39;]) . base = alt.Chart(df_plot, title=&#39;Number of cases in Belgium per day and province&#39;).mark_line(point=True).encode( x=alt.X(&#39;DATE:T&#39;, title=&#39;Datum&#39;), y=alt.Y(&#39;CASES&#39;, title=&#39;Cases per day&#39;), color=&#39;PROVINCE&#39;, tooltip=[&#39;DATE&#39;, &#39;CASES&#39;, &#39;PROVINCE&#39;] ).properties(width=600) base . From the above graph we see a much lower number of cases in Luxembourg, Namur, Waals Brabant. . !pwd . &#39;pwd&#39; is not recognized as an internal or external command, operable program or batch file. . !dir . Volume in drive C is Windows Volume Serial Number is 7808-E933 Directory of C: Users lnh6dt5 AppData Local Temp Mxt121 tmp home_lnh6dt5 blog _notebooks 19/04/2020 14:14 &lt;DIR&gt; . 19/04/2020 14:14 &lt;DIR&gt; .. 19/04/2020 10:37 &lt;DIR&gt; .ipynb_checkpoints 19/04/2020 10:17 23.473 2020-01-28-Altair.ipynb 19/04/2020 10:34 9.228 2020-01-29-bullet-chart-altair.ipynb 19/04/2020 10:26 41.041 2020-02-15-breakins.ipynb 19/04/2020 09:43 30.573 2020-02-20-test.ipynb 19/04/2020 09:49 1.047 2020-04-18-first-test.ipynb 19/04/2020 14:14 1.237.674 2020-04-19-deads-last-ten-year-vs-covid.ipynb 19/04/2020 09:43 &lt;DIR&gt; my_icons 19/04/2020 09:43 771 README.md 7 File(s) 1.343.807 bytes 4 Dir(s) 89.905.336.320 bytes free .",
            "url": "https://cast42.github.io/blog/cast42/jupyter/covid19/belgium/altair/2020/04/19/deads-last-ten-year-vs-covid.html",
            "relUrl": "/cast42/jupyter/covid19/belgium/altair/2020/04/19/deads-last-ten-year-vs-covid.html",
            "date": " ‚Ä¢ Apr 19, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "First test post",
            "content": "import pandas as pd import altair as alt . # Check if this get published .",
            "url": "https://cast42.github.io/blog/jupyter/2020/04/18/first-test.html",
            "relUrl": "/jupyter/2020/04/18/first-test.html",
            "date": " ‚Ä¢ Apr 18, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.‚Ü© . 2. This is the other footnote. You can even have a link!‚Ü© .",
            "url": "https://cast42.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " ‚Ä¢ Feb 20, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Evolution of burglary in Leuven. Is the trend downwards ?",
            "content": "The local police shared a graph with the number of break-ins in Leuven per year. The article shows a graph with a downwards trendline. Can we conclude that the number of breakins is showing a downward trend based on those numbers? Let&#39;s construct a dataframe with the data from the graph. . import numpy as np import pandas as pd import altair as alt df = pd.DataFrame({&#39;year_int&#39;:[y for y in range(2006, 2020)], &#39;breakins&#39;:[1133,834,953,891,1006,1218,992,1079,1266,1112,713,669,730,644]}) df[&#39;year&#39;] = pd.to_datetime(df[&#39;year_int&#39;], format=&#39;%Y&#39;) . points = alt.Chart(df).mark_line(point=True).encode( x=&#39;year&#39;, y=&#39;breakins&#39;, tooltip=&#39;breakins&#39; ) points + points.transform_regression(&#39;year&#39;, &#39;breakins&#39;).mark_line( color=&#39;green&#39; ).properties( title=&#39;Regression trend on the number breakins per year in Leuven&#39; ) . The article claims that the number of breakins stabilizes the last years. Let&#39;s perform a local regression to check that. . # https://opendatascience.com/local-regression-in-python # Loess: https://gist.github.com/AllenDowney/818f6153ef316aee80467c51faee80f8 points + points.transform_loess(&#39;year&#39;, &#39;breakins&#39;).mark_line( color=&#39;green&#39; ).properties( title=&#39;Local regression trend on the number breakins per year in Leuven&#39; ) . But what about the trend line? Are we sure the trend is negative ? Bring in the code based on the blogpost The hacker&#39;s guide to uncertainty estimates to estimate the uncertainty.: . # Code from: https://erikbern.com/2018/10/08/the-hackers-guide-to-uncertainty-estimates.html import scipy.optimize import random def model(xs, k, m): return k * xs + m def neg_log_likelihood(tup, xs, ys): # Since sigma &gt; 0, we use use log(sigma) as the parameter instead. # That way we have an unconstrained problem. k, m, log_sigma = tup sigma = np.exp(log_sigma) delta = model(xs, k, m) - ys return len(xs)/2*np.log(2*np.pi*sigma**2) + np.dot(delta, delta) / (2*sigma**2) def confidence_bands(xs, ys, nr_bootstrap): curves = [] xys = list(zip(xs, ys)) for i in range(nr_bootstrap): # sample with replacement bootstrap = [random.choice(xys) for _ in xys] xs_bootstrap = np.array([x for x, y in bootstrap]) ys_bootstrap = np.array([y for x, y in bootstrap]) k_hat, m_hat, log_sigma_hat = scipy.optimize.minimize( neg_log_likelihood, (0, 0, 0), args=(xs_bootstrap, ys_bootstrap) ).x curves.append( model(xs, k_hat, m_hat) + # Note what&#39;s going on here: we&#39;re _adding_ the random term # to the predictions! np.exp(log_sigma_hat) * np.random.normal(size=xs.shape) ) lo, hi = np.percentile(curves, (2.5, 97.5), axis=0) return lo, hi . # Make a plot with a confidence band df[&#39;lo&#39;], df[&#39;hi&#39;] = confidence_bands(df.index, df[&#39;breakins&#39;], 100) ci = alt.Chart(df).mark_area().encode( x=alt.X(&#39;year:T&#39;, title=&#39;&#39;), y=alt.Y(&#39;lo:Q&#39;), y2=alt.Y2(&#39;hi:Q&#39;, title=&#39;&#39;), color=alt.value(&#39;lightblue&#39;), opacity=alt.value(0.6) ) chart = alt.Chart(df).mark_line(point=True).encode( x=&#39;year&#39;, y=&#39;breakins&#39;, tooltip=&#39;breakins&#39; ) ci + chart + chart.transform_regression(&#39;year&#39;, &#39;breakins&#39;).mark_line( color=&#39;red&#39; ).properties( title=&#39;95% Confidence band of the number of breakins per year in Leuven&#39; ) . On the above chart, we see that a possitive trend might be possible as well. . Linear regression . Let&#39;s perform a linear regression with statsmodel to calculate the confidence interval on the slope of the regression line. . import statsmodels.formula.api as smf . results = smf.ols(&#39;breakins ~ index&#39;, data=df.reset_index()).fit() . results.params . Intercept 1096.314286 index -23.169231 dtype: float64 . The most likely slope of the trend line is 23.17 breakins per year. But how sure are we that the trend is heading down ? . results.summary() . C: Users lnh6dt5 AppData Local Continuum anaconda3 lib site-packages scipy stats stats.py:1535: UserWarning: kurtosistest only valid for n&gt;=20 ... continuing anyway, n=14 &#34;anyway, n=%i&#34; % int(n)) . OLS Regression Results Dep. Variable: breakins | R-squared: 0.223 | . Model: OLS | Adj. R-squared: 0.159 | . Method: Least Squares | F-statistic: 3.451 | . Date: Sun, 19 Apr 2020 | Prob (F-statistic): 0.0879 | . Time: 10:26:45 | Log-Likelihood: -92.105 | . No. Observations: 14 | AIC: 188.2 | . Df Residuals: 12 | BIC: 189.5 | . Df Model: 1 | | . Covariance Type: nonrobust | | . | coef std err t P&gt;|t| [0.025 0.975] . Intercept 1096.3143 | 95.396 | 11.492 | 0.000 | 888.465 | 1304.164 | . index -23.1692 | 12.472 | -1.858 | 0.088 | -50.344 | 4.006 | . Omnibus: 1.503 | Durbin-Watson: 1.035 | . Prob(Omnibus): 0.472 | Jarque-Bera (JB): 1.196 | . Skew: 0.577 | Prob(JB): 0.550 | . Kurtosis: 2.153 | Cond. No. 14.7 | . Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified. The analysis reveals that the slope of the best fitting regression line is 23 breakins less per year. However, the confidence interval of the trend is between -50.344 and 4.006. Also the p)value of the regression coefficient is 0.088. Meaning we have eight percent chance that the negative trend is by accident. Hence, based on the current data we are not 95% percent sure the trend is downwards. Hence we can not conclude, based on this data, that there is a negative trend. This corresponds with the width of the 95% certainty band drawn that allows for an upward trend line: . # Here are the confidence intervals of the regression results.conf_int() . 0 1 . Intercept 888.464586 | 1304.163986 | . index -50.344351 | 4.005889 | . y_low = results.params[&#39;Intercept&#39;] # ?ost likely value of the intercept y_high = results.params[&#39;Intercept&#39;] + results.conf_int()[1][&#39;index&#39;] * df.shape[0] # Value of upward trend for the last year df_upward_trend = pd.DataFrame({&#39;year&#39;:[df[&#39;year&#39;].min(), df[&#39;year&#39;].max()], &#39;breakins&#39;:[y_low, y_high]}) possible_upwards_trend = alt.Chart(df_upward_trend).mark_line( color=&#39;green&#39;, strokeDash=[10,10] ).encode( x=&#39;year:T&#39;, y=alt.Y(&#39;breakins:Q&#39;, title=&#39;Number of breakins per year&#39;) ) points = alt.Chart(df).mark_line(point=True).encode(x=&#39;year&#39;, y=&#39;breakins&#39;, tooltip=&#39;breakins&#39;) (ci + points + points.transform_regression(&#39;year&#39;, &#39;breakins&#39;).mark_line(color=&#39;red&#39;) + possible_upwards_trend).properties( title=&#39;Trend analysis on the number of breakins per year in Leuven, Belgium&#39; ) . In the above graph, we see that a slight positive trend (green dashed line) is in the 95% confidence band on the regression coefficient. We are not sure that the trend on the number of breakins is downwards. .",
            "url": "https://cast42.github.io/blog/cast42/jupyter/altair/2020/02/15/breakins.html",
            "relUrl": "/cast42/jupyter/altair/2020/02/15/breakins.html",
            "date": " ‚Ä¢ Feb 15, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Bullet chart in python Altair",
            "content": "In the article &quot;Bullet Charts - What Is It And How To Use It&quot; I learned about Bullet charts. It&#39;s a specific kind of barchart that must convey the state of a measure or KPI. The goal is to see in a glance if the target is met. Here is an example bullet chart from the article: . # This causes issues to: # from IPython.display import Image # Image(&#39;https://jscharting.com/blog/bullet-charts/images/bullet_components.png&#39;) . . # &lt;img src=&quot;https://jscharting.com/blog/bullet-charts/images/bullet_components.png&quot; alt=&quot;Bullet chart&quot; style=&quot;width: 200px;&quot;/&gt; . Below is some Python code that generates bullets graphs using the Altair library. . import altair as alt import pandas as pd df = pd.DataFrame.from_records([ {&quot;title&quot;:&quot;Revenue&quot;,&quot;subtitle&quot;:&quot;US$, in thousands&quot;,&quot;ranges&quot;:[150,225,300],&quot;measures&quot;:[220,270],&quot;markers&quot;:[250]}, {&quot;title&quot;:&quot;Profit&quot;,&quot;subtitle&quot;:&quot;%&quot;,&quot;ranges&quot;:[20,25,30],&quot;measures&quot;:[21,23],&quot;markers&quot;:[26]}, {&quot;title&quot;:&quot;Order Size&quot;,&quot;subtitle&quot;:&quot;US$, average&quot;,&quot;ranges&quot;:[350,500,600],&quot;measures&quot;:[100,320],&quot;markers&quot;:[550]}, {&quot;title&quot;:&quot;New Customers&quot;,&quot;subtitle&quot;:&quot;count&quot;,&quot;ranges&quot;:[1400,2000,2500],&quot;measures&quot;:[1000,1650],&quot;markers&quot;:[2100]}, {&quot;title&quot;:&quot;Satisfaction&quot;,&quot;subtitle&quot;:&quot;out of 5&quot;,&quot;ranges&quot;:[3.5,4.25,5],&quot;measures&quot;:[3.2,4.7],&quot;markers&quot;:[4.4]} ]) alt.layer( alt.Chart().mark_bar(color=&#39;#eee&#39;).encode(alt.X(&quot;ranges[2]:Q&quot;, scale=alt.Scale(nice=False), title=None)), alt.Chart().mark_bar(color=&#39;#ddd&#39;).encode(x=&quot;ranges[1]:Q&quot;), alt.Chart().mark_bar(color=&#39;#bbb&#39;).encode(x=&quot;ranges[0]:Q&quot;), alt.Chart().mark_bar(color=&#39;steelblue&#39;, size=10).encode(x=&#39;measures[0]:Q&#39;), alt.Chart().mark_tick(color=&#39;black&#39;, size=12).encode(x=&#39;markers[0]:Q&#39;), data=df ).facet( row=alt.Row(&quot;title:O&quot;, title=&#39;&#39;) ).resolve_scale( x=&#39;independent&#39; ) .",
            "url": "https://cast42.github.io/blog/cast42/jupyter/data%20visualisation/altair/2020/01/29/bullet-chart-altair.html",
            "relUrl": "/cast42/jupyter/data%20visualisation/altair/2020/01/29/bullet-chart-altair.html",
            "date": " ‚Ä¢ Jan 29, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Simple Notebook with interactive Altair graph",
            "content": "Test fast template notebook posts . import altair as alt . Make an Altair graph . from vega_datasets import data iris = data.iris() alt.Chart(iris).mark_point().encode( x=&#39;petalLength&#39;, y=&#39;petalWidth&#39;, color=&#39;species&#39;, tooltip=&#39;species&#39; ).interactive() . Enjoy :) .",
            "url": "https://cast42.github.io/blog/cast42/jupyter/covid19/belgium/altair/2020/01/28/Altair.html",
            "relUrl": "/cast42/jupyter/covid19/belgium/altair/2020/01/28/Altair.html",
            "date": " ‚Ä¢ Jan 28, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule: . . Lists . Here‚Äôs a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes ‚Ä¶and‚Ä¶ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.¬†&#8617; . |",
            "url": "https://cast42.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " ‚Ä¢ Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is Lode Nachtergaele. I‚Äôm an engineer by training, and currently hold the position as data scientist at Colruyt Group (a retailer in Belgium), where I spend time on writing notebooks in Python. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.¬†&#8617; . |",
          "url": "https://cast42.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://cast42.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}